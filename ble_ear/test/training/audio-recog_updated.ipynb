{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# TORCH","metadata":{}},{"cell_type":"code","source":"!pip install -q gdown\n!pip install -U -q torch torchaudio torchvision \n!pip install -U -q librosa matplotlib seaborn","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-22T13:01:42.935872Z","iopub.execute_input":"2025-06-22T13:01:42.936116Z","iopub.status.idle":"2025-06-22T13:01:55.471536Z","shell.execute_reply.started":"2025-06-22T13:01:42.936089Z","shell.execute_reply":"2025-06-22T13:01:55.470345Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"import gdown\nimport zipfile\nimport os\n\n# The Google Drive file ID from your link\n# Your link: https://drive.google.com/file/d/1tkqNh_E6yjN3uOyQmf7spQeURm2_b26h/view?usp=sharing\n#            https://drive.google.com/file/d/1CRLxg3tbEh9CTG3oIAPSksz8Pwyjq0SL/view?usp=sharing\n# The ID is: 1tkqNh_E6yjN3uOyQmf7spQeURm2_b26h\n\nfile_id = '1CRLxg3tbEh9CTG3oIAPSksz8Pwyjq0SL'\noutput_zip_path = 'downloaded_dataset.zip'\nextract_to_path = '/kaggle/working/korean_speech_commands_kaggle' \n\n# Download the file\nprint(f\"Downloading file with ID: {file_id} to {output_zip_path}...\")\ngdown.download(id=file_id, output=output_zip_path, quiet=False)\n\nif os.path.exists(output_zip_path):\n    print(f\"Download complete: {output_zip_path}\")\n\n    # Create extraction directory if it doesn't exist\n    if not os.path.exists(extract_to_path):\n        os.makedirs(extract_to_path)\n        print(f\"Created directory: {extract_to_path}\")\n\n    # Extract the zip file\n    print(f\"Extracting {output_zip_path} to {extract_to_path}...\")\n    try:\n        with zipfile.ZipFile(output_zip_path, 'r') as zip_ref:\n            zip_ref.extractall(extract_to_path)\n        print(\"Extraction complete.\")\n        \n        # List the contents of the extracted directory (optional, for verification)\n        print(f\"\\nContents of {extract_to_path}:\")\n        for item in os.listdir(extract_to_path):\n            print(f\"- {item}\")\n            # If your zip file has an intermediate folder, you might need to list inside that\n            # For example, if it extracts to 'extract_to_path/only_command/'\n            if os.path.isdir(os.path.join(extract_to_path, item)):\n                 print(f\"  Contents of {item}: {os.listdir(os.path.join(extract_to_path, item))[:5]}...\")\n\n\n    except zipfile.BadZipFile:\n        print(f\"Error: The downloaded file '{output_zip_path}' is not a valid zip file or is corrupted.\")\n    except Exception as e:\n        print(f\"An error occurred during extraction: {e}\")\n    \n    # Clean up the downloaded zip file (optional)\n    # os.remove(output_zip_path)\n    # print(f\"Removed downloaded zip file: {output_zip_path}\")\nelse:\n    print(f\"Error: Download failed. File '{output_zip_path}' not found.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-22T08:47:15.122519Z","iopub.execute_input":"2025-06-22T08:47:15.122733Z","iopub.status.idle":"2025-06-22T08:47:21.281783Z","shell.execute_reply.started":"2025-06-22T08:47:15.122715Z","shell.execute_reply":"2025-06-22T08:47:21.281000Z"}},"outputs":[{"name":"stdout","text":"Downloading file with ID: 1CRLxg3tbEh9CTG3oIAPSksz8Pwyjq0SL to downloaded_dataset.zip...\n","output_type":"stream"},{"name":"stderr","text":"Downloading...\nFrom (original): https://drive.google.com/uc?id=1CRLxg3tbEh9CTG3oIAPSksz8Pwyjq0SL\nFrom (redirected): https://drive.google.com/uc?id=1CRLxg3tbEh9CTG3oIAPSksz8Pwyjq0SL&confirm=t&uuid=36bada40-8eef-4f39-abb1-4c53979300a1\nTo: /kaggle/working/downloaded_dataset.zip\n100%|██████████| 194M/194M [00:01<00:00, 105MB/s]  \n","output_type":"stream"},{"name":"stdout","text":"Download complete: downloaded_dataset.zip\nCreated directory: /kaggle/working/korean_speech_commands_kaggle\nExtracting downloaded_dataset.zip to /kaggle/working/korean_speech_commands_kaggle...\nExtraction complete.\n\nContents of /kaggle/working/korean_speech_commands_kaggle:\n- all_comand\n  Contents of all_comand: ['위', '후방', '전방', '20미터', '복귀']...\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"## Grok","metadata":{}},{"cell_type":"code","source":"import os\nimport pathlib\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchaudio\nfrom torchaudio import transforms\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom IPython.display import display, Audio\nfrom torch.utils.data import DataLoader, Dataset\nfrom sklearn.model_selection import train_test_split\n\n# Set seed for reproducibility\ntorch.manual_seed(42)\nnp.random.seed(42)\n\n# Define constants\nDATASET_PATH = '/kaggle/working/korean_speech_commands_kaggle/all_comand'  # Replace with your actual path\nMAX_AUDIO_SECONDS = 4\nSAMPLE_RATE = 16000\nOUTPUT_SEQ_LEN = MAX_AUDIO_SECONDS * SAMPLE_RATE  # 64000 samples\nNUM_CLASSES = 32  # Placeholder, will be set based on labels.txt\n\n# Extract folder names and save to labels.txt\ndef save_labels_to_file(data_dir):\n    data_path = pathlib.Path(data_dir)\n    label_names = [d.name for d in data_path.iterdir() if d.is_dir()]\n    label_names.sort()  # Ensure consistent order\n    with open('labels.txt', 'w', encoding='utf-8') as f:\n        f.write(\"Detected Label Names:\\n\")\n        for label in label_names:\n            f.write(f\"Label: {label}\\n\")\n    return label_names\n\n# Custom Dataset\nclass AudioDataset(Dataset):\n    def __init__(self, data_dir, label_names):\n        self.data_dir = pathlib.Path(data_dir)\n        self.label_names = label_names\n        self.data = []\n        self.labels = []\n        for i, label in enumerate(label_names):\n            label_dir = self.data_dir / label\n            for file in label_dir.glob('*.wav'):\n                self.data.append(str(file))\n                self.labels.append(i)\n        self.label_to_idx = {label: idx for idx, label in enumerate(label_names)}\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        waveform, sample_rate = torchaudio.load(self.data[idx])\n        waveform = waveform.mean(dim=0)  # Convert to mono if stereo\n        waveform = torch.nn.functional.pad(waveform, (0, OUTPUT_SEQ_LEN - waveform.shape[0]))[:OUTPUT_SEQ_LEN]\n        spectrogram = self.get_spectrogram(waveform)\n        label = self.labels[idx]\n        return spectrogram, label\n\n    def get_spectrogram(self, waveform):\n        transform = transforms.Spectrogram(n_fft=512, hop_length=128, power=None)\n        spec = transform(waveform)  # Shape: [freq_bins, time_steps] e.g., [257, 501]\n        spec = torch.abs(spec)  # Magnitude spectrogram\n        # Add channel dimension and treat as [N, C, H, W] for interpolation\n        spec = spec.unsqueeze(0).unsqueeze(0)  # Shape: [1, 1, freq_bins, time_steps]\n        spec = torch.nn.functional.interpolate(spec, size=(171, 560), mode='bilinear', align_corners=False)\n        return spec.squeeze(0)  # Shape: [1, 171, 560], remove the batch dimension\n\n# Load labels from labels.txt\nif os.path.exists('labels.txt'):\n    with open('labels.txt', 'r', encoding='utf-8') as f:\n        lines = f.readlines()\n        label_names = [line.split(': ')[1].strip() for line in lines[1:] if ':' in line]\nelse:\n    label_names = save_labels_to_file(DATASET_PATH)\n    with open('labels.txt', 'r', encoding='utf-8') as f:\n        lines = f.readlines()\n        label_names = [line.split(': ')[1].strip() for line in lines[1:] if ':' in line]\nprint(f\"Loaded {len(label_names)} labels: {label_names}\")\n\n# Update NUM_CLASSES based on the number of labels\nNUM_CLASSES = len(label_names)\n\n# Create dataset and split\ndataset = AudioDataset(DATASET_PATH, label_names)\ntrain_data, val_data = train_test_split(list(range(len(dataset))), test_size=0.1, random_state=42)\ntrain_dataset = torch.utils.data.Subset(dataset, train_data)\nval_dataset = torch.utils.data.Subset(dataset, val_data)\n\n# DataLoader\ntrain_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=8)\n\n# Define the model\nclass SpeechModel(nn.Module):\n    def __init__(self, num_classes=NUM_CLASSES):\n        super(SpeechModel, self).__init__()\n        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n        self.pool = nn.MaxPool2d(2, 2)\n        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n        self.dropout = nn.Dropout(0.25)\n        self.fc1 = nn.Linear(64 * 42 * 140, 128)  # Adjusted for [171, 560] after pooling\n        self.fc2 = nn.Linear(128, num_classes)\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n        x = self.pool(self.relu(self.conv1(x)))\n        x = self.pool(self.relu(self.conv2(x)))\n        x = x.view(x.size(0), -1)  # Flatten\n        x = self.dropout(self.relu(self.fc1(x)))\n        x = self.dropout(x)\n        x = self.fc2(x)\n        return x\n\n# Initialize model, loss, and optimizer\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = SpeechModel().to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.0001)\n\n# Training loop\n# def train_model(model, train_loader, val_loader, epochs=50, patience=7, min_epochs=15):\n#     best_val_loss = float('inf')\n#     trigger_times = 0\n\n#     for epoch in range(epochs):\n#         model.train()\n#         train_loss = 0\n#         for batch_idx, (spectrograms, labels) in enumerate(train_loader):\n#             spectrograms, labels = spectrograms.to(device), labels.to(device)\n#             optimizer.zero_grad()\n#             outputs = model(spectrograms)\n#             loss = criterion(outputs, labels)\n#             loss.backward()\n#             optimizer.step()\n#             train_loss += loss.item()\n\n#         # Validation\n#         model.eval()\n#         val_loss = 0\n#         correct = 0\n#         total = 0\n#         with torch.no_grad():\n#             for spectrograms, labels in val_loader:\n#                 spectrograms, labels = spectrograms.to(device), labels.to(device)\n#                 outputs = model(spectrograms)\n#                 loss = criterion(outputs, labels)\n#                 val_loss += loss.item()\n#                 _, predicted = torch.max(outputs.data, 1)\n#                 total += labels.size(0)\n#                 correct += (predicted == labels).sum().item()\n\n#         train_loss = train_loss / len(train_loader)\n#         val_loss = val_loss / len(val_loader)\n#         val_acc = 100 * correct / total\n\n#         print(f'Epoch {epoch+1}/{epochs} - Train Loss: {train_loss:.4f} - Val Loss: {val_loss:.4f} - Val Acc: {val_acc:.2f}%')\n\n#         # Early stopping\n#         if epoch >= min_epochs - 1:  # Only apply early stopping after min_epochs\n#             if val_loss < best_val_loss:\n#                 best_val_loss = val_loss\n#                 trigger_times = 0\n#                 torch.save(model.state_dict(), 'best_model.pth')\n#             else:\n#                 trigger_times += 1\n#                 if trigger_times >= patience:\n#                     print(f'Early stopping at epoch {epoch+1}')\n#                     model.load_state_dict(torch.load('best_model.pth'))\n#                     break\n#         else:\n#             if val_loss < best_val_loss:\n#                 best_val_loss = val_loss\n#                 torch.save(model.state_dict(), 'best_model.pth')\n\n\n\ndef train_model(model, train_loader, val_loader, epochs=50, patience=10, min_epochs=15):\n    best_val_acc = 0.0  # Track best accuracy instead of loss\n    trigger_times = 0\n\n    for epoch in range(epochs):\n        model.train()\n        train_loss = 0\n        for batch_idx, (spectrograms, labels) in enumerate(train_loader):\n            spectrograms, labels = spectrograms.to(device), labels.to(device)\n            optimizer.zero_grad()\n            outputs = model(spectrograms)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            train_loss += loss.item()\n\n        # Validation\n        model.eval()\n        val_loss = 0\n        correct = 0\n        total = 0\n        with torch.no_grad():\n            for spectrograms, labels in val_loader:\n                spectrograms, labels = spectrograms.to(device), labels.to(device)\n                outputs = model(spectrograms)\n                loss = criterion(outputs, labels)\n                val_loss += loss.item()\n                _, predicted = torch.max(outputs.data, 1)\n                total += labels.size(0)\n                correct += (predicted == labels).sum().item()\n\n        train_loss = train_loss / len(train_loader)\n        val_loss = val_loss / len(val_loader)\n        val_acc = 100 * correct / total\n\n        print(f'Epoch {epoch+1}/{epochs} - Train Loss: {train_loss:.4f} - Val Loss: {val_loss:.4f} - Val Acc: {val_acc:.2f}%')\n\n        # Early stopping based on VALIDATION ACCURACY (new logic)\n        if epoch >= min_epochs - 1:  # Only check after min_epochs\n            if val_acc > best_val_acc:\n                best_val_acc = val_acc\n                trigger_times = 0\n                torch.save(model.state_dict(), 'best_model.pth')\n                print(f'New best model saved (Val Acc: {val_acc:.2f}%)')\n            else:\n                trigger_times += 1\n                if trigger_times >= patience:\n                    print(f'Early stopping at epoch {epoch+1} (no improvement in {patience} epochs)')\n                    model.load_state_dict(torch.load('best_model.pth'))  # Restore best model\n                    break\n        else:\n            # Save the model if it's the best so far (even before min_epochs)\n            if val_acc > best_val_acc:\n                best_val_acc = val_acc\n                torch.save(model.state_dict(), 'best_model.pth')\n\n# Train the model\ntrain_model(model, train_loader, val_loader, epochs=50, patience=10, min_epochs=15)\n\n# Inference on a sample from the dataset\nsample_path = os.path.join(DATASET_PATH, '드론', 'aug_clean_speaker0_Drone_3_volume.wav')  # Example file from dataset\nwaveform, sample_rate = torchaudio.load(sample_path)\nwaveform = waveform.mean(dim=0)  # Convert to mono\nwaveform = torch.nn.functional.pad(waveform, (0, OUTPUT_SEQ_LEN - waveform.shape[0]))[:OUTPUT_SEQ_LEN]\nspectrogram = AudioDataset(DATASET_PATH, label_names).get_spectrogram(waveform).to(device)\n\n# Predict\nmodel.eval()\nwith torch.no_grad():\n    output = model(spectrogram.unsqueeze(0))  # Add batch dimension\n    predicted_probabilities = torch.softmax(output, dim=1).cpu().numpy()\n    predicted_index = torch.argmax(output).cpu().numpy()\n    predicted_label = label_names[predicted_index]\n    confidence = predicted_probabilities[0][predicted_index]\n    true_label = '드론'  # Known true label for this file\n    is_correct = predicted_index == label_names.index(true_label)\n\nprint(f\"Predicted label: {predicted_label} (Confidence: {confidence:.2f})\")\nprint(f\"True label: {true_label}\")\nprint(f\"Prediction is {'correct' if is_correct else 'incorrect'}\")\n\n# Plot the prediction\nplt.figure(figsize=(12, 6))\nplt.bar(label_names, predicted_probabilities[0])\nplt.title(f'Prediction for {os.path.basename(sample_path)}')\nplt.xlabel('Labels')\nplt.ylabel('Probability')\nplt.xticks(rotation=90)\nplt.tight_layout()\nplt.show()\n\n# Play the audio\n# display.display(Audio(waveform, rate=SAMPLE_RATE))\n\n# Export to ONNX\ndummy_input = torch.zeros(1, 1, 171, 560).to(device)\ntorch.onnx.export(model, dummy_input, \"b8newpytorch_model.onnx\", opset_version=13, input_names=['input'], output_names=['output'])\nprint(\"Model exported to pytorch_model.onnx\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-20T03:25:37.265865Z","iopub.execute_input":"2025-06-20T03:25:37.267365Z","iopub.status.idle":"2025-06-20T04:05:29.033367Z","shell.execute_reply.started":"2025-06-20T03:25:37.267321Z","shell.execute_reply":"2025-06-20T04:05:29.032511Z"}},"outputs":[{"name":"stdout","text":"Loaded 50 labels: ['100미터', '10미터', '12시', '1미터', '1시', '200미터', '20미터', '2시', '300미터', '3미터', '3시', '400미터', '40미터', '5미터', '60미터', '6시', '80미터', '9시', 'IR꺼', 'IR켜', '경계', '경계모드', '공격모드', '느리게', '대기', '드론', '매우느리게', '매우빠르게', '모드변경', '복귀', '빠르게', '사격', '아래', '야간모드', '우로', '우회전', '위', '전방', '전진', '정지', '정찰', '정찰모드', '조명꺼', '조명켜', '조준', '좌로', '좌회전', '주행모드', '후방', '후진']\nEpoch 1/50 - Train Loss: 3.9222 - Val Loss: 3.8402 - Val Acc: 4.81%\nEpoch 2/50 - Train Loss: 3.7011 - Val Loss: 3.3656 - Val Acc: 22.73%\nEpoch 3/50 - Train Loss: 2.9931 - Val Loss: 2.5164 - Val Acc: 43.58%\nEpoch 4/50 - Train Loss: 2.2193 - Val Loss: 1.8595 - Val Acc: 61.76%\nEpoch 5/50 - Train Loss: 1.7193 - Val Loss: 1.4487 - Val Acc: 67.38%\nEpoch 6/50 - Train Loss: 1.3297 - Val Loss: 1.1844 - Val Acc: 75.13%\nEpoch 7/50 - Train Loss: 1.1239 - Val Loss: 1.1016 - Val Acc: 77.27%\nEpoch 8/50 - Train Loss: 0.9271 - Val Loss: 1.0240 - Val Acc: 77.01%\nEpoch 9/50 - Train Loss: 0.8053 - Val Loss: 0.9818 - Val Acc: 76.74%\nEpoch 10/50 - Train Loss: 0.6777 - Val Loss: 0.9941 - Val Acc: 77.01%\nEpoch 11/50 - Train Loss: 0.5870 - Val Loss: 0.9761 - Val Acc: 75.94%\nEpoch 12/50 - Train Loss: 0.5453 - Val Loss: 0.8866 - Val Acc: 79.41%\nEpoch 13/50 - Train Loss: 0.4946 - Val Loss: 0.9249 - Val Acc: 78.88%\nEpoch 14/50 - Train Loss: 0.4420 - Val Loss: 0.9219 - Val Acc: 78.61%\nEpoch 15/50 - Train Loss: 0.4007 - Val Loss: 0.9538 - Val Acc: 77.01%\nEpoch 16/50 - Train Loss: 0.3877 - Val Loss: 0.9918 - Val Acc: 78.07%\nEpoch 17/50 - Train Loss: 0.3857 - Val Loss: 0.9278 - Val Acc: 78.07%\nEpoch 18/50 - Train Loss: 0.3092 - Val Loss: 1.0341 - Val Acc: 76.47%\nEpoch 19/50 - Train Loss: 0.3157 - Val Loss: 0.9966 - Val Acc: 78.07%\nEpoch 20/50 - Train Loss: 0.2904 - Val Loss: 1.0454 - Val Acc: 77.27%\nEpoch 21/50 - Train Loss: 0.2876 - Val Loss: 1.0055 - Val Acc: 78.34%\nEpoch 22/50 - Train Loss: 0.2767 - Val Loss: 1.0256 - Val Acc: 77.54%\nEpoch 23/50 - Train Loss: 0.2354 - Val Loss: 1.0546 - Val Acc: 79.68%\nNew best model saved (Val Acc: 79.68%)\nEpoch 24/50 - Train Loss: 0.2369 - Val Loss: 1.0311 - Val Acc: 80.21%\nNew best model saved (Val Acc: 80.21%)\nEpoch 25/50 - Train Loss: 0.2249 - Val Loss: 1.0843 - Val Acc: 79.14%\nEpoch 26/50 - Train Loss: 0.2421 - Val Loss: 0.9472 - Val Acc: 79.41%\nEpoch 27/50 - Train Loss: 0.2124 - Val Loss: 0.9827 - Val Acc: 80.75%\nNew best model saved (Val Acc: 80.75%)\nEpoch 28/50 - Train Loss: 0.2044 - Val Loss: 1.1188 - Val Acc: 76.74%\nEpoch 29/50 - Train Loss: 0.1865 - Val Loss: 1.0364 - Val Acc: 80.48%\nEpoch 30/50 - Train Loss: 0.2015 - Val Loss: 1.2245 - Val Acc: 76.20%\nEpoch 31/50 - Train Loss: 0.1833 - Val Loss: 1.2122 - Val Acc: 78.07%\nEpoch 32/50 - Train Loss: 0.1983 - Val Loss: 1.1282 - Val Acc: 79.68%\nEpoch 33/50 - Train Loss: 0.1887 - Val Loss: 1.1287 - Val Acc: 77.81%\nEpoch 34/50 - Train Loss: 0.1879 - Val Loss: 1.3560 - Val Acc: 78.88%\nEpoch 35/50 - Train Loss: 0.1780 - Val Loss: 1.2196 - Val Acc: 77.81%\nEpoch 36/50 - Train Loss: 0.1647 - Val Loss: 1.3370 - Val Acc: 78.34%\nEpoch 37/50 - Train Loss: 0.1462 - Val Loss: 1.3362 - Val Acc: 77.01%\nEarly stopping at epoch 37 (no improvement in 10 epochs)\nPredicted label: 드론 (Confidence: 1.00)\nTrue label: 드론\nPrediction is correct\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_35/2391666398.py:267: UserWarning: Glyph 48120 (\\N{HANGUL SYLLABLE MI}) missing from current font.\n  plt.tight_layout()\n/tmp/ipykernel_35/2391666398.py:267: UserWarning: Glyph 53552 (\\N{HANGUL SYLLABLE TEO}) missing from current font.\n  plt.tight_layout()\n/tmp/ipykernel_35/2391666398.py:267: UserWarning: Glyph 49884 (\\N{HANGUL SYLLABLE SI}) missing from current font.\n  plt.tight_layout()\n/tmp/ipykernel_35/2391666398.py:267: UserWarning: Glyph 44732 (\\N{HANGUL SYLLABLE GGEO}) missing from current font.\n  plt.tight_layout()\n/tmp/ipykernel_35/2391666398.py:267: UserWarning: Glyph 53020 (\\N{HANGUL SYLLABLE KYEO}) missing from current font.\n  plt.tight_layout()\n/tmp/ipykernel_35/2391666398.py:267: UserWarning: Glyph 44221 (\\N{HANGUL SYLLABLE GYEONG}) missing from current font.\n  plt.tight_layout()\n/tmp/ipykernel_35/2391666398.py:267: UserWarning: Glyph 44228 (\\N{HANGUL SYLLABLE GYE}) missing from current font.\n  plt.tight_layout()\n/tmp/ipykernel_35/2391666398.py:267: UserWarning: Glyph 47784 (\\N{HANGUL SYLLABLE MO}) missing from current font.\n  plt.tight_layout()\n/tmp/ipykernel_35/2391666398.py:267: UserWarning: Glyph 46300 (\\N{HANGUL SYLLABLE DEU}) missing from current font.\n  plt.tight_layout()\n/tmp/ipykernel_35/2391666398.py:267: UserWarning: Glyph 44277 (\\N{HANGUL SYLLABLE GONG}) missing from current font.\n  plt.tight_layout()\n/tmp/ipykernel_35/2391666398.py:267: UserWarning: Glyph 44201 (\\N{HANGUL SYLLABLE GYEOG}) missing from current font.\n  plt.tight_layout()\n/tmp/ipykernel_35/2391666398.py:267: UserWarning: Glyph 45712 (\\N{HANGUL SYLLABLE NEU}) missing from current font.\n  plt.tight_layout()\n/tmp/ipykernel_35/2391666398.py:267: UserWarning: Glyph 47532 (\\N{HANGUL SYLLABLE RI}) missing from current font.\n  plt.tight_layout()\n/tmp/ipykernel_35/2391666398.py:267: UserWarning: Glyph 44172 (\\N{HANGUL SYLLABLE GE}) missing from current font.\n  plt.tight_layout()\n/tmp/ipykernel_35/2391666398.py:267: UserWarning: Glyph 45824 (\\N{HANGUL SYLLABLE DAE}) missing from current font.\n  plt.tight_layout()\n/tmp/ipykernel_35/2391666398.py:267: UserWarning: Glyph 44592 (\\N{HANGUL SYLLABLE GI}) missing from current font.\n  plt.tight_layout()\n/tmp/ipykernel_35/2391666398.py:267: UserWarning: Glyph 47200 (\\N{HANGUL SYLLABLE RON}) missing from current font.\n  plt.tight_layout()\n/tmp/ipykernel_35/2391666398.py:267: UserWarning: Glyph 47588 (\\N{HANGUL SYLLABLE MAE}) missing from current font.\n  plt.tight_layout()\n/tmp/ipykernel_35/2391666398.py:267: UserWarning: Glyph 50864 (\\N{HANGUL SYLLABLE U}) missing from current font.\n  plt.tight_layout()\n/tmp/ipykernel_35/2391666398.py:267: UserWarning: Glyph 48736 (\\N{HANGUL SYLLABLE BBA}) missing from current font.\n  plt.tight_layout()\n/tmp/ipykernel_35/2391666398.py:267: UserWarning: Glyph 47476 (\\N{HANGUL SYLLABLE REU}) missing from current font.\n  plt.tight_layout()\n/tmp/ipykernel_35/2391666398.py:267: UserWarning: Glyph 48320 (\\N{HANGUL SYLLABLE BYEON}) missing from current font.\n  plt.tight_layout()\n/tmp/ipykernel_35/2391666398.py:267: UserWarning: Glyph 48373 (\\N{HANGUL SYLLABLE BOG}) missing from current font.\n  plt.tight_layout()\n/tmp/ipykernel_35/2391666398.py:267: UserWarning: Glyph 44480 (\\N{HANGUL SYLLABLE GWI}) missing from current font.\n  plt.tight_layout()\n/tmp/ipykernel_35/2391666398.py:267: UserWarning: Glyph 49324 (\\N{HANGUL SYLLABLE SA}) missing from current font.\n  plt.tight_layout()\n/tmp/ipykernel_35/2391666398.py:267: UserWarning: Glyph 50500 (\\N{HANGUL SYLLABLE A}) missing from current font.\n  plt.tight_layout()\n/tmp/ipykernel_35/2391666398.py:267: UserWarning: Glyph 47000 (\\N{HANGUL SYLLABLE RAE}) missing from current font.\n  plt.tight_layout()\n/tmp/ipykernel_35/2391666398.py:267: UserWarning: Glyph 50556 (\\N{HANGUL SYLLABLE YA}) missing from current font.\n  plt.tight_layout()\n/tmp/ipykernel_35/2391666398.py:267: UserWarning: Glyph 44036 (\\N{HANGUL SYLLABLE GAN}) missing from current font.\n  plt.tight_layout()\n/tmp/ipykernel_35/2391666398.py:267: UserWarning: Glyph 47196 (\\N{HANGUL SYLLABLE RO}) missing from current font.\n  plt.tight_layout()\n/tmp/ipykernel_35/2391666398.py:267: UserWarning: Glyph 54924 (\\N{HANGUL SYLLABLE HOE}) missing from current font.\n  plt.tight_layout()\n/tmp/ipykernel_35/2391666398.py:267: UserWarning: Glyph 51204 (\\N{HANGUL SYLLABLE JEON}) missing from current font.\n  plt.tight_layout()\n/tmp/ipykernel_35/2391666398.py:267: UserWarning: Glyph 50948 (\\N{HANGUL SYLLABLE WI}) missing from current font.\n  plt.tight_layout()\n/tmp/ipykernel_35/2391666398.py:267: UserWarning: Glyph 48169 (\\N{HANGUL SYLLABLE BANG}) missing from current font.\n  plt.tight_layout()\n/tmp/ipykernel_35/2391666398.py:267: UserWarning: Glyph 51652 (\\N{HANGUL SYLLABLE JIN}) missing from current font.\n  plt.tight_layout()\n/tmp/ipykernel_35/2391666398.py:267: UserWarning: Glyph 51221 (\\N{HANGUL SYLLABLE JEONG}) missing from current font.\n  plt.tight_layout()\n/tmp/ipykernel_35/2391666398.py:267: UserWarning: Glyph 51648 (\\N{HANGUL SYLLABLE JI}) missing from current font.\n  plt.tight_layout()\n/tmp/ipykernel_35/2391666398.py:267: UserWarning: Glyph 52272 (\\N{HANGUL SYLLABLE CAL}) missing from current font.\n  plt.tight_layout()\n/tmp/ipykernel_35/2391666398.py:267: UserWarning: Glyph 51312 (\\N{HANGUL SYLLABLE JO}) missing from current font.\n  plt.tight_layout()\n/tmp/ipykernel_35/2391666398.py:267: UserWarning: Glyph 47749 (\\N{HANGUL SYLLABLE MYEONG}) missing from current font.\n  plt.tight_layout()\n/tmp/ipykernel_35/2391666398.py:267: UserWarning: Glyph 51456 (\\N{HANGUL SYLLABLE JUN}) missing from current font.\n  plt.tight_layout()\n/tmp/ipykernel_35/2391666398.py:267: UserWarning: Glyph 51340 (\\N{HANGUL SYLLABLE JWA}) missing from current font.\n  plt.tight_layout()\n/tmp/ipykernel_35/2391666398.py:267: UserWarning: Glyph 51452 (\\N{HANGUL SYLLABLE JU}) missing from current font.\n  plt.tight_layout()\n/tmp/ipykernel_35/2391666398.py:267: UserWarning: Glyph 54665 (\\N{HANGUL SYLLABLE HAENG}) missing from current font.\n  plt.tight_layout()\n/tmp/ipykernel_35/2391666398.py:267: UserWarning: Glyph 54980 (\\N{HANGUL SYLLABLE HU}) missing from current font.\n  plt.tight_layout()\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 1200x600 with 1 Axes>","image/png":"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\n"},"metadata":{}},{"name":"stdout","text":"Model exported to pytorch_model.onnx\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## GEMINI","metadata":{}},{"cell_type":"code","source":"\"\"\"Korean_Speech_Recognition_PyTorch.ipynb\"\"\"\n\nimport os\nimport pathlib\nimport random\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\nimport librosa # For Mel Spectrograms\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader, random_split\nimport torchaudio\nimport torchaudio.transforms as T\nfrom IPython import display\nimport tensorflow as tf\n\n# Set seed for reproducibility\nseed = 42\ntorch.manual_seed(seed)\nnp.random.seed(seed)\nrandom.seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\n# --- Configuration ---\n# --- Configuration ---\nDATASET_PATH = '/kaggle/working/korean_speech_commands_kaggle/all_comand'  # Replace with your actual path\nMAX_AUDIO_SECONDS = 4\nSAMPLE_RATE = 16000\nTARGET_SEQ_LEN_SAMPLES = MAX_AUDIO_SECONDS * SAMPLE_RATE  # 64000 samples (4 seconds)\n\n# Spectrogram / Feature Parameters (Crucial - match your RKNN model's expected input)\nN_MELS = 171        # Number of Mel bands (Height of spectrogram)\nTARGET_FRAMES = 560   # Number of time frames (Width of spectrogram)\nN_FFT = 512         # FFT window size\n\n# Calculate HOP_LENGTH to get close to TARGET_FRAMES from TARGET_SEQ_LEN_SAMPLES\n# Formula: TARGET_FRAMES = floor((num_samples - n_fft) / hop_length) + 1\n# So, hop_length approx (num_samples - n_fft) / (TARGET_FRAMES - 1)\n# For 4s audio (64000 samples):\n# hop_length = (64000 - 512) / (560 - 1) = 63488 / 559 = 113.57...\n# Let's use int(113) or int(114). The subsequent resize will handle exact dimensions.\nHOP_LENGTH = 114 # This will produce (64000 - 512) / 114 + 1 = 557.8 -> 557 frames from librosa\n                 # The torch.nn.functional.interpolate will resize this to TARGET_FRAMES (560).\nprint(f\"Using N_FFT: {N_FFT}, HOP_LENGTH: {HOP_LENGTH}\")\nnum_expected_frames_before_resize = (TARGET_SEQ_LEN_SAMPLES - N_FFT) // HOP_LENGTH + 1\nprint(f\"Number of frames from librosa before resize (approx): {num_expected_frames_before_resize}\")\n\n\nBATCH_SIZE = 8\nEPOCHS = 50\nLEARNING_RATE = 0.001\n\n\n# --- Data Loading and Preprocessing ---\ndata_dir = pathlib.Path(DATASET_PATH)\nif not data_dir.exists():\n    # Attempt to load from Google Drive if in Colab\n    try:\n        from google.colab import drive\n        drive.mount('/content/drive')\n        DRIVE_DATASET_PATH = '/content/drive/MyDrive/ble_p/dataset/only_command.zip' # ADJUST\n        if pathlib.Path(DRIVE_DATASET_PATH).exists():\n            import zipfile\n            print(f\"Extracting {DRIVE_DATASET_PATH} to {DATASET_PATH}...\")\n            os.makedirs(pathlib.Path(DATASET_PATH).parent, exist_ok=True)\n            with zipfile.ZipFile(DRIVE_DATASET_PATH, 'r') as zip_ref:\n                zip_ref.extractall(pathlib.Path(DATASET_PATH).parent) # Extract to parent of 'only_command'\n            print(\"Extraction complete.\")\n        else:\n             raise ValueError(f\"Dataset ZIP not found at {DRIVE_DATASET_PATH} and {DATASET_PATH} does not exist.\")\n    except ModuleNotFoundError: # Not in Colab\n        raise ValueError(f\"Dataset path {DATASET_PATH} does not exist. Please update DATASET_PATH.\")\nelse:\n    print(f\"Dataset found at {DATASET_PATH}\")\n\n\n# Get class names (folder names)\nclass_names = sorted([item.name for item in data_dir.glob('*') if item.is_dir()])\nnum_classes = len(class_names)\nclass_to_idx = {name: i for i, name in enumerate(class_names)}\nidx_to_class = {i: name for i, name in enumerate(class_names)}\n\nprint('Commands (detected folder names):', class_names)\nprint(f\"Found {num_classes} classes.\")\n\n# Save labels to a text file\nwith open('labels_pytorch.txt', 'w', encoding='utf-8') as f:\n    f.write(\"Detected Label Names (PyTorch):\\n\")\n    for i, label in enumerate(class_names):\n        f.write(f\"{i}: {label}\\n\")\n\n\n# Custom PyTorch Dataset\nclass KoreanSpeechDataset(Dataset):\n    def __init__(self, data_path, class_to_idx, sample_rate, target_seq_len_samples,\n                 n_mels, target_frames, n_fft, hop_length, transform=None):\n        self.data_path = pathlib.Path(data_path)\n        self.class_to_idx = class_to_idx\n        self.sample_rate = sample_rate\n        self.target_seq_len_samples = target_seq_len_samples\n        self.n_mels = n_mels\n        self.target_frames = target_frames\n        self.n_fft = n_fft\n        self.hop_length = hop_length\n        self.transform = transform # For data augmentation\n\n        self.filepaths = []\n        self.labels = []\n        for class_name in self.class_to_idx.keys():\n            class_dir = self.data_path / class_name\n            for filepath in class_dir.glob('*.wav'):\n                self.filepaths.append(filepath)\n                self.labels.append(self.class_to_idx[class_name])\n\n    def __len__(self):\n        return len(self.filepaths)\n\n    def __getitem__(self, idx):\n        filepath = self.filepaths[idx]\n        label = self.labels[idx]\n\n        # Load waveform using torchaudio\n        try:\n            waveform, sr = torchaudio.load(filepath)\n        except Exception as e:\n            print(f\"Error loading {filepath}: {e}\")\n            # Return a dummy tensor and label or skip\n            return torch.zeros((1, self.target_seq_len_samples)), torch.tensor(-1)\n\n\n        # Ensure correct sample rate (resample if necessary)\n        if sr != self.sample_rate:\n            resampler = T.Resample(sr, self.sample_rate)\n            waveform = resampler(waveform)\n\n        # Ensure mono\n        if waveform.shape[0] > 1:\n            waveform = torch.mean(waveform, dim=0, keepdim=True)\n\n        # Pad or truncate waveform to target_seq_len_samples\n        current_len = waveform.shape[1]\n        if current_len < self.target_seq_len_samples:\n            padding = self.target_seq_len_samples - current_len\n            waveform = torch.nn.functional.pad(waveform, (0, padding))\n        elif current_len > self.target_seq_len_samples:\n            waveform = waveform[:, :self.target_seq_len_samples]\n\n        # Apply data augmentation if a transform is provided\n        if self.transform:\n            waveform = self.transform(waveform)\n\n        # Get Mel Spectrogram using librosa (as it's often more flexible for params)\n        # Convert to numpy for librosa\n        waveform_np = waveform.squeeze().numpy()\n        # Calculate POWER spectrogram first\n        mel_spec_power = librosa.feature.melspectrogram(y=waveform_np, sr=self.sample_rate,\n                                                  n_fft=self.n_fft, hop_length=self.hop_length,\n                                                  n_mels=self.n_mels, power=2.0) # power=2.0 for power spec\n        # Convert power spectrogram to dB scale\n        log_mel_spec_db = librosa.power_to_db(mel_spec_power, ref=np.max) # Shape: (n_mels, num_frames)\n\n        # Convert to tensor, add channel dimension (which specshow will squeeze out)\n        log_mel_spec_tensor = torch.from_numpy(log_mel_spec_db).float().unsqueeze(0) # [1, n_mels, num_frames]\n        \n        # Resize to target width (TARGET_FRAMES)\n        if log_mel_spec_tensor.shape[2] != self.target_frames: # If number of frames is not already TARGET_FRAMES\n            # Interpolate expects [Batch, Channel, Height, Width] or [C, H, W]\n            # Our log_mel_spec_tensor is effectively [C=1, H=n_mels, W=actual_frames]\n            # We add a dummy batch for interpolate, then squeeze it out\n            resized_spec = torch.nn.functional.interpolate(\n                log_mel_spec_tensor.unsqueeze(0), # [1, 1, n_mels, actual_frames]\n                size=(self.n_mels, self.target_frames), # Target H (n_mels), W (target_frames)\n                mode='bilinear',\n                align_corners=False\n            ).squeeze(0) # Back to [1, n_mels, target_frames]\n        else:\n            resized_spec = log_mel_spec_tensor\n\n        return resized_spec, torch.tensor(label)\n\n\n# Create dataset instance\nfull_dataset = KoreanSpeechDataset(data_dir, class_to_idx, SAMPLE_RATE, TARGET_SEQ_LEN_SAMPLES,\n                                   N_MELS, TARGET_FRAMES, N_FFT, HOP_LENGTH)\n\n# Split dataset\ntrain_size = int(0.8 * len(full_dataset))\nval_size = int(0.1 * len(full_dataset))\ntest_size = len(full_dataset) - train_size - val_size\ntrain_dataset, val_dataset, test_dataset = random_split(full_dataset, [train_size, val_size, test_size])\n\nprint(f\"Train dataset size: {len(train_dataset)} samples\")\nprint(f\"Validation dataset size: {len(val_dataset)} samples\")\nprint(f\"Test dataset size: {len(test_dataset)} samples\")\n\n# Create DataLoaders\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\nval_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\ntest_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n\n# Verify a batch\nfor specs, labels in train_loader:\n    print(\"Batch spectrogram shape:\", specs.shape) # Should be [BATCH_SIZE, 1, N_MELS, TARGET_FRAMES]\n    print(\"Batch labels shape:\", labels.shape)\n    example_spectrograms = specs\n    example_spect_labels = labels\n    break\n\n\n# Plot spectrogram (function adapted for PyTorch tensors)\ndef plot_spectrogram_pytorch(spectrogram_tensor, ax, title=\"Spectrogram\"):\n    # Assuming spectrogram_tensor from your DataLoader is already the log_mel_spec_db \n    # (output of librosa.power_to_db) and has shape [1, N_MELS, TARGET_FRAMES]\n    \n    spectrogram_to_plot = spectrogram_tensor.squeeze().cpu().numpy() \n    # Now spectrogram_to_plot has shape (N_MELS, TARGET_FRAMES), e.g., (171, 560)\n    # This is already log-scaled (dB) if your dataset prepares it that way.\n\n    # librosa.display.specshow handles axes correctly for (freq_bins, time_frames) input\n    img = librosa.display.specshow(spectrogram_to_plot, \n                                   sr=SAMPLE_RATE,      # Defined in your notebook\n                                   hop_length=HOP_LENGTH, # Defined in your notebook\n                                   x_axis='time', \n                                   y_axis='mel',        # Assumes it's a Mel spectrogram\n                                   ax=ax,\n                                   cmap='magma')       # You can choose other cmaps like 'viridis'\n    ax.set_title(title)\n    \n\n\n# Visualize some spectrograms from a batch\nif 'example_spectrograms' in locals():\n    rows = 3\n    cols = 3\n    n = rows*cols\n    fig, axes = plt.subplots(rows, cols, figsize=(16, 9))\n    for i in range(min(n, example_spectrograms.size(0))): # Handle cases where batch is smaller than n\n        r = i // cols\n        c = i % cols\n        ax = axes[r][c]\n        plot_spectrogram_pytorch(example_spectrograms[i], ax, title=idx_to_class[example_spect_labels[i].item()])\n    plt.tight_layout()\n    plt.show()\nelse:\n    print(\"No spectrogram examples to plot. Check data loading.\")\n\n\n# --- Model Definition ---\nclass SimpleCNNKWS(nn.Module):\n    def __init__(self, input_height, input_width, num_classes):\n        super(SimpleCNNKWS, self).__init__()\n        # Input shape: [Batch, Channels=1, Height=N_MELS, Width=TARGET_FRAMES]\n        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding='same')\n        self.relu1 = nn.ReLU()\n        self.bn1 = nn.BatchNorm2d(32) # Batch norm after conv, before relu or after relu\n        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.dropout1 = nn.Dropout(0.25)\n\n        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding='same')\n        self.relu2 = nn.ReLU()\n        self.bn2 = nn.BatchNorm2d(64)\n        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.dropout2 = nn.Dropout(0.25)\n\n        # Calculate the flattened size after convolutions and pooling\n        # After conv1+pool1: H_out = H_in/2, W_out = W_in/2\n        # After conv2+pool2: H_out = H_in/4, W_out = W_in/4\n        # Example: Input (171, 560) -> Pool1 -> (85, 280) -> Pool2 -> (42, 140) (approx due to int division)\n        # Let's calculate it dynamically or ensure it's correct\n        # For input H=171, W=560:\n        # Conv1 (same padding) -> (171, 560)\n        # Pool1 (kernel=2, stride=2) -> (171//2, 560//2) = (85, 280)\n        # Conv2 (same padding) -> (85, 280)\n        # Pool2 (kernel=2, stride=2) -> (85//2, 280//2) = (42, 140)\n        # So, flattened_size = 64 * 42 * 140\n        self.flattened_size = 64 * (input_height // 4) * (input_width // 4)\n\n        self.flatten = nn.Flatten()\n        self.fc1 = nn.Linear(self.flattened_size, 128)\n        self.relu3 = nn.ReLU()\n        self.bn3 = nn.BatchNorm1d(128) # BatchNorm1d for dense layer input\n        self.dropout3 = nn.Dropout(0.5)\n        self.fc2 = nn.Linear(128, num_classes)\n\n    def forward(self, x):\n        # x shape: [batch, 1, N_MELS, TARGET_FRAMES]\n        x = self.pool1(self.bn1(self.relu1(self.conv1(x))))\n        x = self.dropout1(x)\n        x = self.pool2(self.bn2(self.relu2(self.conv2(x))))\n        x = self.dropout2(x)\n        x = self.flatten(x)\n        x = self.dropout3(self.bn3(self.relu3(self.fc1(x))))\n        x = self.fc2(x) # Output logits\n        return x\n\n# Instantiate model\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = SimpleCNNKWS(input_height=N_MELS, input_width=TARGET_FRAMES, num_classes=num_classes).to(device)\nprint(model)\n\n# Test with a dummy input\ndummy_input_for_model_test = torch.randn(BATCH_SIZE, 1, N_MELS, TARGET_FRAMES).to(device)\noutput_test = model(dummy_input_for_model_test)\nprint(\"Model output shape:\", output_test.shape) # Should be [BATCH_SIZE, num_classes]\n\n\n# --- Training Loop ---\ncriterion = nn.CrossEntropyLoss() # Softmax is included in CrossEntropyLoss\noptimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n\n# Early stopping parameters\npatience = 5\nbest_val_loss = float('inf')\nepochs_no_improve = 0\n\nhistory = {'loss': [], 'val_loss': [], 'accuracy': [], 'val_accuracy': []}\n\nfor epoch in range(EPOCHS):\n    model.train()\n    running_loss = 0.0\n    correct_train = 0\n    total_train = 0\n\n    for i, (inputs, labels) in enumerate(train_loader):\n        inputs, labels = inputs.to(device), labels.to(device)\n\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n        _, predicted = torch.max(outputs.data, 1)\n        total_train += labels.size(0)\n        correct_train += (predicted == labels).sum().item()\n\n        if (i + 1) % 10 == 0: # Print every 10 mini-batches\n             print(f'Epoch [{epoch+1}/{EPOCHS}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}')\n\n    epoch_loss = running_loss / len(train_loader)\n    epoch_acc = 100 * correct_train / total_train\n    history['loss'].append(epoch_loss)\n    history['accuracy'].append(epoch_acc / 100.0) # Store as fraction for consistency with Keras history\n    print(f\"Epoch {epoch+1} Training Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.2f}%\")\n\n    # Validation\n    model.eval()\n    val_loss = 0.0\n    correct_val = 0\n    total_val = 0\n    with torch.no_grad():\n        for inputs, labels in val_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            val_loss += loss.item()\n            _, predicted = torch.max(outputs.data, 1)\n            total_val += labels.size(0)\n            correct_val += (predicted == labels).sum().item()\n\n    epoch_val_loss = val_loss / len(val_loader)\n    epoch_val_acc = 100 * correct_val / total_val\n    history['val_loss'].append(epoch_val_loss)\n    history['val_accuracy'].append(epoch_val_acc / 100.0)\n    print(f\"Epoch {epoch+1} Validation Loss: {epoch_val_loss:.4f}, Accuracy: {epoch_val_acc:.2f}%\")\n\n    # Early stopping\n    if epoch_val_loss < best_val_loss:\n        best_val_loss = epoch_val_loss\n        epochs_no_improve = 0\n        torch.save(model.state_dict(), 'best_korean_kws_pytorch_model.pth') # Save best model\n        print(f\"Validation loss improved. Saved best model to best_korean_kws_pytorch_model.pth\")\n    else:\n        epochs_no_improve += 1\n\n    if epochs_no_improve >= patience:\n        print(f\"Early stopping triggered after {epoch+1} epochs.\")\n        break\n\n# Load the best model for evaluation\nmodel.load_state_dict(torch.load('best_korean_kws_pytorch_model.pth'))\n\n\n# --- Plot training results ---\nplt.figure(figsize=(16, 6))\nplt.subplot(1, 2, 1)\nplt.plot(range(len(history['loss'])), history['loss'], label='loss')\nplt.plot(range(len(history['val_loss'])), history['val_loss'], label='val_loss')\nplt.legend()\nplt.ylim([0, max(plt.ylim()) if plt.ylim()[1] > 0 else 1]) # Adjust y-lim\nplt.xlabel('Epoch')\nplt.ylabel('Loss [CrossEntropy]')\n\nplt.subplot(1, 2, 2)\nplt.plot(range(len(history['accuracy'])), [acc * 100 for acc in history['accuracy']], label='accuracy')\nplt.plot(range(len(history['val_accuracy'])), [acc * 100 for acc in history['val_accuracy']], label='val_accuracy')\nplt.legend()\nplt.ylim([0, 100])\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy [%]')\nplt.show()\n\n\n# --- Evaluate on Test Set ---\nmodel.eval()\ntest_loss = 0.0\ncorrect_test = 0\ntotal_test = 0\nall_preds = []\nall_true = []\n\nwith torch.no_grad():\n    for inputs, labels in test_loader:\n        inputs, labels = inputs.to(device), labels.to(device)\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        test_loss += loss.item()\n        _, predicted = torch.max(outputs.data, 1)\n        total_test += labels.size(0)\n        correct_test += (predicted == labels).sum().item()\n        all_preds.extend(predicted.cpu().numpy())\n        all_true.extend(labels.cpu().numpy())\n\navg_test_loss = test_loss / len(test_loader)\ntest_accuracy = 100 * correct_test / total_test\nprint(f\"Test Loss: {avg_test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%\")\n\n\n# --- Confusion Matrix ---\nif total_test > 0 : # Only plot if test set is not empty\n    confusion_mtx = tf.math.confusion_matrix(all_true, all_preds) # Using tf for consistency with original notebook\n    plt.figure(figsize=(12, 10))\n    sns.heatmap(confusion_mtx,\n                xticklabels=class_names, # Use PyTorch derived class_names\n                yticklabels=class_names,\n                annot=True, fmt='g')\n    plt.xlabel('Prediction')\n    plt.ylabel('Label')\n    plt.show()\nelse:\n    print(\"Test dataset is empty, skipping confusion matrix.\")\n\n\n# --- Inference on a Sample File ---\n# Create a function for inference like in the TF notebook\ndef infer_pytorch_model(model_to_infer, audio_filepath_str, class_idx_to_name_map):\n    model_to_infer.eval() # Ensure model is in eval mode\n    # Load and preprocess the single audio file\n    waveform, sr = torchaudio.load(audio_filepath_str)\n    if sr != SAMPLE_RATE:\n        resampler = T.Resample(sr, SAMPLE_RATE)\n        waveform = resampler(waveform)\n    if waveform.shape[0] > 1: waveform = torch.mean(waveform, dim=0, keepdim=True)\n    \n    current_len = waveform.shape[1]\n    if current_len < TARGET_SEQ_LEN_SAMPLES:\n        padding = TARGET_SEQ_LEN_SAMPLES - current_len\n        waveform = torch.nn.functional.pad(waveform, (0, padding))\n    elif current_len > TARGET_SEQ_LEN_SAMPLES:\n        waveform = waveform[:, :TARGET_SEQ_LEN_SAMPLES]\n\n    waveform_np = waveform.squeeze().numpy()\n    mel_spec = librosa.feature.melspectrogram(y=waveform_np, sr=SAMPLE_RATE,\n                                              n_fft=N_FFT, hop_length=HOP_LENGTH,\n                                              n_mels=N_MELS)\n    log_mel_spec = librosa.power_to_db(mel_spec, ref=np.max)\n    log_mel_spec_tensor = torch.from_numpy(log_mel_spec).float().unsqueeze(0) # [1, N_MELS, actual_frames]\n    \n    if log_mel_spec_tensor.shape[2] != TARGET_FRAMES:\n        resized_spec = torch.nn.functional.interpolate(\n            log_mel_spec_tensor.unsqueeze(0), # Add batch: [1, 1, N_MELS, actual_frames]\n            size=(N_MELS, TARGET_FRAMES),     # Target H, W\n            mode='bilinear',\n            align_corners=False\n        ).squeeze(0) # Remove batch: [1, N_MELS, TARGET_FRAMES]\n    else:\n        resized_spec = log_mel_spec_tensor\n\n    input_tensor = resized_spec.unsqueeze(0).to(device) # Add batch dim back: [1, 1, N_MELS, TARGET_FRAMES]\n\n    with torch.no_grad():\n        logits = model_to_infer(input_tensor)\n        probabilities = torch.softmax(logits, dim=1)\n        predicted_idx = torch.argmax(probabilities, dim=1).item()\n        predicted_label = class_idx_to_name_map[predicted_idx]\n\n    print(f\"\\nInference on: {audio_filepath_str}\")\n    print(f\"Predicted class ID: {predicted_idx}, Predicted Label: {predicted_label}\")\n    \n    plt.figure(figsize=(8,6))\n    plt.bar(class_idx_to_name_map.values(), probabilities.squeeze().cpu().numpy())\n    plt.title(f'Prediction for: {os.path.basename(audio_filepath_str)} -> {predicted_label}')\n    plt.xticks(rotation=90)\n    plt.ylabel(\"Probability\")\n    plt.show()\n    \n    # Display audio for verification\n    display.display(display.Audio(waveform_np, rate=SAMPLE_RATE)) # Display original (padded/trimmed) waveform\n\n# Example usage of inference function\n# Make sure this file exists in your dataset structure\nsample_path_str = str(data_dir / class_names[0] / os.listdir(data_dir / class_names[0])[0]) # Takes the first file from the first class\nif os.path.exists(sample_path_str):\n    infer_pytorch_model(model, sample_path_str, idx_to_class)\nelse:\n    print(f\"Sample audio file for inference not found at: {sample_path_str}\")\n\n\n# --- Export to ONNX ---\n# Load the best model weights\nmodel.load_state_dict(torch.load('best_korean_kws_pytorch_model.pth'))\nmodel.eval() # IMPORTANT: Set to evaluation mode before export\n\n# Create a dummy input with the correct shape [Batch, Channels, Height, Width]\n# This MUST match the input shape your model expects.\n# For our SimpleCNNKWS, input is [Batch, 1, N_MELS, TARGET_FRAMES]\ndummy_input = torch.randn(1, 1, N_MELS, TARGET_FRAMES).to(device)\nonnx_model_path = \"batch8korean_kws_pytorch.onnx\"\n\ntry:\n    torch.onnx.export(model,                        # model being run\n                      dummy_input,                  # model input (or a tuple for multiple inputs)\n                      onnx_model_path,              # where to save the model\n                      export_params=True,           # store the trained parameter weights inside the model file\n                      opset_version=13,             # the ONNX version to export the model to\n                      do_constant_folding=True,     # whether to execute constant folding for optimization\n                      input_names = ['input_spectrogram'], # model's input names\n                      output_names = ['output_logits'],   # model's output names\n                      dynamic_axes={'input_spectrogram' : {0 : 'batch_size'}, # variable length axes\n                                    'output_logits' : {0 : 'batch_size'}})\n    print(f\"Model successfully exported to ONNX: {onnx_model_path}\")\nexcept Exception as e:\n    print(f\"Error during ONNX export: {e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-20T05:05:46.462816Z","iopub.execute_input":"2025-06-20T05:05:46.463146Z","iopub.status.idle":"2025-06-20T05:05:46.477534Z","shell.execute_reply.started":"2025-06-20T05:05:46.463120Z","shell.execute_reply":"2025-06-20T05:05:46.476922Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Wav2vec2 custom","metadata":{}},{"cell_type":"code","source":"!pip uninstall -y transformers tokenizers safetensors accelerate\n!pip install -U transformers tokenizers safetensors accelerate","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T10:32:40.255726Z","iopub.execute_input":"2025-06-21T10:32:40.255986Z","iopub.status.idle":"2025-06-21T10:34:23.739137Z","shell.execute_reply.started":"2025-06-21T10:32:40.255966Z","shell.execute_reply":"2025-06-21T10:34:23.738154Z"}},"outputs":[{"name":"stdout","text":"Found existing installation: transformers 4.51.3\nUninstalling transformers-4.51.3:\n  Successfully uninstalled transformers-4.51.3\nFound existing installation: tokenizers 0.21.1\nUninstalling tokenizers-0.21.1:\n  Successfully uninstalled tokenizers-0.21.1\nFound existing installation: safetensors 0.5.3\nUninstalling safetensors-0.5.3:\n  Successfully uninstalled safetensors-0.5.3\nFound existing installation: accelerate 1.5.2\nUninstalling accelerate-1.5.2:\n  Successfully uninstalled accelerate-1.5.2\nCollecting transformers\n  Downloading transformers-4.52.4-py3-none-any.whl.metadata (38 kB)\nCollecting tokenizers\n  Downloading tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\nCollecting safetensors\n  Downloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\nCollecting accelerate\n  Downloading accelerate-1.8.1-py3-none-any.whl.metadata (19 kB)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.31.1)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (7.0.0)\nRequirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.6.0+cu124)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.0.0->accelerate)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.0.0->accelerate)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.0.0->accelerate)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.0.0->accelerate)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.0.0->accelerate)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.0.0->accelerate)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\nDownloading transformers-4.52.4-py3-none-any.whl (10.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.5/10.5 MB\u001b[0m \u001b[31m92.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m85.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (471 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m471.6/471.6 kB\u001b[0m \u001b[31m35.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading accelerate-1.8.1-py3-none-any.whl (365 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m365.3/365.3 kB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0mm\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m56.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: safetensors, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, tokenizers, nvidia-cusolver-cu12, transformers, accelerate\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.9.41\n    Uninstalling nvidia-nvjitlink-cu12-12.9.41:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.9.41\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.10.19\n    Uninstalling nvidia-curand-cu12-10.3.10.19:\n      Successfully uninstalled nvidia-curand-cu12-10.3.10.19\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.4.0.6\n    Uninstalling nvidia-cufft-cu12-11.4.0.6:\n      Successfully uninstalled nvidia-cufft-cu12-11.4.0.6\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.9.0.13\n    Uninstalling nvidia-cublas-cu12-12.9.0.13:\n      Successfully uninstalled nvidia-cublas-cu12-12.9.0.13\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.9.5\n    Uninstalling nvidia-cusparse-cu12-12.5.9.5:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.9.5\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.7.4.40\n    Uninstalling nvidia-cusolver-cu12-11.7.4.40:\n      Successfully uninstalled nvidia-cusolver-cu12-11.7.4.40\nSuccessfully installed accelerate-1.8.1 nvidia-cublas-cu12-12.4.5.8 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 safetensors-0.5.3 tokenizers-0.21.1 transformers-4.52.4\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import gdown\nimport zipfile\nimport os\n\n# The Google Drive file ID from your link\n# Your link: https://drive.google.com/file/d/1tkqNh_E6yjN3uOyQmf7spQeURm2_b26h/view?usp=sharing\n#            https://drive.google.com/file/d/1CRLxg3tbEh9CTG3oIAPSksz8Pwyjq0SL/view?usp=sharing\n# The ID is: 1tkqNh_E6yjN3uOyQmf7spQeURm2_b26h\n\nfile_id = '1CRLxg3tbEh9CTG3oIAPSksz8Pwyjq0SL'\noutput_zip_path = 'downloaded_dataset.zip'\nextract_to_path = '/kaggle/working/korean_speech_commands_kaggle' \n\n# Download the file\nprint(f\"Downloading file with ID: {file_id} to {output_zip_path}...\")\ngdown.download(id=file_id, output=output_zip_path, quiet=False)\n\nif os.path.exists(output_zip_path):\n    print(f\"Download complete: {output_zip_path}\")\n\n    # Create extraction directory if it doesn't exist\n    if not os.path.exists(extract_to_path):\n        os.makedirs(extract_to_path)\n        print(f\"Created directory: {extract_to_path}\")\n\n    # Extract the zip file\n    print(f\"Extracting {output_zip_path} to {extract_to_path}...\")\n    try:\n        with zipfile.ZipFile(output_zip_path, 'r') as zip_ref:\n            zip_ref.extractall(extract_to_path)\n        print(\"Extraction complete.\")\n        \n        # List the contents of the extracted directory (optional, for verification)\n        print(f\"\\nContents of {extract_to_path}:\")\n        for item in os.listdir(extract_to_path):\n            print(f\"- {item}\")\n            # If your zip file has an intermediate folder, you might need to list inside that\n            # For example, if it extracts to 'extract_to_path/only_command/'\n            if os.path.isdir(os.path.join(extract_to_path, item)):\n                 print(f\"  Contents of {item}: {os.listdir(os.path.join(extract_to_path, item))[:5]}...\")\n\n\n    except zipfile.BadZipFile:\n        print(f\"Error: The downloaded file '{output_zip_path}' is not a valid zip file or is corrupted.\")\n    except Exception as e:\n        print(f\"An error occurred during extraction: {e}\")\n    \n    # Clean up the downloaded zip file (optional)\n    # os.remove(output_zip_path)\n    # print(f\"Removed downloaded zip file: {output_zip_path}\")\nelse:\n    print(f\"Error: Download failed. File '{output_zip_path}' not found.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T10:34:23.741215Z","iopub.execute_input":"2025-06-21T10:34:23.741530Z","iopub.status.idle":"2025-06-21T10:34:33.015290Z","shell.execute_reply.started":"2025-06-21T10:34:23.741498Z","shell.execute_reply":"2025-06-21T10:34:33.014570Z"}},"outputs":[{"name":"stdout","text":"Downloading file with ID: 1CRLxg3tbEh9CTG3oIAPSksz8Pwyjq0SL to downloaded_dataset.zip...\n","output_type":"stream"},{"name":"stderr","text":"Downloading...\nFrom (original): https://drive.google.com/uc?id=1CRLxg3tbEh9CTG3oIAPSksz8Pwyjq0SL\nFrom (redirected): https://drive.google.com/uc?id=1CRLxg3tbEh9CTG3oIAPSksz8Pwyjq0SL&confirm=t&uuid=bc439c41-70e8-4814-9875-53700a7f8026\nTo: /kaggle/working/downloaded_dataset.zip\n100%|██████████| 194M/194M [00:03<00:00, 48.7MB/s] \n","output_type":"stream"},{"name":"stdout","text":"Download complete: downloaded_dataset.zip\nCreated directory: /kaggle/working/korean_speech_commands_kaggle\nExtracting downloaded_dataset.zip to /kaggle/working/korean_speech_commands_kaggle...\nExtraction complete.\n\nContents of /kaggle/working/korean_speech_commands_kaggle:\n- all_comand\n  Contents of all_comand: ['느리게', '전방', '3미터', '3시', '400미터']...\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"from datasets import load_dataset, Audio, DatasetDict\nimport os\nimport pathlib\nimport numpy as np\n\n# --- Configuration ---\nDATASET_PATH = '/kaggle/working/korean_speech_commands_kaggle/all_comand' # Your dataset root\n# MODEL_CHECKPOINT = \"facebook/wav2vec2-base-960h\" # Or a Korean Wav2Vec2 model\n# For Korean model, e.g.:\nMODEL_CHECKPOINT = \"kresnik/wav2vec2-large-xlsr-korean\"\n\nTARGET_SAMPLE_RATE = 16_000\nMAX_AUDIO_SECONDS = 4 # Based on your previous info\nTARGET_SEQ_LEN_SAMPLES = MAX_AUDIO_SECONDS * TARGET_SAMPLE_RATE\n\n# --- Load your dataset ---\n# This creates a dataset where each row has 'audio' (with path, array, sr) and 'label' (int)\n# Option A: Using Hugging Face `load_dataset` with `audiofolder`\n# Your data_dir should point to the directory containing the class folders\ndata_dir = pathlib.Path(DATASET_PATH)\ndataset = load_dataset(\"audiofolder\", data_dir=str(data_dir), drop_labels=False) # drop_labels=False to keep folder names as string labels initially\n\n# The 'label' column will now have string labels (your folder names)\n# We need to convert them to integer IDs\nclass_names = sorted(list(set(dataset['train']['label']))) # Get unique folder names\nnum_classes = len(class_names)\nlabel_to_id = {name: i for i, name in enumerate(class_names)}\nid_to_label = {i: name for i, name in enumerate(class_names)}\n\nprint(f\"Found {num_classes} classes: {class_names}\")\n\ndef map_label_to_id(example):\n    example[\"label\"] = label_to_id[example[\"label\"]]\n    return example\n\ndataset = dataset.map(map_label_to_id)\nprint(\"Dataset example after label mapping:\", dataset['train'][0])\n\n\n# --- Preprocess Audio (Resample and ensure fixed length) ---\n# Cast the 'audio' column to the desired sampling rate\ndataset = dataset.cast_column(\"audio\", Audio(sampling_rate=TARGET_SAMPLE_RATE))\n\ndef preprocess_audio(batch):\n    # The \"audio\" column now contains a dict with \"array\", \"path\", \"sampling_rate\"\n    # We want the \"array\" (the waveform)\n    audio_arrays = [x[\"array\"] for x in batch[\"audio\"]]\n    \n    # Pad or truncate each waveform to TARGET_SEQ_LEN_SAMPLES\n    processed_waveforms = []\n    for waveform in audio_arrays:\n        current_len = len(waveform)\n        if current_len < TARGET_SEQ_LEN_SAMPLES:\n            padding = TARGET_SEQ_LEN_SAMPLES - current_len\n            # Pad with zeros at the end\n            padded_waveform = np.pad(waveform, (0, padding), mode='constant')\n        elif current_len > TARGET_SEQ_LEN_SAMPLES:\n            padded_waveform = waveform[:TARGET_SEQ_LEN_SAMPLES]\n        else:\n            padded_waveform = waveform\n        processed_waveforms.append(padded_waveform)\n    \n    batch[\"input_values\"] = processed_waveforms # This will be fed to Wav2Vec2 processor\n    return batch\n\ndataset = dataset.map(preprocess_audio, batched=True, remove_columns=[\"audio\"])\nprint(\"Dataset example after audio preprocessing:\", dataset['train'][0].keys())\nprint(\"Input values shape for one example:\", np.array(dataset['train'][0]['input_values']).shape)\n\n\n# --- Split dataset (if not already split, e.g., 'train' is all your data) ---\n# If your `load_dataset(\"audiofolder\", ...)` only created a 'train' split:\nif 'test' not in dataset:\n    train_test_split = dataset['train'].train_test_split(test_size=0.2) # 20% for test/eval\n    dataset_dict = DatasetDict({\n        'train': train_test_split['train'],\n        'test': train_test_split['test'] # Use this for evaluation\n    })\nelse: # If load_dataset created train/test splits (e.g. if you had subfolders like train/test in DATASET_PATH)\n    dataset_dict = dataset\n\ntrain_dataset = dataset_dict[\"train\"]\neval_dataset = dataset_dict[\"test\"] # Or create a separate validation split from train if needed\n\nprint(f\"Train dataset size: {len(train_dataset)} samples\")\nprint(f\"Eval dataset size: {len(eval_dataset)} samples\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T09:46:44.753126Z","iopub.execute_input":"2025-06-21T09:46:44.753583Z","iopub.status.idle":"2025-06-21T09:47:16.975068Z","shell.execute_reply.started":"2025-06-21T09:46:44.753560Z","shell.execute_reply":"2025-06-21T09:47:16.974304Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Resolving data files:   0%|          | 0/3740 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9d190234df1c43299f9f4e1379049e62"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0/3740 [00:00<?, ?files/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ac3ad87a3a4644989ad214ad6ba92283"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b1c124b2e82b4cd0b315152f99edce69"}},"metadata":{}},{"name":"stdout","text":"Found 50 classes: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/3740 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ad9a767cab524b67bd8444739883422b"}},"metadata":{}},{"name":"stdout","text":"Dataset example after label mapping: {'audio': {'path': '/kaggle/working/korean_speech_commands_kaggle/all_comand/100미터/aug_clean_100meters_10_noisy.wav', 'array': array([ 0.01220703, -0.00802612, -0.00448608, ..., -0.00415039,\n       -0.00494385, -0.00073242]), 'sampling_rate': 16000}, 'label': 0}\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/3740 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"581de888302c468fa14d0171f9a7dd27"}},"metadata":{}},{"name":"stdout","text":"Dataset example after audio preprocessing: dict_keys(['label', 'input_values'])\nInput values shape for one example: (64000,)\nTrain dataset size: 2992 samples\nEval dataset size: 748 samples\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# from transformers import AutoProcessor, Wav2Vec2Model # Or AutoModel\nfrom transformers import AutoProcessor, Wav2Vec2Model\nimport torch\nimport torch.nn as nn\n\nprocessor = AutoProcessor.from_pretrained(MODEL_CHECKPOINT)\n# Load only the base Wav2Vec2 model (without the CTC head for ASR transcription)\nwav2vec2_base_model = Wav2Vec2Model.from_pretrained(MODEL_CHECKPOINT) \n# OR use AutoModel for more flexibility if checkpoint is not specifically Wav2Vec2\n# wav2vec2_base_model = AutoModel.from_pretrained(MODEL_CHECKPOINT)\n\n# --- Define your KWS Classification Model ---\n# --- Define your KWS Classification Model ---\nclass KWSModel(nn.Module):\n    def __init__(self, base_model, num_classes, freeze_feature_extractor=True):\n        super().__init__()\n        self.wav2vec2 = base_model\n        \n        if freeze_feature_extractor:\n            for param in self.wav2vec2.parameters():\n                param.requires_grad = False\n        \n        self.classifier_input_dim = self.wav2vec2.config.hidden_size\n        if hasattr(self.wav2vec2.config, 'project_hid') and self.wav2vec2.config.project_hid is not None:\n             self.classifier_input_dim = self.wav2vec2.config.project_hid\n\n        self.classifier = nn.Sequential(\n            nn.Linear(self.classifier_input_dim, 256),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(256, num_classes)\n        )\n\n    # MODIFICATION HERE: Add 'labels=None' to the arguments\n    def forward(self, input_values, attention_mask=None, labels=None): \n        outputs = self.wav2vec2(input_values, attention_mask=attention_mask)\n        hidden_states = outputs.last_hidden_state\n        \n        if attention_mask is not None:\n            expanded_mask = attention_mask.unsqueeze(-1)\n            sum_hidden_states = (hidden_states * expanded_mask).sum(dim=1)\n            sum_mask = expanded_mask.sum(dim=1)\n            sum_mask = torch.clamp(sum_mask, min=1e-9)\n            pooled_output = sum_hidden_states / sum_mask\n        else:\n            pooled_output = torch.mean(hidden_states, dim=1)\n            \n        logits = self.classifier(pooled_output)\n\n        # The Trainer will compute loss if labels are provided and logits are returned.\n        # If you want your model to compute loss internally when labels are present:\n        loss = None\n        if labels is not None:\n            loss_fct = nn.CrossEntropyLoss()\n            loss = loss_fct(logits.view(-1, self.config.num_labels if hasattr(self, 'config') else logits.shape[-1]), labels.view(-1)) # Ensure num_labels is correct\n\n        # Return a tuple or an object similar to Hugging Face model outputs\n        # For Trainer to compute loss, just returning logits is fine if labels are passed.\n        # If you compute loss here, return it as well.\n        if loss is not None:\n            # For Hugging Face models, they often return a custom output object\n            # For simplicity, let's return a tuple (loss, logits)\n            return (loss, logits)\n        else:\n            return logits # If no labels, just return logits\n\n# Instantiate your KWS model\nmodel = KWSModel(wav2vec2_base_model, num_classes=num_classes, freeze_feature_extractor=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T09:47:54.503415Z","iopub.execute_input":"2025-06-21T09:47:54.503880Z","iopub.status.idle":"2025-06-21T09:48:24.491269Z","shell.execute_reply.started":"2025-06-21T09:47:54.503856Z","shell.execute_reply":"2025-06-21T09:48:24.490494Z"}},"outputs":[{"name":"stderr","text":"2025-06-21 09:48:03.022358: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1750499283.198574      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1750499283.252092      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/214 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ee608c9a9dbf4476891fda93c37796d3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/161 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"394e55d9b7fe48ecb57b590cbe61730c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/2.31k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a68461329f514cd0a1f228cdd5a1aa70"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/18.2k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5b307b51808b491fbf34d058c8b4a4d3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.27G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aaea3d51c79d445b97bb873f0bb92740"}},"metadata":{}},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"KWSModel(\n  (wav2vec2): Wav2Vec2Model(\n    (feature_extractor): Wav2Vec2FeatureEncoder(\n      (conv_layers): ModuleList(\n        (0): Wav2Vec2LayerNormConvLayer(\n          (conv): Conv1d(1, 512, kernel_size=(10,), stride=(5,))\n          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (activation): GELUActivation()\n        )\n        (1-4): 4 x Wav2Vec2LayerNormConvLayer(\n          (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,))\n          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (activation): GELUActivation()\n        )\n        (5-6): 2 x Wav2Vec2LayerNormConvLayer(\n          (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,))\n          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (activation): GELUActivation()\n        )\n      )\n    )\n    (feature_projection): Wav2Vec2FeatureProjection(\n      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n      (projection): Linear(in_features=512, out_features=1024, bias=True)\n      (dropout): Dropout(p=0.0, inplace=False)\n    )\n    (encoder): Wav2Vec2EncoderStableLayerNorm(\n      (pos_conv_embed): Wav2Vec2PositionalConvEmbedding(\n        (conv): ParametrizedConv1d(\n          1024, 1024, kernel_size=(128,), stride=(1,), padding=(64,), groups=16\n          (parametrizations): ModuleDict(\n            (weight): ParametrizationList(\n              (0): _WeightNorm()\n            )\n          )\n        )\n        (padding): Wav2Vec2SamePadLayer()\n        (activation): GELUActivation()\n      )\n      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n      (layers): ModuleList(\n        (0-23): 24 x Wav2Vec2EncoderLayerStableLayerNorm(\n          (attention): Wav2Vec2SdpaAttention(\n            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          )\n          (dropout): Dropout(p=0.1, inplace=False)\n          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          (feed_forward): Wav2Vec2FeedForward(\n            (intermediate_dropout): Dropout(p=0.0, inplace=False)\n            (intermediate_dense): Linear(in_features=1024, out_features=4096, bias=True)\n            (intermediate_act_fn): GELUActivation()\n            (output_dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (output_dropout): Dropout(p=0.1, inplace=False)\n          )\n          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n    )\n  )\n  (classifier): Sequential(\n    (0): Linear(in_features=1024, out_features=256, bias=True)\n    (1): ReLU()\n    (2): Dropout(p=0.3, inplace=False)\n    (3): Linear(in_features=256, out_features=50, bias=True)\n  )\n)"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"def prepare_dataset_for_kws(batch):\n    # Process the audio waveforms\n    # The processor handles normalization and potentially other steps\n    processed = processor(batch[\"input_values\"], sampling_rate=TARGET_SAMPLE_RATE, return_tensors=\"pt\", padding=\"longest\")\n    # 'pt' for PyTorch tensors. 'padding=\"longest\"' will pad sequences in batch to max length in batch.\n    \n    batch[\"input_values\"] = processed.input_values # This is what the model expects\n    batch[\"attention_mask\"] = processed.attention_mask # Important for handling padding\n    # batch[\"labels\"] is already the integer class ID\n    return batch\n\n# Apply this to your datasets\n# Note: This processing is now done on-the-fly by the DataCollator usually,\n# or you can pre-process. For simplicity with the Trainer, let's make a DataCollator.\n# If pre-processing, be careful with batching as padding=\"longest\" works per batch.\n# For now, let's design the data collator to do this.wav2vec2-large-ko\n# So, the dataset should still just contain the raw (but fixed-length) \"input_values\" and \"label\".","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T09:48:53.513608Z","iopub.execute_input":"2025-06-21T09:48:53.514868Z","iopub.status.idle":"2025-06-21T09:48:53.520486Z","shell.execute_reply.started":"2025-06-21T09:48:53.514835Z","shell.execute_reply":"2025-06-21T09:48:53.519638Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"import torch\nfrom dataclasses import dataclass\nfrom typing import Any, Dict, List, Union, Optional\n\n@dataclass\nclass KWSDataCollator:\n    processor: Any # Should be your Wav2Vec2Processor\n    padding: Union[bool, str] = \"longest\" # Pad to longest in batch\n    max_length: Optional[int] = None      # Max length for padding/truncation\n    pad_to_multiple_of: Optional[int] = None \n\n    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n        input_features = [{\"input_values\": feature[\"input_values\"]} for feature in features]\n        label_features = [feature[\"label\"] for feature in features] # Simple list of integer labels\n\n        # Process audio input_values. This handles normalization and padding within the batch.\n        batch = self.processor.pad(\n            input_features,\n            padding=self.padding,\n            max_length=self.max_length,\n            pad_to_multiple_of=self.pad_to_multiple_of,\n            return_tensors=\"pt\", # Return PyTorch tensors\n        )\n\n        # Labels are just integers for classification\n        batch[\"labels\"] = torch.tensor(label_features, dtype=torch.long)\n        \n        return batch\n\ndata_collator = KWSDataCollator(processor=processor)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T09:49:17.134300Z","iopub.execute_input":"2025-06-21T09:49:17.134579Z","iopub.status.idle":"2025-06-21T09:49:17.142340Z","shell.execute_reply.started":"2025-06-21T09:49:17.134560Z","shell.execute_reply":"2025-06-21T09:49:17.141215Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"!pip install -q evaluate","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T09:49:25.833910Z","iopub.execute_input":"2025-06-21T09:49:25.834258Z","iopub.status.idle":"2025-06-21T09:49:28.984688Z","shell.execute_reply.started":"2025-06-21T09:49:25.834227Z","shell.execute_reply":"2025-06-21T09:49:28.983671Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"from transformers import TrainingArguments, Trainer\n# You'll also need to define a compute_metrics function for evaluation\nimport numpy as np\n# from datasets import load_metric\nimport evaluate \n\n# accuracy_metric = load_metric(\"accuracy\")\naccuracy_metric = evaluate.load(\"accuracy\") # MODIFY THIS LINE\n\ndef compute_metrics(eval_pred):\n    predictions, labels = eval_pred\n    predictions = np.argmax(predictions, axis=1)\n    return accuracy_metric.compute(predictions=predictions, references=labels)\n\n\nfrom transformers import TrainingArguments\n\ntraining_args = TrainingArguments(\n    output_dir=\"/kaggle/working/korean_kws_wav2vec3\",\n    # group_by_length=False, # Can often be omitted if inputs are consistently processed\n    per_device_train_batch_size=4,\n    per_device_eval_batch_size=4,\n    gradient_accumulation_steps=2,\n    # evaluation_strategy=\"steps\", # REMOVE THIS\n    eval_strategy=\"steps\",        # TRY THIS or \"epoch\"\n    eval_steps=200,               # Keep this if you want step-based evaluation\n    num_train_epochs=1,\n    fp16=torch.cuda.is_available(),\n    save_steps=200,\n    logging_steps=50,\n    learning_rate=3e-5,\n    warmup_steps=100,\n    save_total_limit=2,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"accuracy\",\n    report_to=\"tensorboard\",\n    # remove_unused_columns=False # This is important, keep it or set explicitly\n)\n\ntrainer = Trainer(\n    model=model, # Your KWSModel instance\n    data_collator=data_collator,\n    args=training_args,\n    compute_metrics=compute_metrics,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset, # This will be used as the evaluation set\n    tokenizer=processor.feature_extractor, # For logging purposes, not strictly used by KWSModel directly for tokenizing text\n)\n\ntrainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T09:49:39.868040Z","iopub.execute_input":"2025-06-21T09:49:39.868796Z","iopub.status.idle":"2025-06-21T09:49:45.864224Z","shell.execute_reply.started":"2025-06-21T09:49:39.868756Z","shell.execute_reply":"2025-06-21T09:49:45.863046Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/4.20k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"67adcea405d443a4aa1d663222bc8b76"}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_35/175881537.py:40: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_35/175881537.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     48\u001b[0m )\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2238\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2239\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2240\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2241\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2242\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2553\u001b[0m                     )\n\u001b[1;32m   2554\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2555\u001b[0;31m                         \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2556\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2557\u001b[0m                     if (\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3743\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3744\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss_context_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3745\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3746\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3747\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3808\u001b[0m                 \u001b[0mloss_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"num_items_in_batch\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3809\u001b[0m             \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mloss_kwargs\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3810\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3811\u001b[0m         \u001b[0;31m# Save past state if it exists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3812\u001b[0m         \u001b[0;31m# TODO: this needs to be fixed and made cleaner later.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/data_parallel.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    191\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mmodule_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m             \u001b[0mreplicas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/data_parallel.py\u001b[0m in \u001b[0;36mparallel_apply\u001b[0;34m(self, replicas, inputs, kwargs)\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplicas\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m     ) -> List[Any]:\n\u001b[0;32m--> 212\u001b[0;31m         return parallel_apply(\n\u001b[0m\u001b[1;32m    213\u001b[0m             \u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/parallel_apply.py\u001b[0m in \u001b[0;36mparallel_apply\u001b[0;34m(modules, inputs, kwargs_tup, devices)\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m             \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m         \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    731\u001b[0m             \u001b[0;31m# instantiate since we don't know how to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 733\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    734\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    735\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Caught RuntimeError in replica 0 on device 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/parallel_apply.py\", line 96, in _worker\n    output = module(*input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/tmp/ipykernel_35/2969675459.py\", line 41, in forward\n    sum_hidden_states = (hidden_states * expanded_mask).sum(dim=1)\n                         ~~~~~~~~~~~~~~^~~~~~~~~~~~~~~\nRuntimeError: The size of tensor a (199) must match the size of tensor b (64000) at non-singleton dimension 1\n"],"ename":"RuntimeError","evalue":"Caught RuntimeError in replica 0 on device 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/parallel_apply.py\", line 96, in _worker\n    output = module(*input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/tmp/ipykernel_35/2969675459.py\", line 41, in forward\n    sum_hidden_states = (hidden_states * expanded_mask).sum(dim=1)\n                         ~~~~~~~~~~~~~~^~~~~~~~~~~~~~~\nRuntimeError: The size of tensor a (199) must match the size of tensor b (64000) at non-singleton dimension 1\n","output_type":"error"}],"execution_count":9},{"cell_type":"code","source":"metrics = trainer.evaluate(eval_dataset) # Evaluate on your eval_dataset\nprint(metrics)\n\n# Save the fine-tuned KWS model (the whole thing, including Wav2Vec2 base)\ntrainer.save_model(\"/kaggle/working/korean_kws_wav2vec2\")\n# Save the processor too, as it's needed for preprocessing during inference\nprocessor.save_pretrained(\"/kaggle/working/korean_kws_wav2vec2\")\n# Save your label mapping\nimport json\nwith open(\"/kaggle/working/korean_kws_wav2vec2/label_mapping.json\", \"w\") as f:\n    json.dump({\"id_to_label\": id_to_label, \"label_to_id\": label_to_id}, f)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-20T11:39:58.305908Z","iopub.execute_input":"2025-06-20T11:39:58.306566Z","iopub.status.idle":"2025-06-20T11:40:32.778519Z","shell.execute_reply.started":"2025-06-20T11:39:58.306542Z","shell.execute_reply":"2025-06-20T11:40:32.777892Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'eval_loss': 3.916285991668701, 'eval_accuracy': 0.022727272727272728, 'eval_runtime': 33.6999, 'eval_samples_per_second': 22.196, 'eval_steps_per_second': 2.789, 'epoch': 1.0}\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"import torch\nfrom transformers import AutoModel # Or Wav2Vec2Model if you used that directly\nimport pickle\nimport os\nimport torch\n# Ensure your KWSModel class definition is available\n# class KWSModel(nn.Module):\n#     ... (your definition) ...\nfrom transformers import AutoModel # Or Wav2Vec2Model\n# Ensure MODEL_CHECKPOINT, num_classes, device are defined\n\n# --- Configuration ---\n# MODEL_CHECKPOINT = \"facebook/wav2vec2-base-960h\" # Or your Korean base model\n# num_classes = len(class_names) \n# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nSAVED_MODEL_DIR = \"/kaggle/working/korean_kws_wav2vec2/checkpoint-187\"\n\n# 1. Re-instantiate your KWSModel\nbase_model_for_onnx = AutoModel.from_pretrained(MODEL_CHECKPOINT)\nmodel_for_onnx = KWSModel(base_model_for_onnx, num_classes=num_classes, freeze_feature_extractor=False)\n\nweights_path_safetensors = os.path.join(SAVED_MODEL_DIR, \"model.safetensors\")\nweights_path_bin = os.path.join(SAVED_MODEL_DIR, \"pytorch_model.bin\") # Older format\n\nif os.path.exists(weights_path_safetensors):\n    print(f\"Found model.safetensors. Attempting to load with safetensors library...\")\n    try:\n        from safetensors.torch import load_file\n        state_dict = load_file(weights_path_safetensors, device=str(device)) # Use str(device) for safetensors\n        model_for_onnx.load_state_dict(state_dict)\n        print(\"Successfully loaded KWSModel state_dict using safetensors.torch.load_file\")\n    except ImportError:\n        print(\"ERROR: `safetensors` library not installed. Please install with: !pip install safetensors\")\n        print(\"Cannot load .safetensors file without it.\")\n        raise\n    except Exception as e:\n        print(f\"ERROR: Failed to load state_dict from .safetensors file using safetensors.torch.load_file: {e}\")\n        raise\n\nelif os.path.exists(weights_path_bin):\n    print(f\"Found pytorch_model.bin. Attempting to load with torch.load...\")\n    try:\n        # For .bin files, weights_only=False is often necessary if they contain pickled data\n        state_dict = torch.load(weights_path_bin, map_location=device, weights_only=False)\n        model_for_onnx.load_state_dict(state_dict)\n        print(\"Successfully loaded KWSModel state_dict from pytorch_model.bin (with weights_only=False)\")\n    except Exception as e:\n        print(f\"ERROR: Failed to load state_dict from .bin file using torch.load: {e}\")\n        raise\nelse:\n    raise FileNotFoundError(f\"Could not find model weights (model.safetensors or pytorch_model.bin) in {SAVED_MODEL_DIR}\")\n\nmodel_for_onnx.to(device)\nmodel_for_onnx.eval()\n# Prepare a dummy input that goes through your Wav2Vec2Processor\n# This processor should be the one you used for training/data collation\n# processor = AutoProcessor.from_pretrained(MODEL_CHECKPOINT) # Or load from SAVED_MODEL_DIR if saved\nprocessor_for_onnx = AutoProcessor.from_pretrained(SAVED_MODEL_DIR)\n\n\ndummy_waveform_list = [torch.randn(TARGET_SEQ_LEN_SAMPLES).tolist()] # Processor expects list of lists or list of np arrays\nprocessed_dummy = processor_for_onnx(dummy_waveform_list, \n                                 sampling_rate=TARGET_SAMPLE_RATE, \n                                 return_tensors=\"pt\", \n                                 padding=\"longest\") # Use padding like in collator\n\ndummy_input_values = processed_dummy.input_values.to(device)\ndummy_attention_mask = processed_dummy.attention_mask.to(device)\n\nonnx_model_path = \"/kaggle/working/korean_kws_wav2vec2.onnx\"\n\nprint(f\"Dummy input_values shape for ONNX export: {dummy_input_values.shape}\")\nprint(f\"Dummy attention_mask shape for ONNX export: {dummy_attention_mask.shape}\")\n\n\ntry:\n    torch.onnx.export(model_for_onnx,\n                      args=(dummy_input_values, dummy_attention_mask), # Tuple of inputs\n                      f=onnx_model_path,\n                      export_params=True,\n                      opset_version=13,\n                      do_constant_folding=True,\n                      input_names=['input_values', 'attention_mask'],\n                      output_names=['output_logits'],\n                      dynamic_axes={'input_values': {0: 'batch_size', 1: 'sequence_length'},\n                                    'attention_mask': {0: 'batch_size', 1: 'sequence_length'},\n                                    'output_logits': {0: 'batch_size'}})\n    print(f\"Model successfully exported to ONNX: {onnx_model_path}\")\nexcept Exception as e:\n    print(f\"Error during ONNX export: {e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-20T11:58:58.110262Z","iopub.execute_input":"2025-06-20T11:58:58.110857Z","iopub.status.idle":"2025-06-20T11:58:59.042126Z","shell.execute_reply.started":"2025-06-20T11:58:58.110833Z","shell.execute_reply":"2025-06-20T11:58:59.041128Z"}},"outputs":[{"name":"stderr","text":"Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['masked_spec_embed']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Found model.safetensors. Attempting to load with safetensors library...\nSuccessfully loaded KWSModel state_dict using safetensors.torch.load_file\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_35/2666365914.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;31m# This processor should be the one you used for training/data collation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;31m# processor = AutoProcessor.from_pretrained(MODEL_CHECKPOINT) # Or load from SAVED_MODEL_DIR if saved\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m \u001b[0mprocessor_for_onnx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoProcessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSAVED_MODEL_DIR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/processing_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    379\u001b[0m         \u001b[0;31m# Last try: we use the PROCESSOR_MAPPING.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mPROCESSOR_MAPPING\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mPROCESSOR_MAPPING\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    382\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m         \u001b[0;31m# At this stage, there doesn't seem to be a `Processor` class available for this model, so let's try a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/wav2vec2/processing_wav2vec2.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mOSError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m             warnings.warn(\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/processing_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, **kwargs)\u001b[0m\n\u001b[1;32m   1183\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"token\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1185\u001b[0;31m         \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_arguments_from_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1186\u001b[0m         \u001b[0mprocessor_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_processor_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1187\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_args_and_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprocessor_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/processing_utils.py\u001b[0m in \u001b[0;36m_get_arguments_from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m   1246\u001b[0m                 \u001b[0mattribute_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_possibly_dynamic_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1248\u001b[0;31m             \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattribute_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1249\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/tokenization_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1033\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtokenizer_class_py\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1035\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer_class_py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1036\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1037\u001b[0m                     raise ValueError(\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2023\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"loading file {file_path} from cache at {resolved_vocab_files[file_id]}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2024\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2025\u001b[0;31m         return cls._from_pretrained(\n\u001b[0m\u001b[1;32m   2026\u001b[0m             \u001b[0mresolved_vocab_files\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2027\u001b[0m             \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m_from_pretrained\u001b[0;34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, token, cache_dir, local_files_only, _commit_hash, _is_local, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2276\u001b[0m         \u001b[0;31m# Instantiate the tokenizer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2277\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2278\u001b[0;31m             \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minit_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0minit_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2279\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mimport_protobuf_decode_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2280\u001b[0m             logger.info(\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/wav2vec2/tokenization_wav2vec2.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, vocab_file, bos_token, eos_token, unk_token, pad_token, word_delimiter_token, replace_word_delimiter_char, do_lower_case, target_lang, **kwargs)\u001b[0m\n\u001b[1;32m    166\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_lang\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_lang\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mvocab_handle\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_handle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: expected str, bytes or os.PathLike object, not NoneType"],"ename":"TypeError","evalue":"expected str, bytes or os.PathLike object, not NoneType","output_type":"error"}],"execution_count":17},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport torch\nimport torchaudio\nimport numpy as np\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import classification_report\nfrom transformers import Wav2Vec2Model, Wav2Vec2Processor\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\n\n# CONFIG\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nSAMPLE_RATE = 16000\nMAX_AUDIO_LEN = 3  # shorter max length for low memory\nDATA_DIR = \"/kaggle/working/korean_speech_commands_kaggle/all_comand\"  # your folder path\nPRETRAINED_MODEL = \"kkonjeong/wav2vec2-base-korean\"  # smaller Korean model\n\n# LOAD PRETRAINED Wav2Vec2 MODEL AND PROCESSOR\nprocessor = Wav2Vec2Processor.from_pretrained(PRETRAINED_MODEL)\nfeature_extractor = Wav2Vec2Model.from_pretrained(PRETRAINED_MODEL).to(DEVICE)\nfeature_extractor.eval()\n\n# LOAD DATA\ndef load_dataset(data_dir):\n    file_paths = []\n    labels = []\n\n    for label in os.listdir(data_dir):\n        class_path = os.path.join(data_dir, label)\n        if not os.path.isdir(class_path):\n            continue\n        for fname in os.listdir(class_path):\n            if fname.endswith(\".wav\"):\n                file_paths.append(os.path.join(class_path, fname))\n                labels.append(label)\n    return file_paths, labels\n\nfile_paths, labels = load_dataset(DATA_DIR)\nlabel_encoder = LabelEncoder()\nlabel_encoder.fit(labels)\n\n# DATASET CLASS\nclass KoreanCommandDataset(Dataset):\n    def __init__(self, file_paths, labels, label_encoder):\n        self.file_paths = file_paths\n        self.labels = labels\n        self.label_encoder = label_encoder\n\n    def __len__(self):\n        return len(self.file_paths)\n\n    def __getitem__(self, idx):\n        path = self.file_paths[idx]\n        label = self.labels[idx]\n\n        waveform, sr = torchaudio.load(path)\n        waveform = waveform.mean(0).unsqueeze(0)  # mono\n        if sr != SAMPLE_RATE:\n            resampler = torchaudio.transforms.Resample(sr, SAMPLE_RATE)\n            waveform = resampler(waveform)\n\n        waveform = waveform[:, :SAMPLE_RATE * MAX_AUDIO_LEN]\n\n        inputs = processor(waveform.squeeze(), sampling_rate=SAMPLE_RATE, return_tensors=\"pt\", padding=True)\n        with torch.no_grad():\n            outputs = feature_extractor(inputs.input_values.to(DEVICE)).last_hidden_state\n            embedding = torch.mean(outputs, dim=1).squeeze().cpu().numpy()\n\n        return torch.tensor(embedding, dtype=torch.float32), torch.tensor(self.label_encoder.transform([label])[0], dtype=torch.long)\n\n# SPLIT DATA\ntrain_files, test_files, train_labels, test_labels = train_test_split(file_paths, labels, test_size=0.2, stratify=labels, random_state=42)\ntrain_dataset = KoreanCommandDataset(train_files, train_labels, label_encoder)\ntest_dataset = KoreanCommandDataset(test_files, test_labels, label_encoder)\ntrain_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)  # reduced batch size\ntest_loader = DataLoader(test_dataset, batch_size=8)\n\n# MLP CLASSIFIER\nclass MLPClassifier(nn.Module):\n    def __init__(self, input_dim, num_classes):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(input_dim, 256),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(256, 128),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(128, num_classes)\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n# TRAINING SETUP\nINPUT_DIM = feature_extractor.config.hidden_size\nNUM_CLASSES = len(label_encoder.classes_)\nmodel = MLPClassifier(INPUT_DIM, NUM_CLASSES).to(DEVICE)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n\n# TRAINING LOOP\nEPOCHS = 1  # reduced for quick runs\nfor epoch in range(EPOCHS):\n    model.train()\n    total_loss = 0\n    for X_batch, y_batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n        X_batch, y_batch = X_batch.to(DEVICE), y_batch.to(DEVICE)\n        optimizer.zero_grad()\n        outputs = model(X_batch)\n        loss = criterion(outputs, y_batch)\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n    print(f\"Epoch {epoch+1}, Loss: {total_loss / len(train_loader):.4f}\")\n\n# EVALUATION\nmodel.eval()\nall_preds = []\nall_true = []\n\nwith torch.no_grad():\n    for X_batch, y_batch in test_loader:\n        X_batch = X_batch.to(DEVICE)\n        outputs = model(X_batch)\n        preds = torch.argmax(outputs, dim=1).cpu().numpy()\n        all_preds.extend(preds)\n        all_true.extend(y_batch.numpy())\n\nprint(\"\\nClassification Report:\")\nprint(classification_report(all_true, all_preds, target_names=label_encoder.classes_))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T10:35:26.981207Z","iopub.execute_input":"2025-06-21T10:35:26.981468Z","iopub.status.idle":"2025-06-21T10:37:47.338839Z","shell.execute_reply.started":"2025-06-21T10:35:26.981449Z","shell.execute_reply":"2025-06-21T10:37:47.337986Z"}},"outputs":[{"name":"stderr","text":"2025-06-21 10:35:40.666176: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1750502140.852710      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1750502140.912361      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/257 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"425ab7a9920e41729cc4efe69dd7b06e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.09k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6d4bf5788c0943f3b3e09af8447bd05c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/697 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"332be73a76564148aee43cc1d467e661"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/30.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7145516009f54856aafa4efbb9687d78"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/96.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7205e834a25540cab2c179b35cfe6f4f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/2.37k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"00aad68bc149485c9c3118540533265f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/378M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"71a3c709ae584958a80fb379a6e954e1"}},"metadata":{}},{"name":"stderr","text":"Epoch 1: 100%|██████████| 374/374 [01:24<00:00,  4.43it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1, Loss: 3.9081\n\nClassification Report:\n              precision    recall  f1-score   support\n\n       100미터       0.02      0.07      0.04        14\n        10미터       0.00      0.00      0.00        15\n         12시       0.00      0.00      0.00        15\n         1미터       0.00      0.00      0.00        15\n          1시       0.00      0.00      0.00        15\n       200미터       0.00      0.00      0.00        15\n        20미터       0.00      0.00      0.00        15\n          2시       0.00      0.00      0.00        15\n       300미터       0.00      0.00      0.00        15\n         3미터       0.00      0.00      0.00        15\n          3시       0.00      0.00      0.00        15\n       400미터       0.00      0.00      0.00        15\n        40미터       0.00      0.00      0.00        15\n         5미터       0.00      0.00      0.00        15\n        60미터       0.00      0.00      0.00        15\n          6시       0.00      0.00      0.00        15\n        80미터       0.00      0.00      0.00        15\n          9시       0.00      0.00      0.00        15\n         IR꺼       0.00      0.00      0.00        15\n         IR켜       0.00      0.00      0.00        15\n          경계       0.00      0.00      0.00        15\n        경계모드       0.00      0.00      0.00        15\n        공격모드       0.00      0.00      0.00        15\n         느리게       0.00      0.00      0.00        15\n          대기       0.00      0.00      0.00        15\n          드론       0.00      0.00      0.00        15\n       매우느리게       0.00      0.00      0.00        15\n       매우빠르게       0.00      0.00      0.00        15\n        모드변경       0.03      0.73      0.06        15\n          복귀       0.00      0.00      0.00        15\n         빠르게       0.00      0.00      0.00        15\n          사격       0.00      0.00      0.00        15\n          아래       0.00      0.00      0.00        15\n        야간모드       0.00      0.00      0.00        15\n          우로       0.02      0.53      0.05        15\n         우회전       0.00      0.00      0.00        15\n           위       0.00      0.00      0.00        15\n          전방       0.00      0.00      0.00        15\n          전진       0.00      0.00      0.00        15\n          정지       0.00      0.00      0.00        15\n          정찰       0.00      0.00      0.00        15\n        정찰모드       0.00      0.00      0.00        15\n         조명꺼       0.00      0.00      0.00        14\n         조명켜       0.00      0.00      0.00        15\n          조준       0.00      0.00      0.00        15\n          좌로       0.00      0.00      0.00        15\n         좌회전       0.00      0.00      0.00        15\n        주행모드       0.00      0.00      0.00        15\n          후방       0.00      0.00      0.00        15\n          후진       0.00      0.00      0.00        15\n\n    accuracy                           0.03       748\n   macro avg       0.00      0.03      0.00       748\nweighted avg       0.00      0.03      0.00       748\n\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"torch.save(model.state_dict(), \"korean_command_classifier.pt\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T10:38:38.451618Z","iopub.execute_input":"2025-06-21T10:38:38.452303Z","iopub.status.idle":"2025-06-21T10:38:38.460426Z","shell.execute_reply.started":"2025-06-21T10:38:38.452278Z","shell.execute_reply":"2025-06-21T10:38:38.459699Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"model = MLPClassifier(INPUT_DIM, NUM_CLASSES)\nmodel.load_state_dict(torch.load(\"korean_command_classifier.pt\"))\nmodel.eval()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T10:38:58.314333Z","iopub.execute_input":"2025-06-21T10:38:58.314604Z","iopub.status.idle":"2025-06-21T10:38:58.327227Z","shell.execute_reply.started":"2025-06-21T10:38:58.314583Z","shell.execute_reply":"2025-06-21T10:38:58.326609Z"}},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"MLPClassifier(\n  (net): Sequential(\n    (0): Linear(in_features=768, out_features=256, bias=True)\n    (1): ReLU()\n    (2): Dropout(p=0.2, inplace=False)\n    (3): Linear(in_features=256, out_features=128, bias=True)\n    (4): ReLU()\n    (5): Dropout(p=0.2, inplace=False)\n    (6): Linear(in_features=128, out_features=50, bias=True)\n  )\n)"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"import torch\n\n# Assume INPUT_DIM and NUM_CLASSES are known\ndummy_input = torch.randn(1, INPUT_DIM).to(DEVICE)\n\nmodel.to(DEVICE)\nmodel.eval()\n\ntorch.onnx.export(\n    model,                    # model being run\n    dummy_input,              # model input (or a tuple for multiple inputs)\n    \"korean_command_classifier.onnx\",  # where to save the ONNX file\n    export_params=True,       # store the trained parameter weights inside the model file\n    opset_version=12,         # ONNX opset version (choose 11+ for better support)\n    do_constant_folding=True, # optimization\n    input_names = ['input'],  # the model's input names\n    output_names = ['output'],# the model's output names\n    dynamic_axes={'input' : {0 : 'batch_size'},    # variable batch size\n                  'output' : {0 : 'batch_size'}}\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T10:42:03.489028Z","iopub.execute_input":"2025-06-21T10:42:03.489334Z","iopub.status.idle":"2025-06-21T10:42:03.657660Z","shell.execute_reply.started":"2025-06-21T10:42:03.489310Z","shell.execute_reply":"2025-06-21T10:42:03.657140Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport torch\nimport torchaudio\nimport numpy as np\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import classification_report\nfrom transformers import Wav2Vec2Model, Wav2Vec2Processor\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\n\n\n# -------------------- CONFIG --------------------\nDEVICE          = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nSAMPLE_RATE     = 16_000\nMAX_AUDIO_LEN   = 4                # seconds (trim / pad)\nDATA_DIR        = \"/kaggle/working/korean_speech_commands_kaggle/all_comand\"        # root folder with <label>/<wav files>\nPRETRAINED_MODEL = \"kkonjeong/wav2vec2-base-korean\"  # compact Korean wav2vec2\nMODEL_PT_PATH   = \"new0622korean_command_classifier.pt\"\nMODEL_ONNX_PATH = \"new0622korean_command_classifier.onnx\"\n\n# ------------------ LOAD Wav2Vec2 ----------------\nprint(\"Loading wav2vec2 model…\")\nprocessor        = Wav2Vec2Processor.from_pretrained(PRETRAINED_MODEL)\nfeature_extractor = Wav2Vec2Model.from_pretrained(PRETRAINED_MODEL).to(DEVICE)\nfeature_extractor.eval()  # frozen\n\n# ---------------  DATA UTILS --------------------\n\ndef load_dataset(data_dir: str):\n    \"\"\"Scan dataset/<label>/<wav> → returns (paths, labels) lists.\"\"\"\n    fpaths, lbls = [], []\n    for label in sorted(os.listdir(data_dir)):\n        ldir = os.path.join(data_dir, label)\n        if not os.path.isdir(ldir):\n            continue\n        for fname in os.listdir(ldir):\n            if fname.endswith(\".wav\"):\n                fpaths.append(os.path.join(ldir, fname))\n                lbls.append(label)\n    return fpaths, lbls\n\nprint(\"Indexing dataset …\")\nfile_paths, labels = load_dataset(DATA_DIR)\nlabel_encoder       = LabelEncoder().fit(labels)\nNUM_CLASSES         = len(label_encoder.classes_)\n\n# ------------- Torch Dataset --------------------\nclass KoreanCommandDataset(Dataset):\n    def __init__(self, fpaths, lbls):\n        self.fpaths = fpaths\n        self.lbls   = lbls\n\n    def __len__(self):\n        return len(self.fpaths)\n\n    def __getitem__(self, idx):\n        path  = self.fpaths[idx]\n        label = self.lbls[idx]\n\n        # ---- load & resample ----\n        waveform, sr = torchaudio.load(path)\n        waveform = waveform.mean(0, keepdim=True)  # mono\n        if sr != SAMPLE_RATE:\n            waveform = torchaudio.functional.resample(waveform, sr, SAMPLE_RATE)\n        waveform = waveform[:, :SAMPLE_RATE * MAX_AUDIO_LEN]\n\n        # ---- extract embedding ----\n        inputs   = processor(waveform.squeeze(), sampling_rate=SAMPLE_RATE, return_tensors=\"pt\", padding=True)\n        with torch.no_grad():\n            hidden_states = feature_extractor(inputs.input_values.to(DEVICE)).last_hidden_state\n            embedding     = hidden_states.mean(dim=1).squeeze().cpu()  # (768,)\n\n        label_id = label_encoder.transform([label])[0]\n        return embedding, torch.tensor(label_id, dtype=torch.long)\n\n# ------------ Dataloaders -----------------------\ntrain_f, test_f, train_l, test_l = train_test_split(file_paths, labels, test_size=0.2, stratify=labels, random_state=42)\ntrain_ds  = KoreanCommandDataset(train_f, train_l)\ntest_ds   = KoreanCommandDataset(test_f,  test_l)\ntrain_dl  = DataLoader(train_ds, batch_size=8, shuffle=True)\ntest_dl   = DataLoader(test_ds,  batch_size=8)\n\n# ----------------- MLP Classifier ---------------\nclass MLPClassifier(nn.Module):\n    def __init__(self, in_dim: int, n_cls: int):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(in_dim, 256), nn.ReLU(), nn.Dropout(0.2),\n            nn.Linear(256, 128),    nn.ReLU(), nn.Dropout(0.2),\n            nn.Linear(128, n_cls)\n        )\n    def forward(self, x):\n        return self.net(x)\n\nINPUT_DIM = feature_extractor.config.hidden_size  # 768 for base model\nclassifier = MLPClassifier(INPUT_DIM, NUM_CLASSES).to(DEVICE)\n\n# -------------- Train ---------------------------\nEPOCHS = 150\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(classifier.parameters(), lr=1e-4)\n\nfor epoch in range(EPOCHS):\n    classifier.train()\n    running_loss = 0.0\n    for X, y in tqdm(train_dl, desc=f\"Epoch {epoch+1}\"):\n        X, y = X.to(DEVICE), y.to(DEVICE)\n        optimizer.zero_grad()\n        logits = classifier(X)\n        loss   = criterion(logits, y)\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item()\n    print(f\"Epoch {epoch+1}  |  avg loss: {running_loss / len(train_dl):.4f}\")\n\n# -------------- Evaluate ------------------------\nclassifier.eval()\nall_preds, all_true = [], []\nwith torch.no_grad():\n    for X, y in test_dl:\n        preds = classifier(X.to(DEVICE)).argmax(dim=1).cpu().numpy()\n        all_preds.extend(preds)\n        all_true.extend(y.numpy())\n\nprint(\"\\nClassification Report:\")\nprint(classification_report(all_true, all_preds, target_names=label_encoder.classes_))\n\n# -------------- Save PT -------------------------\nprint(f\"Saving weights → {MODEL_PT_PATH}\")\ntorch.save(classifier.state_dict(), MODEL_PT_PATH)\n\n# -------------- Export ONNX ---------------------\ndummy_input = torch.randn(1, INPUT_DIM).to(DEVICE)\ntorch.onnx.export(\n    classifier, dummy_input, MODEL_ONNX_PATH,\n    input_names=[\"input\"], output_names=[\"output\"],\n    dynamic_axes={\"input\": {0: \"batch_size\"}, \"output\": {0: \"batch_size\"}},\n    export_params=True, opset_version=12, do_constant_folding=True,\n)\nprint(f\"ONNX model saved → {MODEL_ONNX_PATH}\\n\")\n\n# ---------------- Inference Helper --------------\n\ndef predict_command(wav_path: str):\n    \"\"\"Return the predicted label (string) for a single Korean .wav file.\"\"\"\n    classifier.eval()\n    waveform, sr = torchaudio.load(wav_path)\n    waveform = waveform.mean(0, keepdim=True)\n    if sr != SAMPLE_RATE:\n        waveform = torchaudio.functional.resample(waveform, sr, SAMPLE_RATE)\n    waveform = waveform[:, :SAMPLE_RATE * MAX_AUDIO_LEN]\n    inputs   = processor(waveform.squeeze(), sampling_rate=SAMPLE_RATE, return_tensors=\"pt\", padding=True)\n    with torch.no_grad():\n        hidden = feature_extractor(inputs.input_values.to(DEVICE)).last_hidden_state\n        emb    = hidden.mean(dim=1)\n        logits = classifier(emb)\n        pred_id = logits.argmax(dim=1).item()\n    return label_encoder.inverse_transform([pred_id])[0]\n\nif __name__ == \"__main__\":\n    test_wav = \"sample.wav\"  # TODO: change to your file\n    if os.path.exists(test_wav):\n        print(\"Predicted command:\", predict_command(test_wav))\n    else:\n        print(\"Put a test .wav path in `test_wav` and run again.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-22T03:28:13.906692Z","iopub.execute_input":"2025-06-22T03:28:13.907289Z","iopub.status.idle":"2025-06-22T05:10:52.069747Z","shell.execute_reply.started":"2025-06-22T03:28:13.907266Z","shell.execute_reply":"2025-06-22T05:10:52.068997Z"}},"outputs":[{"name":"stderr","text":"2025-06-22 03:28:30.857626: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1750562911.320440      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1750562911.443062      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"Loading wav2vec2 model…\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/257 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b3e50e395237436a90c97051b76386e3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.09k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"04262c5052c64002bfa1f8f4141e4527"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/697 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ff9b0a7c6eac490893c6cdc4727d8daf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/30.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8981fecaf7d94322a99970b561366f81"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/96.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c731042f81394663b1c9ce1f72b608bf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/2.37k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"82f49f529edb4cfe810a41d8eff0a130"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/378M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7db771ebaa6c427f8439f8bafb80a53d"}},"metadata":{}},{"name":"stdout","text":"Indexing dataset …\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1: 100%|██████████| 374/374 [01:14<00:00,  5.00it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1  |  avg loss: 3.9098\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2: 100%|██████████| 374/374 [01:16<00:00,  4.92it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2  |  avg loss: 3.8734\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3: 100%|██████████| 374/374 [01:16<00:00,  4.91it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3  |  avg loss: 3.8144\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4: 100%|██████████| 374/374 [01:16<00:00,  4.87it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4  |  avg loss: 3.7682\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5: 100%|██████████| 374/374 [01:16<00:00,  4.89it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5  |  avg loss: 3.7254\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6: 100%|██████████| 374/374 [01:16<00:00,  4.89it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6  |  avg loss: 3.6653\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7: 100%|██████████| 374/374 [01:16<00:00,  4.90it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7  |  avg loss: 3.5917\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8: 100%|██████████| 374/374 [01:16<00:00,  4.90it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8  |  avg loss: 3.5126\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9: 100%|██████████| 374/374 [01:16<00:00,  4.89it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 9  |  avg loss: 3.4272\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10: 100%|██████████| 374/374 [01:16<00:00,  4.89it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10  |  avg loss: 3.3415\n","output_type":"stream"},{"name":"stderr","text":"Epoch 11: 100%|██████████| 374/374 [01:16<00:00,  4.91it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 11  |  avg loss: 3.2447\n","output_type":"stream"},{"name":"stderr","text":"Epoch 12: 100%|██████████| 374/374 [01:16<00:00,  4.91it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 12  |  avg loss: 3.1480\n","output_type":"stream"},{"name":"stderr","text":"Epoch 13: 100%|██████████| 374/374 [01:16<00:00,  4.90it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 13  |  avg loss: 3.0460\n","output_type":"stream"},{"name":"stderr","text":"Epoch 14: 100%|██████████| 374/374 [01:16<00:00,  4.88it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 14  |  avg loss: 2.9674\n","output_type":"stream"},{"name":"stderr","text":"Epoch 15: 100%|██████████| 374/374 [01:16<00:00,  4.91it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 15  |  avg loss: 2.8834\n","output_type":"stream"},{"name":"stderr","text":"Epoch 16: 100%|██████████| 374/374 [01:16<00:00,  4.91it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 16  |  avg loss: 2.8004\n","output_type":"stream"},{"name":"stderr","text":"Epoch 17: 100%|██████████| 374/374 [01:16<00:00,  4.92it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 17  |  avg loss: 2.7394\n","output_type":"stream"},{"name":"stderr","text":"Epoch 18: 100%|██████████| 374/374 [01:16<00:00,  4.91it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 18  |  avg loss: 2.6598\n","output_type":"stream"},{"name":"stderr","text":"Epoch 19: 100%|██████████| 374/374 [01:15<00:00,  4.92it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 19  |  avg loss: 2.5952\n","output_type":"stream"},{"name":"stderr","text":"Epoch 20: 100%|██████████| 374/374 [01:15<00:00,  4.92it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 20  |  avg loss: 2.5434\n","output_type":"stream"},{"name":"stderr","text":"Epoch 21: 100%|██████████| 374/374 [01:16<00:00,  4.91it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 21  |  avg loss: 2.4834\n","output_type":"stream"},{"name":"stderr","text":"Epoch 22: 100%|██████████| 374/374 [01:16<00:00,  4.92it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 22  |  avg loss: 2.4208\n","output_type":"stream"},{"name":"stderr","text":"Epoch 23: 100%|██████████| 374/374 [01:15<00:00,  4.92it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 23  |  avg loss: 2.3985\n","output_type":"stream"},{"name":"stderr","text":"Epoch 24: 100%|██████████| 374/374 [01:15<00:00,  4.92it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 24  |  avg loss: 2.3301\n","output_type":"stream"},{"name":"stderr","text":"Epoch 25: 100%|██████████| 374/374 [01:15<00:00,  4.93it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 25  |  avg loss: 2.2906\n","output_type":"stream"},{"name":"stderr","text":"Epoch 26: 100%|██████████| 374/374 [01:15<00:00,  4.92it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 26  |  avg loss: 2.2482\n","output_type":"stream"},{"name":"stderr","text":"Epoch 27: 100%|██████████| 374/374 [01:15<00:00,  4.93it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 27  |  avg loss: 2.2003\n","output_type":"stream"},{"name":"stderr","text":"Epoch 28: 100%|██████████| 374/374 [01:15<00:00,  4.93it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 28  |  avg loss: 2.1610\n","output_type":"stream"},{"name":"stderr","text":"Epoch 29: 100%|██████████| 374/374 [01:15<00:00,  4.93it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 29  |  avg loss: 2.1367\n","output_type":"stream"},{"name":"stderr","text":"Epoch 30: 100%|██████████| 374/374 [01:15<00:00,  4.93it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 30  |  avg loss: 2.0790\n","output_type":"stream"},{"name":"stderr","text":"Epoch 31: 100%|██████████| 374/374 [01:16<00:00,  4.92it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 31  |  avg loss: 2.0544\n","output_type":"stream"},{"name":"stderr","text":"Epoch 32: 100%|██████████| 374/374 [01:16<00:00,  4.92it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 32  |  avg loss: 2.0182\n","output_type":"stream"},{"name":"stderr","text":"Epoch 33: 100%|██████████| 374/374 [01:15<00:00,  4.92it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 33  |  avg loss: 1.9700\n","output_type":"stream"},{"name":"stderr","text":"Epoch 34: 100%|██████████| 374/374 [01:15<00:00,  4.92it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 34  |  avg loss: 1.9354\n","output_type":"stream"},{"name":"stderr","text":"Epoch 35: 100%|██████████| 374/374 [01:15<00:00,  4.93it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 35  |  avg loss: 1.9117\n","output_type":"stream"},{"name":"stderr","text":"Epoch 36: 100%|██████████| 374/374 [01:16<00:00,  4.92it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 36  |  avg loss: 1.8734\n","output_type":"stream"},{"name":"stderr","text":"Epoch 37: 100%|██████████| 374/374 [01:15<00:00,  4.93it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 37  |  avg loss: 1.8463\n","output_type":"stream"},{"name":"stderr","text":"Epoch 38: 100%|██████████| 374/374 [01:15<00:00,  4.93it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 38  |  avg loss: 1.8013\n","output_type":"stream"},{"name":"stderr","text":"Epoch 39: 100%|██████████| 374/374 [01:15<00:00,  4.93it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 39  |  avg loss: 1.7896\n","output_type":"stream"},{"name":"stderr","text":"Epoch 40: 100%|██████████| 374/374 [01:16<00:00,  4.92it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 40  |  avg loss: 1.7519\n","output_type":"stream"},{"name":"stderr","text":"Epoch 41: 100%|██████████| 374/374 [01:15<00:00,  4.92it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 41  |  avg loss: 1.7276\n","output_type":"stream"},{"name":"stderr","text":"Epoch 42: 100%|██████████| 374/374 [01:16<00:00,  4.92it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 42  |  avg loss: 1.6881\n","output_type":"stream"},{"name":"stderr","text":"Epoch 43: 100%|██████████| 374/374 [01:15<00:00,  4.93it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 43  |  avg loss: 1.6830\n","output_type":"stream"},{"name":"stderr","text":"Epoch 44: 100%|██████████| 374/374 [01:15<00:00,  4.92it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 44  |  avg loss: 1.6228\n","output_type":"stream"},{"name":"stderr","text":"Epoch 45: 100%|██████████| 374/374 [01:16<00:00,  4.91it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 45  |  avg loss: 1.6057\n","output_type":"stream"},{"name":"stderr","text":"Epoch 46: 100%|██████████| 374/374 [01:16<00:00,  4.86it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 46  |  avg loss: 1.5885\n","output_type":"stream"},{"name":"stderr","text":"Epoch 47: 100%|██████████| 374/374 [01:16<00:00,  4.88it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 47  |  avg loss: 1.5487\n","output_type":"stream"},{"name":"stderr","text":"Epoch 48: 100%|██████████| 374/374 [01:16<00:00,  4.90it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 48  |  avg loss: 1.5364\n","output_type":"stream"},{"name":"stderr","text":"Epoch 49: 100%|██████████| 374/374 [01:16<00:00,  4.90it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 49  |  avg loss: 1.5008\n","output_type":"stream"},{"name":"stderr","text":"Epoch 50: 100%|██████████| 374/374 [01:16<00:00,  4.89it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 50  |  avg loss: 1.4987\n","output_type":"stream"},{"name":"stderr","text":"Epoch 51: 100%|██████████| 374/374 [01:16<00:00,  4.91it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 51  |  avg loss: 1.4663\n","output_type":"stream"},{"name":"stderr","text":"Epoch 52: 100%|██████████| 374/374 [01:15<00:00,  4.93it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 52  |  avg loss: 1.4374\n","output_type":"stream"},{"name":"stderr","text":"Epoch 53: 100%|██████████| 374/374 [01:16<00:00,  4.91it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 53  |  avg loss: 1.4376\n","output_type":"stream"},{"name":"stderr","text":"Epoch 54: 100%|██████████| 374/374 [01:15<00:00,  4.92it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 54  |  avg loss: 1.4094\n","output_type":"stream"},{"name":"stderr","text":"Epoch 55: 100%|██████████| 374/374 [01:15<00:00,  4.92it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 55  |  avg loss: 1.3966\n","output_type":"stream"},{"name":"stderr","text":"Epoch 56: 100%|██████████| 374/374 [01:16<00:00,  4.92it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 56  |  avg loss: 1.3830\n","output_type":"stream"},{"name":"stderr","text":"Epoch 57: 100%|██████████| 374/374 [01:16<00:00,  4.92it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 57  |  avg loss: 1.3549\n","output_type":"stream"},{"name":"stderr","text":"Epoch 58: 100%|██████████| 374/374 [01:15<00:00,  4.93it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 58  |  avg loss: 1.3438\n","output_type":"stream"},{"name":"stderr","text":"Epoch 59: 100%|██████████| 374/374 [01:16<00:00,  4.89it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 59  |  avg loss: 1.3321\n","output_type":"stream"},{"name":"stderr","text":"Epoch 60: 100%|██████████| 374/374 [01:15<00:00,  4.93it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 60  |  avg loss: 1.2969\n","output_type":"stream"},{"name":"stderr","text":"Epoch 61: 100%|██████████| 374/374 [01:15<00:00,  4.92it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 61  |  avg loss: 1.3063\n","output_type":"stream"},{"name":"stderr","text":"Epoch 62: 100%|██████████| 374/374 [01:15<00:00,  4.93it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 62  |  avg loss: 1.2769\n","output_type":"stream"},{"name":"stderr","text":"Epoch 63: 100%|██████████| 374/374 [01:15<00:00,  4.92it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 63  |  avg loss: 1.2707\n","output_type":"stream"},{"name":"stderr","text":"Epoch 64: 100%|██████████| 374/374 [01:15<00:00,  4.92it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 64  |  avg loss: 1.2305\n","output_type":"stream"},{"name":"stderr","text":"Epoch 65: 100%|██████████| 374/374 [01:16<00:00,  4.92it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 65  |  avg loss: 1.2263\n","output_type":"stream"},{"name":"stderr","text":"Epoch 66: 100%|██████████| 374/374 [01:15<00:00,  4.92it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 66  |  avg loss: 1.2203\n","output_type":"stream"},{"name":"stderr","text":"Epoch 67: 100%|██████████| 374/374 [01:15<00:00,  4.93it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 67  |  avg loss: 1.1975\n","output_type":"stream"},{"name":"stderr","text":"Epoch 68: 100%|██████████| 374/374 [01:16<00:00,  4.92it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 68  |  avg loss: 1.1974\n","output_type":"stream"},{"name":"stderr","text":"Epoch 69: 100%|██████████| 374/374 [01:15<00:00,  4.92it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 69  |  avg loss: 1.1821\n","output_type":"stream"},{"name":"stderr","text":"Epoch 70: 100%|██████████| 374/374 [01:15<00:00,  4.93it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 70  |  avg loss: 1.1666\n","output_type":"stream"},{"name":"stderr","text":"Epoch 71: 100%|██████████| 374/374 [01:15<00:00,  4.93it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 71  |  avg loss: 1.1640\n","output_type":"stream"},{"name":"stderr","text":"Epoch 72: 100%|██████████| 374/374 [01:16<00:00,  4.92it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 72  |  avg loss: 1.1315\n","output_type":"stream"},{"name":"stderr","text":"Epoch 73: 100%|██████████| 374/374 [01:15<00:00,  4.92it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 73  |  avg loss: 1.1281\n","output_type":"stream"},{"name":"stderr","text":"Epoch 74: 100%|██████████| 374/374 [01:15<00:00,  4.92it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 74  |  avg loss: 1.1059\n","output_type":"stream"},{"name":"stderr","text":"Epoch 75: 100%|██████████| 374/374 [01:16<00:00,  4.92it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 75  |  avg loss: 1.0990\n","output_type":"stream"},{"name":"stderr","text":"Epoch 76: 100%|██████████| 374/374 [01:15<00:00,  4.93it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 76  |  avg loss: 1.0846\n","output_type":"stream"},{"name":"stderr","text":"Epoch 77: 100%|██████████| 374/374 [01:15<00:00,  4.93it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 77  |  avg loss: 1.0846\n","output_type":"stream"},{"name":"stderr","text":"Epoch 78: 100%|██████████| 374/374 [01:15<00:00,  4.92it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 78  |  avg loss: 1.0745\n","output_type":"stream"},{"name":"stderr","text":"Epoch 79: 100%|██████████| 374/374 [01:16<00:00,  4.92it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 79  |  avg loss: 1.0610\n","output_type":"stream"},{"name":"stderr","text":"Epoch 80: 100%|██████████| 374/374 [01:15<00:00,  4.92it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 80  |  avg loss: 1.0658\n\nClassification Report:\n              precision    recall  f1-score   support\n\n       100미터       0.36      0.29      0.32        14\n        10미터       0.69      0.73      0.71        15\n         12시       0.72      0.87      0.79        15\n         1미터       0.60      0.60      0.60        15\n          1시       0.77      0.67      0.71        15\n       200미터       0.90      0.60      0.72        15\n        20미터       0.45      0.67      0.54        15\n          2시       0.63      0.80      0.71        15\n       300미터       0.83      0.33      0.48        15\n         3미터       0.67      0.80      0.73        15\n          3시       0.57      0.80      0.67        15\n       400미터       0.50      0.53      0.52        15\n        40미터       0.50      0.73      0.59        15\n         5미터       0.62      0.53      0.57        15\n        60미터       0.60      0.80      0.69        15\n          6시       0.50      0.47      0.48        15\n        80미터       1.00      0.27      0.42        15\n          9시       0.71      0.67      0.69        15\n         IR꺼       0.93      0.93      0.93        15\n         IR켜       0.76      0.87      0.81        15\n          경계       0.62      0.67      0.65        15\n        경계모드       0.60      0.60      0.60        15\n        공격모드       0.93      0.87      0.90        15\n         느리게       0.75      1.00      0.86        15\n          대기       0.90      0.60      0.72        15\n          드론       0.78      0.47      0.58        15\n       매우느리게       0.83      0.67      0.74        15\n       매우빠르게       0.61      0.73      0.67        15\n        모드변경       0.86      0.80      0.83        15\n          복귀       0.75      0.80      0.77        15\n         빠르게       0.62      0.67      0.65        15\n          사격       0.57      0.80      0.67        15\n          아래       0.85      0.73      0.79        15\n        야간모드       0.93      0.93      0.93        15\n          우로       0.72      0.87      0.79        15\n         우회전       0.60      0.60      0.60        15\n           위       0.69      0.73      0.71        15\n          전방       0.53      0.60      0.56        15\n          전진       0.82      0.93      0.87        15\n          정지       0.62      0.67      0.65        15\n          정찰       0.60      0.60      0.60        15\n        정찰모드       0.91      0.67      0.77        15\n         조명꺼       0.67      0.86      0.75        14\n         조명켜       0.42      0.33      0.37        15\n          조준       0.81      0.87      0.84        15\n          좌로       0.89      0.53      0.67        15\n         좌회전       0.78      0.47      0.58        15\n        주행모드       0.93      0.87      0.90        15\n          후방       0.82      0.93      0.87        15\n          후진       0.73      0.73      0.73        15\n\n    accuracy                           0.69       748\n   macro avg       0.71      0.69      0.69       748\nweighted avg       0.71      0.69      0.69       748\n\nSaving weights → 0622korean_command_classifier.pt\nONNX model saved → 0622korean_command_classifier.onnx\n\nPut a test .wav path in `test_wav` and run again.\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"def predict_command(wav_path: str):\n    \"\"\"Return the predicted label (string) for a single Korean .wav file.\"\"\"\n    classifier.eval()\n    waveform, sr = torchaudio.load(wav_path)\n    waveform = waveform.mean(0, keepdim=True)\n    if sr != SAMPLE_RATE:\n        waveform = torchaudio.functional.resample(waveform, sr, SAMPLE_RATE)\n    waveform = waveform[:, :SAMPLE_RATE * MAX_AUDIO_LEN]\n    inputs   = processor(waveform.squeeze(), sampling_rate=SAMPLE_RATE, return_tensors=\"pt\", padding=True)\n    with torch.no_grad():\n        hidden = feature_extractor(inputs.input_values.to(DEVICE)).last_hidden_state\n        emb    = hidden.mean(dim=1)\n        logits = classifier(emb)\n        pred_id = logits.argmax(dim=1).item()\n    return label_encoder.inverse_transform([pred_id])[0]\n\nif __name__ == \"__main__\":\n    test_wav = \"/kaggle/working/korean_speech_commands_kaggle/all_comand/모드변경/aug_clean_Mode_Change_13_noisy.wav\" \n    if os.path.exists(test_wav):\n        print(\"Predicted command:\", predict_command(test_wav))\n    else:\n        print(\"Put a test .wav path in `test_wav` and run again.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-22T05:22:15.815588Z","iopub.execute_input":"2025-06-22T05:22:15.816124Z","iopub.status.idle":"2025-06-22T05:22:15.861689Z","shell.execute_reply.started":"2025-06-22T05:22:15.816102Z","shell.execute_reply":"2025-06-22T05:22:15.861150Z"}},"outputs":[{"name":"stdout","text":"Predicted command: 모드변경\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"for i, label in enumerate(label_encoder.classes_):\n    print(f\"{i}: {label}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T12:27:06.246383Z","iopub.execute_input":"2025-06-21T12:27:06.246746Z","iopub.status.idle":"2025-06-21T12:27:06.252475Z","shell.execute_reply.started":"2025-06-21T12:27:06.246721Z","shell.execute_reply":"2025-06-21T12:27:06.251553Z"}},"outputs":[{"name":"stdout","text":"0: 100미터\n1: 10미터\n2: 12시\n3: 1미터\n4: 1시\n5: 200미터\n6: 20미터\n7: 2시\n8: 300미터\n9: 3미터\n10: 3시\n11: 400미터\n12: 40미터\n13: 5미터\n14: 60미터\n15: 6시\n16: 80미터\n17: 9시\n18: IR꺼\n19: IR켜\n20: 경계\n21: 경계모드\n22: 공격모드\n23: 느리게\n24: 대기\n25: 드론\n26: 매우느리게\n27: 매우빠르게\n28: 모드변경\n29: 복귀\n30: 빠르게\n31: 사격\n32: 아래\n33: 야간모드\n34: 우로\n35: 우회전\n36: 위\n37: 전방\n38: 전진\n39: 정지\n40: 정찰\n41: 정찰모드\n42: 조명꺼\n43: 조명켜\n44: 조준\n45: 좌로\n46: 좌회전\n47: 주행모드\n48: 후방\n49: 후진\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## SAVE label and earlystop","metadata":{}},{"cell_type":"code","source":"import os\nimport torch\nimport torchaudio\nimport numpy as np\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import classification_report, accuracy_score\nfrom transformers import Wav2Vec2Model, Wav2Vec2Processor\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\n\n\n# -------------------- CONFIG --------------------\nDEVICE          = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nSAMPLE_RATE     = 16_000\nMAX_AUDIO_LEN   = 4                # seconds (trim / pad)\nDATA_DIR        = \"/kaggle/working/korean_speech_commands_kaggle/all_comand\"        # root folder with <label>/<wav files>\nPRETRAINED_MODEL = \"kkonjeong/wav2vec2-base-korean\"  # compact Korean wav2vec2\nMODEL_PT_PATH   = \"korean_command_classifier.pt\"\nMODEL_BEST_PATH = \"best_model.pt\"\nMODEL_ONNX_PATH = \"korean_command_classifier.onnx\"\nLABELS_TXT_PATH = \"labels_wav2vec2.txt\"\nPATIENCE        = 10  # early stopping patience (loss plateaus)\n\n# ------------------ LOAD Wav2Vec2 ----------------\nprint(\"Loading wav2vec2 model…\")\nprocessor        = Wav2Vec2Processor.from_pretrained(PRETRAINED_MODEL)\nfeature_extractor = Wav2Vec2Model.from_pretrained(PRETRAINED_MODEL).to(DEVICE)\nfeature_extractor.eval()  # frozen\n\n# ---------------  DATA UTILS --------------------\n\ndef load_dataset(data_dir: str):\n    \"\"\"Scan dataset/<label>/<wav> → returns (paths, labels) lists.\"\"\"\n    fpaths, lbls = [], []\n    for label in sorted(os.listdir(data_dir)):\n        ldir = os.path.join(data_dir, label)\n        if not os.path.isdir(ldir):\n            continue\n        for fname in os.listdir(ldir):\n            if fname.endswith(\".wav\"):\n                fpaths.append(os.path.join(ldir, fname))\n                lbls.append(label)\n    return fpaths, lbls\n\nprint(\"Indexing dataset …\")\nfile_paths, labels = load_dataset(DATA_DIR)\nlabel_encoder       = LabelEncoder().fit(labels)\nNUM_CLASSES         = len(label_encoder.classes_)\n\n# Save label list to txt\nwith open(LABELS_TXT_PATH, \"w\", encoding=\"utf-8\") as f:\n    for lbl in label_encoder.classes_:\n        f.write(lbl + \"\\n\")\nprint(f\"Saved label names to {LABELS_TXT_PATH}\")\n\n# ------------- Torch Dataset --------------------\nclass KoreanCommandDataset(Dataset):\n    def __init__(self, fpaths, lbls):\n        self.fpaths = fpaths\n        self.lbls   = lbls\n\n    def __len__(self):\n        return len(self.fpaths)\n\n    def __getitem__(self, idx):\n        path  = self.fpaths[idx]\n        label = self.lbls[idx]\n\n        # ---- load & resample ----\n        waveform, sr = torchaudio.load(path)\n        waveform = waveform.mean(0, keepdim=True)  # mono\n        if sr != SAMPLE_RATE:\n            waveform = torchaudio.functional.resample(waveform, sr, SAMPLE_RATE)\n        waveform = waveform[:, :SAMPLE_RATE * MAX_AUDIO_LEN]\n\n        # ---- extract embedding ----\n        inputs   = processor(waveform.squeeze(), sampling_rate=SAMPLE_RATE, return_tensors=\"pt\", padding=True)\n        with torch.no_grad():\n            hidden_states = feature_extractor(inputs.input_values.to(DEVICE)).last_hidden_state\n            embedding     = hidden_states.mean(dim=1).squeeze().cpu()  # (768,)\n\n        label_id = label_encoder.transform([label])[0]\n        return embedding, torch.tensor(label_id, dtype=torch.long)\n\n# ------------ Dataloaders -----------------------\ntrain_f, test_f, train_l, test_l = train_test_split(file_paths, labels, test_size=0.2, stratify=labels, random_state=42)\ntrain_ds  = KoreanCommandDataset(train_f, train_l)\ntest_ds   = KoreanCommandDataset(test_f,  test_l)\ntrain_dl  = DataLoader(train_ds, batch_size=8, shuffle=True)\ntest_dl   = DataLoader(test_ds,  batch_size=8)\n\n# ----------------- MLP Classifier ---------------\nclass MLPClassifier(nn.Module):\n    def __init__(self, in_dim: int, n_cls: int):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(in_dim, 256), nn.ReLU(), nn.Dropout(0.2),\n            nn.Linear(256, 128),    nn.ReLU(), nn.Dropout(0.2),\n            nn.Linear(128, n_cls)\n        )\n    def forward(self, x):\n        return self.net(x)\n\nINPUT_DIM = feature_extractor.config.hidden_size  # 768 for base model\nclassifier = MLPClassifier(INPUT_DIM, NUM_CLASSES).to(DEVICE)\n\n# -------------- Train with Early Stopping ---------------------------\nEPOCHS = 150\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(classifier.parameters(), lr=1e-4)\n\nbest_acc = 0.0\nbest_model_state = None\nloss_history = []\nstagnation_counter = 0\n\nfor epoch in range(EPOCHS):\n    classifier.train()\n    running_loss = 0.0\n    for X, y in tqdm(train_dl, desc=f\"Epoch {epoch+1}\"):\n        X, y = X.to(DEVICE), y.to(DEVICE)\n        optimizer.zero_grad()\n        logits = classifier(X)\n        loss   = criterion(logits, y)\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item()\n\n    avg_loss = running_loss / len(train_dl)\n    loss_history.append(avg_loss)\n    print(f\"Epoch {epoch+1}  |  avg loss: {avg_loss:.4f}\")\n\n    # Evaluate\n    classifier.eval()\n    all_preds, all_true = [], []\n    with torch.no_grad():\n        for X, y in test_dl:\n            preds = classifier(X.to(DEVICE)).argmax(dim=1).cpu().numpy()\n            all_preds.extend(preds)\n            all_true.extend(y.numpy())\n    acc = accuracy_score(all_true, all_preds)\n    print(f\"Validation Accuracy: {acc:.4f}\")\n\n    # Save best\n    if acc > best_acc:\n        best_acc = acc\n        best_model_state = classifier.state_dict()\n        torch.save(best_model_state, MODEL_BEST_PATH)\n        print(f\"[✔] New best model saved to {MODEL_BEST_PATH}\")\n        stagnation_counter = 0\n    else:\n        stagnation_counter += 1\n\n    # Early stopping condition\n    if stagnation_counter >= PATIENCE:\n        print(f\"[!] Early stopping after {epoch+1} epochs (no improvement in {PATIENCE} rounds)\")\n        break\n\n# Save final model too\ntorch.save(classifier.state_dict(), MODEL_PT_PATH)\nprint(f\"Final model saved to {MODEL_PT_PATH}\")\n\n# Export ONNX\nprint(f\"Exporting to ONNX: {MODEL_ONNX_PATH}\")\ndummy_input = torch.randn(1, INPUT_DIM).to(DEVICE)\ntorch.onnx.export(\n    classifier, dummy_input, MODEL_ONNX_PATH,\n    input_names=[\"input\"], output_names=[\"output\"],\n    dynamic_axes={\"input\": {0: \"batch_size\"}, \"output\": {0: \"batch_size\"}},\n    export_params=True, opset_version=12, do_constant_folding=True,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-22T08:47:55.414246Z","iopub.execute_input":"2025-06-22T08:47:55.414700Z","iopub.status.idle":"2025-06-22T12:52:09.614334Z","shell.execute_reply.started":"2025-06-22T08:47:55.414679Z","shell.execute_reply":"2025-06-22T12:52:09.613638Z"}},"outputs":[{"name":"stderr","text":"2025-06-22 08:48:07.324160: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1750582087.531785      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1750582087.593052      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"Loading wav2vec2 model…\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/257 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"08d4a2e303234b8d8205b7cb6d9382de"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.09k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"83ca774908be4077a4d83c28397c5cfd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/697 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1010e4615aae4d4d8f48d760f7e874db"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/30.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"087f6bb2b4c741cc9acd54ea05095fd5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/96.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6854686377a74140a9e5ba75f4124b8f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/2.37k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"165f44f14a634db5ab8122191a017b9f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/378M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4ef79d8324314fdaaa2cdc63f692b89c"}},"metadata":{}},{"name":"stdout","text":"Indexing dataset …\nSaved label names to labels_wav2vec2.txt\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1: 100%|██████████| 374/374 [01:14<00:00,  5.00it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1  |  avg loss: 3.9058\nValidation Accuracy: 0.0267\n[✔] New best model saved to best_model.pt\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2: 100%|██████████| 374/374 [01:18<00:00,  4.76it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2  |  avg loss: 3.8572\nValidation Accuracy: 0.0254\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3: 100%|██████████| 374/374 [01:18<00:00,  4.75it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3  |  avg loss: 3.7946\nValidation Accuracy: 0.0361\n[✔] New best model saved to best_model.pt\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4: 100%|██████████| 374/374 [01:18<00:00,  4.75it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4  |  avg loss: 3.7499\nValidation Accuracy: 0.0628\n[✔] New best model saved to best_model.pt\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5: 100%|██████████| 374/374 [01:18<00:00,  4.75it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5  |  avg loss: 3.7017\nValidation Accuracy: 0.0508\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6: 100%|██████████| 374/374 [01:18<00:00,  4.76it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6  |  avg loss: 3.6411\nValidation Accuracy: 0.0668\n[✔] New best model saved to best_model.pt\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7: 100%|██████████| 374/374 [01:18<00:00,  4.75it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7  |  avg loss: 3.5695\nValidation Accuracy: 0.0882\n[✔] New best model saved to best_model.pt\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8: 100%|██████████| 374/374 [01:18<00:00,  4.75it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8  |  avg loss: 3.5073\nValidation Accuracy: 0.1056\n[✔] New best model saved to best_model.pt\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9: 100%|██████████| 374/374 [01:18<00:00,  4.75it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 9  |  avg loss: 3.4328\nValidation Accuracy: 0.1163\n[✔] New best model saved to best_model.pt\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10: 100%|██████████| 374/374 [01:18<00:00,  4.75it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10  |  avg loss: 3.3581\nValidation Accuracy: 0.1377\n[✔] New best model saved to best_model.pt\n","output_type":"stream"},{"name":"stderr","text":"Epoch 11: 100%|██████████| 374/374 [01:18<00:00,  4.75it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 11  |  avg loss: 3.2757\nValidation Accuracy: 0.1791\n[✔] New best model saved to best_model.pt\n","output_type":"stream"},{"name":"stderr","text":"Epoch 12: 100%|██████████| 374/374 [01:18<00:00,  4.75it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 12  |  avg loss: 3.1859\nValidation Accuracy: 0.1885\n[✔] New best model saved to best_model.pt\n","output_type":"stream"},{"name":"stderr","text":"Epoch 13: 100%|██████████| 374/374 [01:18<00:00,  4.74it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 13  |  avg loss: 3.1083\nValidation Accuracy: 0.2045\n[✔] New best model saved to best_model.pt\n","output_type":"stream"},{"name":"stderr","text":"Epoch 14: 100%|██████████| 374/374 [01:18<00:00,  4.75it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 14  |  avg loss: 2.9984\nValidation Accuracy: 0.2179\n[✔] New best model saved to best_model.pt\n","output_type":"stream"},{"name":"stderr","text":"Epoch 15: 100%|██████████| 374/374 [01:18<00:00,  4.75it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 15  |  avg loss: 2.9096\nValidation Accuracy: 0.2487\n[✔] New best model saved to best_model.pt\n","output_type":"stream"},{"name":"stderr","text":"Epoch 16: 100%|██████████| 374/374 [01:18<00:00,  4.75it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 16  |  avg loss: 2.8057\nValidation Accuracy: 0.2634\n[✔] New best model saved to best_model.pt\n","output_type":"stream"},{"name":"stderr","text":"Epoch 17: 100%|██████████| 374/374 [01:18<00:00,  4.76it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 17  |  avg loss: 2.7346\nValidation Accuracy: 0.2941\n[✔] New best model saved to best_model.pt\n","output_type":"stream"},{"name":"stderr","text":"Epoch 18: 100%|██████████| 374/374 [01:18<00:00,  4.76it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 18  |  avg loss: 2.6524\nValidation Accuracy: 0.2981\n[✔] New best model saved to best_model.pt\n","output_type":"stream"},{"name":"stderr","text":"Epoch 19: 100%|██████████| 374/374 [01:18<00:00,  4.75it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 19  |  avg loss: 2.5729\nValidation Accuracy: 0.3329\n[✔] New best model saved to best_model.pt\n","output_type":"stream"},{"name":"stderr","text":"Epoch 20: 100%|██████████| 374/374 [01:18<00:00,  4.76it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 20  |  avg loss: 2.4934\nValidation Accuracy: 0.3316\n","output_type":"stream"},{"name":"stderr","text":"Epoch 21: 100%|██████████| 374/374 [01:18<00:00,  4.76it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 21  |  avg loss: 2.4382\nValidation Accuracy: 0.3663\n[✔] New best model saved to best_model.pt\n","output_type":"stream"},{"name":"stderr","text":"Epoch 22: 100%|██████████| 374/374 [01:18<00:00,  4.75it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 22  |  avg loss: 2.3869\nValidation Accuracy: 0.3783\n[✔] New best model saved to best_model.pt\n","output_type":"stream"},{"name":"stderr","text":"Epoch 23: 100%|██████████| 374/374 [01:18<00:00,  4.75it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 23  |  avg loss: 2.3330\nValidation Accuracy: 0.3623\n","output_type":"stream"},{"name":"stderr","text":"Epoch 24: 100%|██████████| 374/374 [01:18<00:00,  4.75it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 24  |  avg loss: 2.2984\nValidation Accuracy: 0.3984\n[✔] New best model saved to best_model.pt\n","output_type":"stream"},{"name":"stderr","text":"Epoch 25: 100%|██████████| 374/374 [01:18<00:00,  4.75it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 25  |  avg loss: 2.2273\nValidation Accuracy: 0.4024\n[✔] New best model saved to best_model.pt\n","output_type":"stream"},{"name":"stderr","text":"Epoch 26: 100%|██████████| 374/374 [01:18<00:00,  4.75it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 26  |  avg loss: 2.1896\nValidation Accuracy: 0.4144\n[✔] New best model saved to best_model.pt\n","output_type":"stream"},{"name":"stderr","text":"Epoch 27: 100%|██████████| 374/374 [01:18<00:00,  4.74it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 27  |  avg loss: 2.1486\nValidation Accuracy: 0.4318\n[✔] New best model saved to best_model.pt\n","output_type":"stream"},{"name":"stderr","text":"Epoch 28: 100%|██████████| 374/374 [01:18<00:00,  4.75it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 28  |  avg loss: 2.1080\nValidation Accuracy: 0.4251\n","output_type":"stream"},{"name":"stderr","text":"Epoch 29: 100%|██████████| 374/374 [01:18<00:00,  4.75it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 29  |  avg loss: 2.0739\nValidation Accuracy: 0.4652\n[✔] New best model saved to best_model.pt\n","output_type":"stream"},{"name":"stderr","text":"Epoch 30: 100%|██████████| 374/374 [01:18<00:00,  4.75it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 30  |  avg loss: 2.0433\nValidation Accuracy: 0.4733\n[✔] New best model saved to best_model.pt\n","output_type":"stream"},{"name":"stderr","text":"Epoch 31: 100%|██████████| 374/374 [01:18<00:00,  4.74it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 31  |  avg loss: 1.9835\nValidation Accuracy: 0.4987\n[✔] New best model saved to best_model.pt\n","output_type":"stream"},{"name":"stderr","text":"Epoch 32: 100%|██████████| 374/374 [01:18<00:00,  4.75it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 32  |  avg loss: 1.9537\nValidation Accuracy: 0.4920\n","output_type":"stream"},{"name":"stderr","text":"Epoch 33: 100%|██████████| 374/374 [01:18<00:00,  4.75it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 33  |  avg loss: 1.9120\nValidation Accuracy: 0.4693\n","output_type":"stream"},{"name":"stderr","text":"Epoch 34: 100%|██████████| 374/374 [01:18<00:00,  4.75it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 34  |  avg loss: 1.8722\nValidation Accuracy: 0.4973\n","output_type":"stream"},{"name":"stderr","text":"Epoch 35: 100%|██████████| 374/374 [01:18<00:00,  4.74it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 35  |  avg loss: 1.8334\nValidation Accuracy: 0.5080\n[✔] New best model saved to best_model.pt\n","output_type":"stream"},{"name":"stderr","text":"Epoch 36: 100%|██████████| 374/374 [01:18<00:00,  4.75it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 36  |  avg loss: 1.8097\nValidation Accuracy: 0.5134\n[✔] New best model saved to best_model.pt\n","output_type":"stream"},{"name":"stderr","text":"Epoch 37: 100%|██████████| 374/374 [01:18<00:00,  4.75it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 37  |  avg loss: 1.7564\nValidation Accuracy: 0.5241\n[✔] New best model saved to best_model.pt\n","output_type":"stream"},{"name":"stderr","text":"Epoch 38: 100%|██████████| 374/374 [01:18<00:00,  4.75it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 38  |  avg loss: 1.7487\nValidation Accuracy: 0.5334\n[✔] New best model saved to best_model.pt\n","output_type":"stream"},{"name":"stderr","text":"Epoch 39: 100%|██████████| 374/374 [01:18<00:00,  4.74it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 39  |  avg loss: 1.7118\nValidation Accuracy: 0.5455\n[✔] New best model saved to best_model.pt\n","output_type":"stream"},{"name":"stderr","text":"Epoch 40: 100%|██████████| 374/374 [01:18<00:00,  4.75it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 40  |  avg loss: 1.6693\nValidation Accuracy: 0.5374\n","output_type":"stream"},{"name":"stderr","text":"Epoch 41: 100%|██████████| 374/374 [01:18<00:00,  4.75it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 41  |  avg loss: 1.6509\nValidation Accuracy: 0.5548\n[✔] New best model saved to best_model.pt\n","output_type":"stream"},{"name":"stderr","text":"Epoch 42: 100%|██████████| 374/374 [01:18<00:00,  4.74it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 42  |  avg loss: 1.6205\nValidation Accuracy: 0.5521\n","output_type":"stream"},{"name":"stderr","text":"Epoch 43: 100%|██████████| 374/374 [01:18<00:00,  4.76it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 43  |  avg loss: 1.6034\nValidation Accuracy: 0.5468\n","output_type":"stream"},{"name":"stderr","text":"Epoch 44: 100%|██████████| 374/374 [01:18<00:00,  4.74it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 44  |  avg loss: 1.5730\nValidation Accuracy: 0.5642\n[✔] New best model saved to best_model.pt\n","output_type":"stream"},{"name":"stderr","text":"Epoch 45: 100%|██████████| 374/374 [01:18<00:00,  4.74it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 45  |  avg loss: 1.5652\nValidation Accuracy: 0.5735\n[✔] New best model saved to best_model.pt\n","output_type":"stream"},{"name":"stderr","text":"Epoch 46: 100%|██████████| 374/374 [01:18<00:00,  4.76it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 46  |  avg loss: 1.5226\nValidation Accuracy: 0.5789\n[✔] New best model saved to best_model.pt\n","output_type":"stream"},{"name":"stderr","text":"Epoch 47: 100%|██████████| 374/374 [01:18<00:00,  4.76it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 47  |  avg loss: 1.5192\nValidation Accuracy: 0.5869\n[✔] New best model saved to best_model.pt\n","output_type":"stream"},{"name":"stderr","text":"Epoch 48: 100%|██████████| 374/374 [01:18<00:00,  4.75it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 48  |  avg loss: 1.4981\nValidation Accuracy: 0.5869\n","output_type":"stream"},{"name":"stderr","text":"Epoch 49: 100%|██████████| 374/374 [01:18<00:00,  4.75it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 49  |  avg loss: 1.4387\nValidation Accuracy: 0.5816\n","output_type":"stream"},{"name":"stderr","text":"Epoch 50: 100%|██████████| 374/374 [01:18<00:00,  4.74it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 50  |  avg loss: 1.4298\nValidation Accuracy: 0.5816\n","output_type":"stream"},{"name":"stderr","text":"Epoch 51: 100%|██████████| 374/374 [01:18<00:00,  4.75it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 51  |  avg loss: 1.4202\nValidation Accuracy: 0.5842\n","output_type":"stream"},{"name":"stderr","text":"Epoch 52: 100%|██████████| 374/374 [01:18<00:00,  4.75it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 52  |  avg loss: 1.4110\nValidation Accuracy: 0.5976\n[✔] New best model saved to best_model.pt\n","output_type":"stream"},{"name":"stderr","text":"Epoch 53: 100%|██████████| 374/374 [01:18<00:00,  4.75it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 53  |  avg loss: 1.3818\nValidation Accuracy: 0.5963\n","output_type":"stream"},{"name":"stderr","text":"Epoch 54: 100%|██████████| 374/374 [01:18<00:00,  4.75it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 54  |  avg loss: 1.3623\nValidation Accuracy: 0.5802\n","output_type":"stream"},{"name":"stderr","text":"Epoch 55: 100%|██████████| 374/374 [01:18<00:00,  4.74it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 55  |  avg loss: 1.3441\nValidation Accuracy: 0.6163\n[✔] New best model saved to best_model.pt\n","output_type":"stream"},{"name":"stderr","text":"Epoch 56: 100%|██████████| 374/374 [01:18<00:00,  4.75it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 56  |  avg loss: 1.3386\nValidation Accuracy: 0.6096\n","output_type":"stream"},{"name":"stderr","text":"Epoch 57: 100%|██████████| 374/374 [01:18<00:00,  4.75it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 57  |  avg loss: 1.3227\nValidation Accuracy: 0.6270\n[✔] New best model saved to best_model.pt\n","output_type":"stream"},{"name":"stderr","text":"Epoch 58: 100%|██████████| 374/374 [01:18<00:00,  4.75it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 58  |  avg loss: 1.2946\nValidation Accuracy: 0.6230\n","output_type":"stream"},{"name":"stderr","text":"Epoch 59: 100%|██████████| 374/374 [01:18<00:00,  4.74it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 59  |  avg loss: 1.3038\nValidation Accuracy: 0.6257\n","output_type":"stream"},{"name":"stderr","text":"Epoch 60: 100%|██████████| 374/374 [01:18<00:00,  4.75it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 60  |  avg loss: 1.2726\nValidation Accuracy: 0.6390\n[✔] New best model saved to best_model.pt\n","output_type":"stream"},{"name":"stderr","text":"Epoch 61: 100%|██████████| 374/374 [01:18<00:00,  4.75it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 61  |  avg loss: 1.2586\nValidation Accuracy: 0.6150\n","output_type":"stream"},{"name":"stderr","text":"Epoch 62: 100%|██████████| 374/374 [01:18<00:00,  4.75it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 62  |  avg loss: 1.2420\nValidation Accuracy: 0.6377\n","output_type":"stream"},{"name":"stderr","text":"Epoch 63: 100%|██████████| 374/374 [01:18<00:00,  4.75it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 63  |  avg loss: 1.2379\nValidation Accuracy: 0.6364\n","output_type":"stream"},{"name":"stderr","text":"Epoch 64: 100%|██████████| 374/374 [01:18<00:00,  4.74it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 64  |  avg loss: 1.2112\nValidation Accuracy: 0.6350\n","output_type":"stream"},{"name":"stderr","text":"Epoch 65: 100%|██████████| 374/374 [01:18<00:00,  4.75it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 65  |  avg loss: 1.2048\nValidation Accuracy: 0.6417\n[✔] New best model saved to best_model.pt\n","output_type":"stream"},{"name":"stderr","text":"Epoch 66: 100%|██████████| 374/374 [01:18<00:00,  4.75it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 66  |  avg loss: 1.1800\nValidation Accuracy: 0.6604\n[✔] New best model saved to best_model.pt\n","output_type":"stream"},{"name":"stderr","text":"Epoch 67: 100%|██████████| 374/374 [01:18<00:00,  4.74it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 67  |  avg loss: 1.1880\nValidation Accuracy: 0.6364\n","output_type":"stream"},{"name":"stderr","text":"Epoch 68: 100%|██████████| 374/374 [01:18<00:00,  4.74it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 68  |  avg loss: 1.1492\nValidation Accuracy: 0.6658\n[✔] New best model saved to best_model.pt\n","output_type":"stream"},{"name":"stderr","text":"Epoch 69: 100%|██████████| 374/374 [01:18<00:00,  4.75it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 69  |  avg loss: 1.1457\nValidation Accuracy: 0.6738\n[✔] New best model saved to best_model.pt\n","output_type":"stream"},{"name":"stderr","text":"Epoch 70: 100%|██████████| 374/374 [01:18<00:00,  4.75it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 70  |  avg loss: 1.1446\nValidation Accuracy: 0.6725\n","output_type":"stream"},{"name":"stderr","text":"Epoch 71: 100%|██████████| 374/374 [01:18<00:00,  4.76it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 71  |  avg loss: 1.1243\nValidation Accuracy: 0.6658\n","output_type":"stream"},{"name":"stderr","text":"Epoch 72: 100%|██████████| 374/374 [01:18<00:00,  4.75it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 72  |  avg loss: 1.0998\nValidation Accuracy: 0.6711\n","output_type":"stream"},{"name":"stderr","text":"Epoch 73: 100%|██████████| 374/374 [01:18<00:00,  4.75it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 73  |  avg loss: 1.1092\nValidation Accuracy: 0.6845\n[✔] New best model saved to best_model.pt\n","output_type":"stream"},{"name":"stderr","text":"Epoch 74: 100%|██████████| 374/374 [01:18<00:00,  4.75it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 74  |  avg loss: 1.1040\nValidation Accuracy: 0.6751\n","output_type":"stream"},{"name":"stderr","text":"Epoch 75: 100%|██████████| 374/374 [01:18<00:00,  4.75it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 75  |  avg loss: 1.0769\nValidation Accuracy: 0.6725\n","output_type":"stream"},{"name":"stderr","text":"Epoch 76: 100%|██████████| 374/374 [01:18<00:00,  4.76it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 76  |  avg loss: 1.0525\nValidation Accuracy: 0.6671\n","output_type":"stream"},{"name":"stderr","text":"Epoch 77: 100%|██████████| 374/374 [01:18<00:00,  4.75it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 77  |  avg loss: 1.0525\nValidation Accuracy: 0.6898\n[✔] New best model saved to best_model.pt\n","output_type":"stream"},{"name":"stderr","text":"Epoch 78: 100%|██████████| 374/374 [01:18<00:00,  4.75it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 78  |  avg loss: 1.0479\nValidation Accuracy: 0.6805\n","output_type":"stream"},{"name":"stderr","text":"Epoch 79: 100%|██████████| 374/374 [01:18<00:00,  4.75it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 79  |  avg loss: 1.0211\nValidation Accuracy: 0.6939\n[✔] New best model saved to best_model.pt\n","output_type":"stream"},{"name":"stderr","text":"Epoch 80: 100%|██████████| 374/374 [01:18<00:00,  4.75it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 80  |  avg loss: 1.0156\nValidation Accuracy: 0.6872\n","output_type":"stream"},{"name":"stderr","text":"Epoch 81: 100%|██████████| 374/374 [01:18<00:00,  4.75it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 81  |  avg loss: 1.0146\nValidation Accuracy: 0.6872\n","output_type":"stream"},{"name":"stderr","text":"Epoch 82: 100%|██████████| 374/374 [01:18<00:00,  4.76it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 82  |  avg loss: 0.9997\nValidation Accuracy: 0.7139\n[✔] New best model saved to best_model.pt\n","output_type":"stream"},{"name":"stderr","text":"Epoch 83: 100%|██████████| 374/374 [01:18<00:00,  4.76it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 83  |  avg loss: 0.9803\nValidation Accuracy: 0.6952\n","output_type":"stream"},{"name":"stderr","text":"Epoch 84: 100%|██████████| 374/374 [01:18<00:00,  4.76it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 84  |  avg loss: 0.9837\nValidation Accuracy: 0.6791\n","output_type":"stream"},{"name":"stderr","text":"Epoch 85: 100%|██████████| 374/374 [01:18<00:00,  4.75it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 85  |  avg loss: 0.9694\nValidation Accuracy: 0.7059\n","output_type":"stream"},{"name":"stderr","text":"Epoch 86: 100%|██████████| 374/374 [01:18<00:00,  4.74it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 86  |  avg loss: 0.9651\nValidation Accuracy: 0.6952\n","output_type":"stream"},{"name":"stderr","text":"Epoch 87: 100%|██████████| 374/374 [01:18<00:00,  4.76it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 87  |  avg loss: 0.9389\nValidation Accuracy: 0.7032\n","output_type":"stream"},{"name":"stderr","text":"Epoch 88: 100%|██████████| 374/374 [01:18<00:00,  4.75it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 88  |  avg loss: 0.9320\nValidation Accuracy: 0.7219\n[✔] New best model saved to best_model.pt\n","output_type":"stream"},{"name":"stderr","text":"Epoch 89: 100%|██████████| 374/374 [01:18<00:00,  4.76it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 89  |  avg loss: 0.9296\nValidation Accuracy: 0.7032\n","output_type":"stream"},{"name":"stderr","text":"Epoch 90: 100%|██████████| 374/374 [01:18<00:00,  4.76it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 90  |  avg loss: 0.9127\nValidation Accuracy: 0.7099\n","output_type":"stream"},{"name":"stderr","text":"Epoch 91: 100%|██████████| 374/374 [01:18<00:00,  4.77it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 91  |  avg loss: 0.9070\nValidation Accuracy: 0.7179\n","output_type":"stream"},{"name":"stderr","text":"Epoch 92: 100%|██████████| 374/374 [01:18<00:00,  4.76it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 92  |  avg loss: 0.8921\nValidation Accuracy: 0.7326\n[✔] New best model saved to best_model.pt\n","output_type":"stream"},{"name":"stderr","text":"Epoch 93: 100%|██████████| 374/374 [01:18<00:00,  4.76it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 93  |  avg loss: 0.8777\nValidation Accuracy: 0.7072\n","output_type":"stream"},{"name":"stderr","text":"Epoch 94: 100%|██████████| 374/374 [01:18<00:00,  4.76it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 94  |  avg loss: 0.8701\nValidation Accuracy: 0.7179\n","output_type":"stream"},{"name":"stderr","text":"Epoch 95: 100%|██████████| 374/374 [01:18<00:00,  4.76it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 95  |  avg loss: 0.8652\nValidation Accuracy: 0.7099\n","output_type":"stream"},{"name":"stderr","text":"Epoch 96: 100%|██████████| 374/374 [01:18<00:00,  4.76it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 96  |  avg loss: 0.8793\nValidation Accuracy: 0.7246\n","output_type":"stream"},{"name":"stderr","text":"Epoch 97: 100%|██████████| 374/374 [01:18<00:00,  4.76it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 97  |  avg loss: 0.8692\nValidation Accuracy: 0.7233\n","output_type":"stream"},{"name":"stderr","text":"Epoch 98: 100%|██████████| 374/374 [01:18<00:00,  4.76it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 98  |  avg loss: 0.8462\nValidation Accuracy: 0.7086\n","output_type":"stream"},{"name":"stderr","text":"Epoch 99: 100%|██████████| 374/374 [01:18<00:00,  4.76it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 99  |  avg loss: 0.8392\nValidation Accuracy: 0.7286\n","output_type":"stream"},{"name":"stderr","text":"Epoch 100: 100%|██████████| 374/374 [01:18<00:00,  4.76it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 100  |  avg loss: 0.8339\nValidation Accuracy: 0.7366\n[✔] New best model saved to best_model.pt\n","output_type":"stream"},{"name":"stderr","text":"Epoch 101: 100%|██████████| 374/374 [01:18<00:00,  4.76it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 101  |  avg loss: 0.8094\nValidation Accuracy: 0.7152\n","output_type":"stream"},{"name":"stderr","text":"Epoch 102: 100%|██████████| 374/374 [01:18<00:00,  4.76it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 102  |  avg loss: 0.8146\nValidation Accuracy: 0.7406\n[✔] New best model saved to best_model.pt\n","output_type":"stream"},{"name":"stderr","text":"Epoch 103: 100%|██████████| 374/374 [01:18<00:00,  4.76it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 103  |  avg loss: 0.7981\nValidation Accuracy: 0.7206\n","output_type":"stream"},{"name":"stderr","text":"Epoch 104: 100%|██████████| 374/374 [01:18<00:00,  4.76it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 104  |  avg loss: 0.7888\nValidation Accuracy: 0.7313\n","output_type":"stream"},{"name":"stderr","text":"Epoch 105: 100%|██████████| 374/374 [01:18<00:00,  4.76it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 105  |  avg loss: 0.8050\nValidation Accuracy: 0.7313\n","output_type":"stream"},{"name":"stderr","text":"Epoch 106: 100%|██████████| 374/374 [01:18<00:00,  4.76it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 106  |  avg loss: 0.7935\nValidation Accuracy: 0.7340\n","output_type":"stream"},{"name":"stderr","text":"Epoch 107: 100%|██████████| 374/374 [01:18<00:00,  4.76it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 107  |  avg loss: 0.7876\nValidation Accuracy: 0.7500\n[✔] New best model saved to best_model.pt\n","output_type":"stream"},{"name":"stderr","text":"Epoch 108: 100%|██████████| 374/374 [01:18<00:00,  4.76it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 108  |  avg loss: 0.7624\nValidation Accuracy: 0.7393\n","output_type":"stream"},{"name":"stderr","text":"Epoch 109: 100%|██████████| 374/374 [01:18<00:00,  4.76it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 109  |  avg loss: 0.7441\nValidation Accuracy: 0.7406\n","output_type":"stream"},{"name":"stderr","text":"Epoch 110: 100%|██████████| 374/374 [01:18<00:00,  4.75it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 110  |  avg loss: 0.7536\nValidation Accuracy: 0.7406\n","output_type":"stream"},{"name":"stderr","text":"Epoch 111: 100%|██████████| 374/374 [01:18<00:00,  4.75it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 111  |  avg loss: 0.7543\nValidation Accuracy: 0.7273\n","output_type":"stream"},{"name":"stderr","text":"Epoch 112: 100%|██████████| 374/374 [01:18<00:00,  4.76it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 112  |  avg loss: 0.7492\nValidation Accuracy: 0.7219\n","output_type":"stream"},{"name":"stderr","text":"Epoch 113: 100%|██████████| 374/374 [01:18<00:00,  4.76it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 113  |  avg loss: 0.7051\nValidation Accuracy: 0.7406\n","output_type":"stream"},{"name":"stderr","text":"Epoch 114: 100%|██████████| 374/374 [01:18<00:00,  4.75it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 114  |  avg loss: 0.7158\nValidation Accuracy: 0.7500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 115: 100%|██████████| 374/374 [01:18<00:00,  4.75it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 115  |  avg loss: 0.7067\nValidation Accuracy: 0.7527\n[✔] New best model saved to best_model.pt\n","output_type":"stream"},{"name":"stderr","text":"Epoch 116: 100%|██████████| 374/374 [01:18<00:00,  4.75it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 116  |  avg loss: 0.7078\nValidation Accuracy: 0.7353\n","output_type":"stream"},{"name":"stderr","text":"Epoch 117: 100%|██████████| 374/374 [01:18<00:00,  4.75it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 117  |  avg loss: 0.6864\nValidation Accuracy: 0.7567\n[✔] New best model saved to best_model.pt\n","output_type":"stream"},{"name":"stderr","text":"Epoch 118: 100%|██████████| 374/374 [01:18<00:00,  4.75it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 118  |  avg loss: 0.6918\nValidation Accuracy: 0.7594\n[✔] New best model saved to best_model.pt\n","output_type":"stream"},{"name":"stderr","text":"Epoch 119: 100%|██████████| 374/374 [01:18<00:00,  4.76it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 119  |  avg loss: 0.6881\nValidation Accuracy: 0.7634\n[✔] New best model saved to best_model.pt\n","output_type":"stream"},{"name":"stderr","text":"Epoch 120: 100%|██████████| 374/374 [01:18<00:00,  4.75it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 120  |  avg loss: 0.6791\nValidation Accuracy: 0.7580\n","output_type":"stream"},{"name":"stderr","text":"Epoch 121: 100%|██████████| 374/374 [01:18<00:00,  4.76it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 121  |  avg loss: 0.6885\nValidation Accuracy: 0.7647\n[✔] New best model saved to best_model.pt\n","output_type":"stream"},{"name":"stderr","text":"Epoch 122: 100%|██████████| 374/374 [01:18<00:00,  4.76it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 122  |  avg loss: 0.6910\nValidation Accuracy: 0.7580\n","output_type":"stream"},{"name":"stderr","text":"Epoch 123: 100%|██████████| 374/374 [01:18<00:00,  4.76it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 123  |  avg loss: 0.6512\nValidation Accuracy: 0.7553\n","output_type":"stream"},{"name":"stderr","text":"Epoch 124: 100%|██████████| 374/374 [01:18<00:00,  4.76it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 124  |  avg loss: 0.6658\nValidation Accuracy: 0.7580\n","output_type":"stream"},{"name":"stderr","text":"Epoch 125: 100%|██████████| 374/374 [01:18<00:00,  4.76it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 125  |  avg loss: 0.6516\nValidation Accuracy: 0.7553\n","output_type":"stream"},{"name":"stderr","text":"Epoch 126: 100%|██████████| 374/374 [01:18<00:00,  4.76it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 126  |  avg loss: 0.6401\nValidation Accuracy: 0.7674\n[✔] New best model saved to best_model.pt\n","output_type":"stream"},{"name":"stderr","text":"Epoch 127: 100%|██████████| 374/374 [01:18<00:00,  4.76it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 127  |  avg loss: 0.6391\nValidation Accuracy: 0.7701\n[✔] New best model saved to best_model.pt\n","output_type":"stream"},{"name":"stderr","text":"Epoch 128: 100%|██████████| 374/374 [01:18<00:00,  4.76it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 128  |  avg loss: 0.6392\nValidation Accuracy: 0.7821\n[✔] New best model saved to best_model.pt\n","output_type":"stream"},{"name":"stderr","text":"Epoch 129: 100%|██████████| 374/374 [01:18<00:00,  4.76it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 129  |  avg loss: 0.6272\nValidation Accuracy: 0.7620\n","output_type":"stream"},{"name":"stderr","text":"Epoch 130: 100%|██████████| 374/374 [01:18<00:00,  4.76it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 130  |  avg loss: 0.6389\nValidation Accuracy: 0.7767\n","output_type":"stream"},{"name":"stderr","text":"Epoch 131: 100%|██████████| 374/374 [01:18<00:00,  4.76it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 131  |  avg loss: 0.6015\nValidation Accuracy: 0.7553\n","output_type":"stream"},{"name":"stderr","text":"Epoch 132: 100%|██████████| 374/374 [01:18<00:00,  4.76it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 132  |  avg loss: 0.6225\nValidation Accuracy: 0.7754\n","output_type":"stream"},{"name":"stderr","text":"Epoch 133: 100%|██████████| 374/374 [01:18<00:00,  4.76it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 133  |  avg loss: 0.6281\nValidation Accuracy: 0.7781\n","output_type":"stream"},{"name":"stderr","text":"Epoch 134: 100%|██████████| 374/374 [01:18<00:00,  4.76it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 134  |  avg loss: 0.6020\nValidation Accuracy: 0.7794\n","output_type":"stream"},{"name":"stderr","text":"Epoch 135: 100%|██████████| 374/374 [01:18<00:00,  4.76it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 135  |  avg loss: 0.6027\nValidation Accuracy: 0.7834\n[✔] New best model saved to best_model.pt\n","output_type":"stream"},{"name":"stderr","text":"Epoch 136: 100%|██████████| 374/374 [01:18<00:00,  4.76it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 136  |  avg loss: 0.5971\nValidation Accuracy: 0.7620\n","output_type":"stream"},{"name":"stderr","text":"Epoch 137: 100%|██████████| 374/374 [01:18<00:00,  4.76it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 137  |  avg loss: 0.5911\nValidation Accuracy: 0.7741\n","output_type":"stream"},{"name":"stderr","text":"Epoch 138: 100%|██████████| 374/374 [01:18<00:00,  4.76it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 138  |  avg loss: 0.5783\nValidation Accuracy: 0.7781\n","output_type":"stream"},{"name":"stderr","text":"Epoch 139: 100%|██████████| 374/374 [01:18<00:00,  4.76it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 139  |  avg loss: 0.5715\nValidation Accuracy: 0.7955\n[✔] New best model saved to best_model.pt\n","output_type":"stream"},{"name":"stderr","text":"Epoch 140: 100%|██████████| 374/374 [01:18<00:00,  4.76it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 140  |  avg loss: 0.5885\nValidation Accuracy: 0.7794\n","output_type":"stream"},{"name":"stderr","text":"Epoch 141: 100%|██████████| 374/374 [01:18<00:00,  4.76it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 141  |  avg loss: 0.5638\nValidation Accuracy: 0.7767\n","output_type":"stream"},{"name":"stderr","text":"Epoch 142: 100%|██████████| 374/374 [01:18<00:00,  4.76it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 142  |  avg loss: 0.5496\nValidation Accuracy: 0.7767\n","output_type":"stream"},{"name":"stderr","text":"Epoch 143: 100%|██████████| 374/374 [01:18<00:00,  4.75it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 143  |  avg loss: 0.5663\nValidation Accuracy: 0.7848\n","output_type":"stream"},{"name":"stderr","text":"Epoch 144: 100%|██████████| 374/374 [01:18<00:00,  4.75it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 144  |  avg loss: 0.5544\nValidation Accuracy: 0.7794\n","output_type":"stream"},{"name":"stderr","text":"Epoch 145: 100%|██████████| 374/374 [01:18<00:00,  4.75it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 145  |  avg loss: 0.5504\nValidation Accuracy: 0.7874\n","output_type":"stream"},{"name":"stderr","text":"Epoch 146: 100%|██████████| 374/374 [01:19<00:00,  4.73it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 146  |  avg loss: 0.5598\nValidation Accuracy: 0.7848\n","output_type":"stream"},{"name":"stderr","text":"Epoch 147: 100%|██████████| 374/374 [01:18<00:00,  4.76it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 147  |  avg loss: 0.5358\nValidation Accuracy: 0.7888\n","output_type":"stream"},{"name":"stderr","text":"Epoch 148: 100%|██████████| 374/374 [01:18<00:00,  4.76it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 148  |  avg loss: 0.5400\nValidation Accuracy: 0.7901\n","output_type":"stream"},{"name":"stderr","text":"Epoch 149: 100%|██████████| 374/374 [01:18<00:00,  4.75it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 149  |  avg loss: 0.5456\nValidation Accuracy: 0.7861\n[!] Early stopping after 149 epochs (no improvement in 10 rounds)\nFinal model saved to korean_command_classifier.pt\nExporting to ONNX: korean_command_classifier.onnx\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# ---------------- Inference Helper --------------\n\ndef predict_command(wav_path: str):\n    \"\"\"Return the predicted label (string) for a single Korean .wav file.\"\"\"\n    classifier.eval()\n    waveform, sr = torchaudio.load(wav_path)\n    waveform = waveform.mean(0, keepdim=True)\n    if sr != SAMPLE_RATE:\n        waveform = torchaudio.functional.resample(waveform, sr, SAMPLE_RATE)\n    waveform = waveform[:, :SAMPLE_RATE * MAX_AUDIO_LEN]\n    inputs   = processor(waveform.squeeze(), sampling_rate=SAMPLE_RATE, return_tensors=\"pt\", padding=True)\n    with torch.no_grad():\n        hidden = feature_extractor(inputs.input_values.to(DEVICE)).last_hidden_state\n        emb    = hidden.mean(dim=1)\n        logits = classifier(emb)\n        pred_id = logits.argmax(dim=1).item()\n    return label_encoder.inverse_transform([pred_id])[0]\n\nif __name__ == \"__main__\":\n    test_wav = \"/kaggle/working/korean_speech_commands_kaggle/all_comand/400미터/noaug_clean_speaker0_400meters_1.wav\"  # TODO: change to your file\n    if os.path.exists(test_wav):\n        print(\"Predicted command:\", predict_command(test_wav))\n    else:\n        print(\"Put a test .wav path in `test_wav` and run again.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-22T12:56:38.903516Z","iopub.execute_input":"2025-06-22T12:56:38.904072Z","iopub.status.idle":"2025-06-22T12:56:38.950379Z","shell.execute_reply.started":"2025-06-22T12:56:38.904049Z","shell.execute_reply":"2025-06-22T12:56:38.949731Z"}},"outputs":[{"name":"stdout","text":"Predicted command: 400미터\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}