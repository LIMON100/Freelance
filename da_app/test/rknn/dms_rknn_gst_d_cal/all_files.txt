# File: CMakeLists.txt
cmake_minimum_required(VERSION 3.10)

project(rknn_multi_model_demo)

if (ENABLE_ASAN)
    message(STATUS "BUILD WITH ADDRESS SANITIZER")
    set(CMAKE_C_FLAGS_DEBUG "${CMAKE_C_FLAGS_DEBUG} -fno-omit-frame-pointer -fsanitize=address")
    set(CMAKE_CXX_FLAGS_DEBUG "${CMAKE_CXX_FLAGS_DEBUG} -fno-omit-frame-pointer -fsanitize=address")
    set(CMAKE_LINKER_FLAGS_DEBUG "${CMAKE_LINKER_FLAGS_DEBUG} -fno-omit-frame-pointer -fsanitize=address")
endif ()

# --- Find Eigen3 ---
find_package(Eigen3 REQUIRED)
if(NOT Eigen3_FOUND)
    message(FATAL_ERROR "Eigen3 not found! Please install libeigen3-dev or set Eigen3_DIR.")
endif()

# --- Find PkgConfig ---
find_package(PkgConfig REQUIRED)

# --- Find GStreamer and related modules ---
pkg_search_module(LIBXML2 REQUIRED libxml-2.0)
pkg_search_module(GST REQUIRED gstreamer-1.0)
pkg_search_module(GST_VIDEO REQUIRED gstreamer-video-1.0)
pkg_search_module(GST_RTSP REQUIRED gstreamer-rtsp-1.0)
pkg_search_module(GST_RTSP_SERVER REQUIRED gstreamer-rtsp-server-1.0)
pkg_search_module(GST_GL REQUIRED gstreamer-gl-1.0)
pkg_search_module(GST_GL_EGL REQUIRED gstreamer-gl-egl-1.0)
pkg_search_module(GST_APP REQUIRED gstreamer-app-1.0) # Added for appsink
pkg_search_module(CV REQUIRED opencv4)
pkg_search_module(GST_ALLOCATORS REQUIRED gstreamer-allocators-1.0)

# --- Use relative paths from the OLD CMakeLists ---
add_subdirectory(${CMAKE_CURRENT_SOURCE_DIR}/../../../3rdparty/ 3rdparty.out)
add_subdirectory(${CMAKE_CURRENT_SOURCE_DIR}/../../../utils/ utils.out)

# add_definitions(-DZERO_COPY)

set(CMAKE_INSTALL_RPATH "$ORIGIN/../lib")

# --- Select correct YOLO implementation based on SOC ---
set(rknpu_yolo11_impl_file yolo_detector/rknpu2/yolo11.cc) # Default

if (TARGET_SOC STREQUAL "rv1106" OR TARGET_SOC STREQUAL "rv1103")
    add_definitions(-DRV1106_1103)
    set(rknpu_yolo11_impl_file yolo_detector/rknpu2/yolo11_rv1106_1103.cc)
    include_directories(${CMAKE_CURRENT_SOURCE_DIR}/../../../3rdparty/allocator/dma)
endif()

if(TARGET_SOC STREQUAL "rk1808" OR TARGET_SOC STREQUAL "rv1109" OR TARGET_SOC STREQUAL "rv1126")
    add_definitions(-DRKNPU1)
    set(rknpu_yolo11_impl_file yolo_detector/rknpu1/yolo11.cc)
endif()

# Define executable and source files
add_executable(${PROJECT_NAME}
    main.cc                         # Main application logic
    face_analyzer/face_analyzer.cc  # Face/Iris analysis implementation
    yolo_detector/postprocess.cc    # YOLO postprocessing
    ${rknpu_yolo11_impl_file}       # Selected YOLO implementation
    # --- Add Behavior Analysis Sources ---
    behavior_analysis/BlinkDetector.cpp
    behavior_analysis/YawnDetector.cpp
    behavior_analysis/HeadPoseTracker.cpp
    behavior_analysis/KSSCalculator.cpp
)

# Link libraries
target_link_libraries(${PROJECT_NAME}
    imageutils
    fileutils
    imagedrawing
    dl
    librga.so
    librknnrt.so
    ${GST_LIBRARIES}
    ${GST_VIDEO_LIBRARIES}
    ${GST_RTSP_LIBRARIES}
    ${GST_RTSP_SERVER_LIBRARIES}
    ${GST_GL_LIBRARIES}
    ${GST_APP_LIBRARIES}  # Added for appsink
    ${GST_GL_EGL_LIBRARIES}
    ${CV_LIBRARIES}
    ${GST_ALLOCATORS_LIBRARIES}
)

# Platform-specific libraries
if (CMAKE_SYSTEM_NAME STREQUAL "Linux")
    set(THREADS_PREFER_PTHREAD_FLAG ON)
    find_package(Threads REQUIRED)
    target_link_libraries(${PROJECT_NAME} Threads::Threads)
endif()

# Include directories
target_include_directories(${PROJECT_NAME} PRIVATE
    ${CMAKE_CURRENT_SOURCE_DIR}
    ${CMAKE_CURRENT_SOURCE_DIR}/face_analyzer
    ${CMAKE_CURRENT_SOURCE_DIR}/yolo_detector
    ${CMAKE_CURRENT_SOURCE_DIR}/behavior_analysis
    ${CMAKE_CURRENT_SOURCE_DIR}/../../../utils
    ${GST_INCLUDE_DIRS}         # GStreamer include dirs
    ${GST_VIDEO_INCLUDE_DIRS}   # GStreamer video include dirs
    ${GST_RTSP_INCLUDE_DIRS}    # GStreamer RTSP include dirs
    ${GST_RTSP_SERVER_INCLUDE_DIRS}  # GStreamer RTSP server include dirs
    ${GST_GL_INCLUDE_DIRS}      # GStreamer GL include dirs
    ${GST_APP_INCLUDE_DIRS}     # GStreamer app include dirs
    ${GST_GL_EGL_INCLUDE_DIRS}  # GStreamer GL EGL include dirs
    ${CV_INCLUDE_DIRS}          # OpenCV include dirs
    ${EIGEN3_INCLUDE_DIRS}      # Eigen include path
    ${GST_ALLOCATORS_INCLUDE_DIRS}
)

# --- Install Rules ---
install(TARGETS ${PROJECT_NAME} DESTINATION .)
install(FILES ${CMAKE_CURRENT_SOURCE_DIR}/../model/test.jpg ${CMAKE_CURRENT_SOURCE_DIR}/../model/5.jpg DESTINATION model)
install(FILES
    ${CMAKE_CURRENT_SOURCE_DIR}/../model/rf.rknn
    # ${CMAKE_CURRENT_SOURCE_DIR}/../model/faceD.rknn
    ${CMAKE_CURRENT_SOURCE_DIR}/../model/faceL.rknn
    ${CMAKE_CURRENT_SOURCE_DIR}/../model/faceI.rknn
    ${CMAKE_CURRENT_SOURCE_DIR}/../model/od.rknn
    ${CMAKE_CURRENT_SOURCE_DIR}/../model/custom_class.txt
    DESTINATION model
    OPTIONAL
)

# File: main.cc
// #include <stdint.h>
// #include <stdio.h>
// #include <stdlib.h>
// #include <string.h>
// #include <vector>
// #include <string>
// #include <sstream>
// #include <iomanip>
// #include <chrono>
// #include <stdexcept>
// #include <cmath>
// #include <opencv2/core/types.hpp> // Needed for cv::Rect, cv::Point
// #include <opencv2/opencv.hpp>
// #include <thread>
// #include <queue>
// #include <mutex>
// #include <atomic>
// #include <memory>
// #include <sys/sysinfo.h>
// #include <sys/stat.h>
// #include <unistd.h>
// #include <dirent.h>

// // Include project headers
// #include "face_analyzer/face_analyzer.h"
// #include "yolo_detector/yolo11.h"
// #include "behavior_analysis/BlinkDetector.hpp"
// #include "behavior_analysis/YawnDetector.hpp"
// #include "behavior_analysis/HeadPoseTracker.hpp"
// #include "behavior_analysis/KSSCalculator.hpp"
// #include "image_utils.h"         // Make sure crop_image_simple is declared here or implement it
// #include "file_utils.h"
// #include "image_drawing.h"

// // GStreamer Includes
// #include <gst/gst.h>
// #include <gst/video/video.h>
// #include <gst/app/gstappsink.h>
// #include <gst/allocators/gstdmabuf.h>
// #include <gst/app/gstappsrc.h> 

// // Resource Monitoring Includes
// #include <fstream>
// #include <numeric>
// #include <deque>

// // --- Debugging Flags ---
// #define DEBUG_DRAW_ROIS // Comment out to disable drawing ROIs

// // --- Define colors ---
// #ifndef COLOR_MAGENTA
// #define COLOR_MAGENTA (0xFF00FF) // Pinkish/Magenta
// #endif
// #ifndef COLOR_YELLOW
// #define COLOR_YELLOW (0x00FFFF) // Yellow
// #endif
// #ifndef COLOR_WHITE
// #define COLOR_WHITE (0xFFFFFF) // White
// #endif
// #ifndef COLOR_GREEN
// #define COLOR_GREEN (0x00FF00) // Green
// #endif
// #ifndef COLOR_RED
// #define COLOR_RED (0x0000FF) // Red
// #endif
// #ifndef COLOR_BLUE
// #define COLOR_BLUE (0xFF0000) // Blue
// #endif
// #ifndef COLOR_ORANGE
// #define COLOR_ORANGE (0x00A5FF) // Orange
// #endif
// #ifndef COLOR_CYAN
// #define COLOR_CYAN (0xFFFF00) // Cyan
// #endif


// // --- Application Context ---
// typedef struct app_context_t {
//     face_analyzer_app_context_t face_ctx;
//     yolo11_app_context_t        yolo_ctx;
// } app_context_t;

// // --- Global GStreamer elements ---
// static GstElement* pipeline_ = nullptr; // Saving pipeline
// static GstElement* appsrc_ = nullptr;   // Source for saving pipeline
// static GstElement* appsink_ = nullptr;  // Sink for input pipeline

// // --- Helper Functions ---
// // ... (Keep all helper functions: calculate_centroid, convert_landmarks_to_cvpoint, calculate_ear_simple, calculate_mouth_dist_simple, parse_head_pose_value, calculate_stddev) ...
// cv::Point calculate_centroid(const point_t landmarks[], int count) { if (count == 0) return cv::Point(-1, -1); long long sum_x = 0, sum_y = 0; int valid_points = 0; for (int i = 0; i < count; ++i) { if (landmarks[i].x > -10000 && landmarks[i].x < 10000 && landmarks[i].y > -10000 && landmarks[i].y < 10000) { sum_x += landmarks[i].x; sum_y += landmarks[i].y; valid_points++; } } if (valid_points == 0) return cv::Point(-1, -1); return cv::Point(static_cast<int>(sum_x / valid_points), static_cast<int>(sum_y / valid_points)); }
// std::vector<cv::Point> convert_landmarks_to_cvpoint(const point_t landmarks[], int count) { std::vector<cv::Point> cv_landmarks; if (count <= 0) return cv_landmarks; cv_landmarks.reserve(count); for (int i = 0; i < count; ++i) { if (landmarks[i].x > -10000 && landmarks[i].x < 10000 && landmarks[i].y > -10000 && landmarks[i].y < 10000) { cv_landmarks.emplace_back(cv::Point(landmarks[i].x, landmarks[i].y)); } } return cv_landmarks; }
// float calculate_ear_simple(const std::vector<cv::Point>& landmarks, const std::vector<int>& eye_points) { if (landmarks.empty()) return 1.0f; int max_idx = 0; for(int idx : eye_points) { if (idx > max_idx) max_idx = idx; } if (max_idx >= landmarks.size()) { return 1.0f; } try { cv::Point p1=landmarks.at(eye_points.at(0)); cv::Point p2=landmarks.at(eye_points.at(1)); cv::Point p3=landmarks.at(eye_points.at(2)); cv::Point p4=landmarks.at(eye_points.at(3)); cv::Point p5=landmarks.at(eye_points.at(4)); cv::Point p6=landmarks.at(eye_points.at(5)); double v1=cv::norm(p2-p6); double v2=cv::norm(p3-p5); double h=cv::norm(p1-p4); if(h<1e-6) return 1.0f; return static_cast<float>((v1+v2)/(2.0*h)); } catch(const std::out_of_range& oor) { std::cerr << "Out of Range error in calculate_ear_simple: " << oor.what() << std::endl; return 1.0f; } catch(...) { std::cerr << "Unknown exception in calculate_ear_simple" << std::endl; return 1.0f; } }
// double calculate_mouth_dist_simple(const std::vector<cv::Point>& landmarks) { if (landmarks.empty() || landmarks.size() <= 14) return 0.0; try { return cv::norm(landmarks.at(13)-landmarks.at(14)); } catch(const std::out_of_range& oor) { std::cerr << "Out of Range error in calculate_mouth_dist_simple: " << oor.what() << std::endl; return 0.0; } catch (...) { std::cerr << "Unknown exception in calculate_mouth_dist_simple" << std::endl; return 0.0; } }
// double parse_head_pose_value(const std::string& s) { try { std::string n=s; size_t d=n.find(" deg"); if(d!=std::string::npos) n=n.substr(0,d); size_t f=n.find_first_not_of(" \t"); if(f==std::string::npos) return 0.0; size_t l=n.find_last_not_of(" \t"); n=n.substr(f,l-f+1); if(n.empty()) return 0.0; return std::stod(n); } catch (const std::exception& e) { printf("WARN: Ex parse head pose '%s': %s\n", s.c_str(), e.what()); return 0.0; } catch (...) { printf("WARN: Unk ex parse head pose '%s'.\n", s.c_str()); return 0.0; } }
// template <typename T> T calculate_stddev(const std::deque<T>& data) { if (data.size() < 2) return T(0); T sum = std::accumulate(data.begin(), data.end(), T(0)); T mean = sum / data.size(); T sq_sum = std::inner_product(data.begin(), data.end(), data.begin(), T(0)); T variance = sq_sum / data.size() - mean * mean; return std::sqrt(std::max(T(0), variance)); }

// // --- Simple CPU Cropping Function ---
// static int crop_image_simple(image_buffer_t *src_img, image_buffer_t *dst_img, box_rect_t crop_box) {
//     if (!src_img || !src_img->virt_addr || !dst_img) {
//         printf("ERROR: crop_image_simple null input/output pointer.\n");
//         return -1;
//     }

//     int channels = 0;
//     if (src_img->format == IMAGE_FORMAT_RGB888) {
//         channels = 3;
//     }
// // Put preprocessor directives on separate lines
// #ifdef IMAGE_FORMAT_BGR888
//     else if (src_img->format == IMAGE_FORMAT_BGR888) {
//         channels = 3;
//     }
// #endif
//     // Add checks for other formats if needed, otherwise error out
//     else if (channels == 0) { // Check if format wasn't handled
//         printf("ERROR: crop_image_simple unsupported format %d\n", src_img->format);
//         return -1;
//     }

//     int src_w = src_img->width;
//     int src_h = src_img->height;
//     int crop_x = crop_box.left;
//     int crop_y = crop_box.top;
//     int crop_w = crop_box.right - crop_box.left;
//     int crop_h = crop_box.bottom - crop_box.top;

//     if (crop_w <= 0 || crop_h <= 0) {
//         printf("ERROR: crop ROI invalid size (%dx%d) from box [%d,%d,%d,%d]\n",
//                crop_w, crop_h, crop_box.left, crop_box.top, crop_box.right, crop_box.bottom);
//         return -1;
//     }

//     int x_start = std::max(0, crop_x);
//     int y_start = std::max(0, crop_y);
//     int x_end = std::min(src_w, crop_x + crop_w);
//     int y_end = std::min(src_h, crop_y + crop_h);
//     int valid_crop_w = x_end - x_start;
//     int valid_crop_h = y_end - y_start;

//     if (valid_crop_w <= 0 || valid_crop_h <= 0) {
//         printf("ERROR: Clamped crop ROI zero size. Original ROI [%d,%d,%d,%d], Clamped to src %dx%d -> [%d,%d,%d,%d]\n",
//                crop_box.left, crop_box.top, crop_box.right, crop_box.bottom,
//                src_w, src_h,
//                x_start, y_start, x_end, y_end);
//         return -1;
//     }

//     dst_img->width = crop_w;
//     dst_img->height = crop_h;
//     dst_img->format = src_img->format; // Keep original format
//     dst_img->size = crop_w * crop_h * channels; // Use calculated channels

//     // Allocate or check destination buffer
//     if (dst_img->virt_addr == NULL) {
//         dst_img->virt_addr = (unsigned char*)malloc(dst_img->size);
//         if (!dst_img->virt_addr) {
//             printf("ERROR: Failed alloc memory for crop (%d bytes)\n", dst_img->size);
//             return -1;
//         }
//     } else if (dst_img->size < (size_t)(crop_w * crop_h * channels)) {
//         // Optional: Realloc if needed, or return error
//         printf("ERROR: Dest buffer size %d too small for crop %d.\n", dst_img->size, crop_w * crop_h * channels);
//         // Consider reallocating:
//         // void* new_addr = realloc(dst_img->virt_addr, dst_img->size);
//         // if (!new_addr) { /* handle realloc failure */ return -1; }
//         // dst_img->virt_addr = (unsigned char*)new_addr;
//         return -1; // Current behavior is error
//     }

//     memset(dst_img->virt_addr, 0, dst_img->size); // Clear destination

//     unsigned char* src_data = src_img->virt_addr;
//     unsigned char* dst_data = dst_img->virt_addr;
//     // Use width_stride if provided, otherwise calculate based on width
//     size_t src_stride = src_img->width_stride > 0 ? src_img->width_stride : (size_t)src_w * channels;
//     size_t dst_stride = (size_t)crop_w * channels; // Destination is assumed packed
//     int dst_x_offset = x_start - crop_x;
//     int dst_y_offset = y_start - crop_y;

//     // Copy valid region row by row
//     for (int y = 0; y < valid_crop_h; ++y) {
//         unsigned char* src_row_ptr = src_data + (size_t)(y_start + y) * src_stride + (size_t)x_start * channels;
//         unsigned char* dst_row_ptr = dst_data + (size_t)(dst_y_offset + y) * dst_stride + (size_t)dst_x_offset * channels;
//         memcpy(dst_row_ptr, src_row_ptr, (size_t)valid_crop_w * channels);
//     }
//     return 0;
// }
// // --- End Helper Functions ---


// // --- YOLO Worker Thread ---
// struct YoloInputData { long frame_id; std::shared_ptr<image_buffer_t> image; }; struct YoloOutputData { long frame_id; object_detect_result_list results; }; std::queue<YoloInputData> yolo_input_queue; std::queue<YoloOutputData> yolo_output_queue; std::mutex yolo_input_mutex; std::mutex yolo_output_mutex; std::atomic<bool> stop_yolo_worker(false); const int MAX_QUEUE_SIZE = 5;
// void yolo_worker_thread_func(yolo11_app_context_t* yolo_ctx_ptr) { while (!stop_yolo_worker.load()) { YoloInputData input_data; bool got_data = false; { std::unique_lock<std::mutex> lock(yolo_input_mutex); if (!yolo_input_queue.empty()) { input_data = yolo_input_queue.front(); yolo_input_queue.pop(); got_data = true; } } if (got_data && input_data.image) { YoloOutputData output_data; output_data.frame_id = input_data.frame_id; memset(&output_data.results, 0, sizeof(output_data.results)); int ret = inference_yolo11(yolo_ctx_ptr, input_data.image.get(), &output_data.results); if (ret != 0) printf("WARN: YOLO Worker inference failed (frame %ld), ret=%d\n", input_data.frame_id, ret); { std::unique_lock<std::mutex> lock(yolo_output_mutex); if (yolo_output_queue.size() >= MAX_QUEUE_SIZE) yolo_output_queue.pop(); yolo_output_queue.push(output_data); } if (input_data.image->virt_addr) { free(input_data.image->virt_addr); input_data.image->virt_addr = nullptr; } } else { std::this_thread::sleep_for(std::chrono::milliseconds(5)); } } printf("YOLO Worker Thread Exiting.\n");}


// // --- GStreamer Saving Pipeline Setup ---
// // void setupPipeline() { gst_init(nullptr, nullptr); std::string dir = "/userdata/test_cpp/dms_gst"; if (access(dir.c_str(), W_OK) != 0) { std::cerr << "Directory " << dir << " is not writable or does not exist" << std::endl; return; } DIR* directory = opendir(dir.c_str()); if (!directory) { std::cerr << "Failed to open directory " << dir << std::endl; return; } int mkv_count = 0; struct dirent* entry; while ((entry = readdir(directory)) != nullptr) { std::string filename = entry->d_name; if (filename.find(".mkv") != std::string::npos) mkv_count++; } closedir(directory); std::string filepath = dir + "/dms_multi_" + std::to_string(mkv_count + 1) + ".mkv"; std::string pipeline_str = "appsrc name=source ! queue ! videoconvert ! video/x-raw,format=NV12 ! mpph265enc rc-mode=cbr bps=4000000 gop=30 qp-min=10 qp-max=51 ! h265parse ! matroskamux ! filesink location=" + filepath; std::cout << "Saving Pipeline: " << pipeline_str << std::endl; GError* error = nullptr; pipeline_ = gst_parse_launch(pipeline_str.c_str(), &error); if (!pipeline_ || error) { std::cerr << "Failed to create saving pipeline: " << (error ? error->message : "Unknown error") << std::endl; if (error) g_error_free(error); return; } appsrc_ = gst_bin_get_by_name(GST_BIN(pipeline_), "source"); if (!appsrc_) { std::cerr << "Failed to get appsrc" << std::endl; gst_object_unref(pipeline_); pipeline_ = nullptr; return; } GstCaps* caps = gst_caps_new_simple("video/x-raw", "format", G_TYPE_STRING, "BGR", "width", G_TYPE_INT, 1920, "height", G_TYPE_INT, 1080, "framerate", GST_TYPE_FRACTION, 60, 1, nullptr); g_object_set(G_OBJECT(appsrc_), "caps", caps, "format", GST_FORMAT_TIME, nullptr); gst_caps_unref(caps); if (gst_element_set_state(pipeline_, GST_STATE_PLAYING) == GST_STATE_CHANGE_FAILURE) { std::cerr << "Failed to set saving pipeline to playing" << std::endl; gst_object_unref(appsrc_); gst_object_unref(pipeline_); pipeline_ = nullptr; appsrc_ = nullptr; } }

// void setupPipeline() {
//     gst_init(nullptr, nullptr);
//     const std::string dir = "/userdata/test_cpp/dms_gst"; // Or your desired path

//     // Check if directory exists and is writable
//     struct stat st = {0};
//     if (stat(dir.c_str(), &st) == -1) {
//         if (mkdir(dir.c_str(), 0700) == -1) { // Create if it doesn't exist
//              std::cerr << "Error: Cannot create directory " << dir << ": " << strerror(errno) << std::endl;
//              return;
//         }
//         std::cout << "INFO: Created directory " << dir << std::endl;
//     } else if (access(dir.c_str(), W_OK) != 0) {
//         std::cerr << "Error: Directory " << dir << " is not writable." << std::endl;
//         return;
//     }


//     // Open directory and count .mkv files
//     DIR* directory = opendir(dir.c_str());
//     if (!directory) {
//         std::cerr << "Failed to open directory " << dir << "\n";
//         return;
//     }

//     int mkv_count = 0;
//     struct dirent* entry; // Use struct dirent*
//     while ((entry = readdir(directory)) != nullptr) { // Correct loop condition
//         if (std::string_view(entry->d_name).find(".mkv") != std::string_view::npos) {
//             ++mkv_count;
//         }
//     }
//     closedir(directory);

//     // Construct pipeline string
//     const std::string filepath = dir + "/dms_multi_" + std::to_string(mkv_count + 1) + ".mkv";
//     const std::string pipeline_str =
//         "appsrc name=source ! queue ! videoconvert ! video/x-raw,format=NV12 ! " // Assuming NV12 for encoder
//         "mpph265enc rc-mode=cbr bps=4000000 gop=30 qp-min=10 qp-max=51 ! "
//         "h265parse ! matroskamux ! filesink location=" + filepath;

//     std::cout << "Saving Pipeline: " << pipeline_str << "\n";

//     // Create pipeline
//     GError* error = nullptr;
//     pipeline_ = gst_parse_launch(pipeline_str.c_str(), &error);
//     if (!pipeline_ || error) {
//         std::cerr << "Failed to create saving pipeline: " << (error ? error->message : "Unknown error") << "\n";
//         if (error) {
//             g_error_free(error);
//         }
//         return;
//     }

//     // Get appsrc
//     appsrc_ = gst_bin_get_by_name(GST_BIN(pipeline_), "source");
//     if (!appsrc_) {
//         std::cerr << "Failed to get appsrc\n";
//         gst_object_unref(pipeline_);
//         pipeline_ = nullptr;
//         return;
//     }

//     // ***** REMOVE CAPS SETTING FROM HERE *****
//     // GstCaps* caps = gst_caps_new_simple(...);
//     // g_object_set(G_OBJECT(appsrc_), "caps", caps, "format", GST_FORMAT_TIME, nullptr);
//     // gst_caps_unref(caps);
//     // ******************************************

//      // Set appsrc properties needed BEFORE setting caps later
//      g_object_set(G_OBJECT(appsrc_),
//                   "stream-type", GST_APP_STREAM_TYPE_STREAM, // GST_APP_STREAM_TYPE_SEEKABLE if needed
//                   "format", GST_FORMAT_TIME,
//                   "is-live", FALSE, // Set to TRUE if it's a live source
//                   NULL);


//     // Start pipeline playing (it will wait for data and caps)
//     if (gst_element_set_state(pipeline_, GST_STATE_PLAYING) == GST_STATE_CHANGE_FAILURE) {
//         std::cerr << "Failed to set saving pipeline to playing state\n";
//         gst_object_unref(appsrc_);
//         gst_object_unref(pipeline_);
//         appsrc_ = nullptr;
//         pipeline_ = nullptr;
//     } else {
//         std::cout << "INFO: Saving pipeline created and set to PLAYING (waiting for caps/data).\n";
//     }
// }


// void pushFrameToPipeline(unsigned char* data, int size, int width, int height, GstClockTime duration) { if (!appsrc_) return; GstBuffer* buffer = gst_buffer_new_allocate(nullptr, size, nullptr); GstMapInfo map; if (!gst_buffer_map(buffer, &map, GST_MAP_WRITE)) { std::cerr << "Failed map buffer" << std::endl; gst_buffer_unref(buffer); return; } if (map.size != (guint)size) { std::cerr << "Buffer size mismatch: " << map.size << " vs " << size << std::endl; gst_buffer_unmap(buffer, &map); gst_buffer_unref(buffer); return; } memcpy(map.data, data, size); gst_buffer_unmap(buffer, &map); static GstClockTime timestamp = 0; GST_BUFFER_PTS(buffer) = timestamp; GST_BUFFER_DURATION(buffer) = duration; timestamp += GST_BUFFER_DURATION(buffer); GstFlowReturn ret; g_signal_emit_by_name(appsrc_, "push-buffer", buffer, &ret); if (ret != GST_FLOW_OK) std::cerr << "Failed push buffer, ret=" << ret << std::endl; gst_buffer_unref(buffer); }


// // --- Resource Monitoring Functions ---
// // ... (Keep resource monitoring functions as is) ...
// void calculateOverallFPS(double frame_duration_ms, std::deque<double>& times_deque, double& fps_variable, int max_records) { times_deque.push_back(frame_duration_ms); if (times_deque.size() > max_records) times_deque.pop_front(); if (!times_deque.empty()) { double sum = std::accumulate(times_deque.begin(), times_deque.end(), 0.0); double avg_time_ms = sum / times_deque.size(); fps_variable = (avg_time_ms > 0) ? (1000.0 / avg_time_ms) : 0.0; } else { fps_variable = 0.0; } }
// void getCPUUsage(double& cpu_usage_variable, long& prev_idle, long& prev_total) { std::ifstream file("/proc/stat"); if (!file.is_open()) return; std::string line; std::getline(file, line); file.close(); long user, nice, system, idle, iowait, irq, softirq, steal, guest, guest_nice; user = nice = system = idle = iowait = irq = softirq = steal = guest = guest_nice = 0; std::istringstream iss(line); std::string cpu_label; iss >> cpu_label >> user >> nice >> system >> idle >> iowait >> irq >> softirq >> steal >> guest >> guest_nice; long currentIdleTime = idle + iowait; long currentTotalTime = user + nice + system + idle + iowait + irq + softirq + steal; long diffIdle = currentIdleTime - prev_idle; long diffTotal = currentTotalTime - prev_total; if (diffTotal > 0) cpu_usage_variable = 100.0 * (double)(diffTotal - diffIdle) / diffTotal; prev_idle = currentIdleTime; prev_total = currentTotalTime; }
// void getTemperature(double& temp_variable) { const char* temp_paths[] = {"/sys/class/thermal/thermal_zone0/temp", "/sys/class/thermal/thermal_zone1/temp"}; bool temp_read = false; for (const char* path : temp_paths) { std::ifstream file(path); double temp_milliC = 0; if (file >> temp_milliC) { temp_variable = temp_milliC / 1000.0; temp_read = true; file.close(); break; } file.close(); } }


// /*-------------------------------------------
//                   Main Function
// -------------------------------------------*/
// int main(int argc, char **argv) {

//     // ... (Variable declarations: ret, frame_id, monitoring vars, model paths, video source) ...
//     int ret = 0; long current_frame_id = 0; std::deque<double> frame_times; const int max_time_records = 60; double overallFPS = 0.0; double currentCpuUsage = 0.0; long prevIdleTime = 0, prevTotalTime = 0; double currentTemp = 0.0;

//     // const char *detection_model_path = "../../model/faceD.rknn";
//     const char *detection_model_path = "../../model/rf.rknn";
//     const char *landmark_model_path  = "../../model/faceL.rknn";
//     const char *iris_model_path = "../../model/faceI.rknn";
//     const char *yolo_model_path = "../../model/od.rknn";

//     const char *video_source = "filesrc location=../../model/gunsan6.mkv ! decodebin ! queue ! videoconvert ! video/x-raw,format=BGR ! appsink name=sink sync=false";

//     // --- Initialization ---
//     setupPipeline(); // Setup saving pipeline (without caps)
//     // ... (Init contexts, buffers, behavior modules) ...
//     app_context_t app_ctx; memset(&app_ctx, 0, sizeof(app_context_t)); image_buffer_t src_image; memset(&src_image, 0, sizeof(image_buffer_t)); face_analyzer_result_t face_results; memset(&face_results, 0, sizeof(face_results)); object_detect_result_list yolo_results; memset(&yolo_results, 0, sizeof(yolo_results)); my::BlinkDetector blinkDetector; YawnDetector yawnDetector; my::HeadPoseTracker headPoseTracker; KSSCalculator kssCalculator;

//     // --- Calibration State Variables ---
//     // ... (Calibration vars) ...
//      bool calibration_done = false; std::chrono::steady_clock::time_point calibration_start_time; bool calibration_timer_started = false; int consecutive_valid_eyes_frames = 0; const int REQUIRED_VALID_EYES_FRAMES = 60; bool ear_calibrated = false; bool mouth_calibrated = false; std::deque<float> calib_left_ears; std::deque<float> calib_right_ears; std::deque<double> calib_mouth_dists; int consecutive_stable_ear_frames = 0; int consecutive_stable_mouth_frames = 0; const int CALIB_WINDOW_SIZE = 30; const float EAR_STDDEV_THRESHOLD = 0.04; const double MOUTH_DIST_STDDEV_THRESHOLD = 15.0; const int REQUIRED_STABLE_FRAMES = CALIB_WINDOW_SIZE + 5; const double CALIBRATION_TIMEOUT_SECONDS = 10.0;


//     // --- Driver ID & Tracking State ---
//     // ... (Driver ID vars) ...
//      bool driver_identified_ever = false; bool driver_tracked_this_frame = false; int current_tracked_driver_idx = -1; cv::Point prev_driver_centroid = cv::Point(-1, -1); const double MAX_CENTROID_DISTANCE = 150.0; int driver_search_timeout_frames = 0; const int DRIVER_SEARCH_MAX_FRAMES = 60;

//     // --- Fixed Driver Object ROI ---
//     // ... (Fixed ROI vars) ...
//      cv::Rect driver_object_roi; bool driver_roi_defined = false; bool valid_object_roi = false;

//     // --- State variables for KSS and display ---
//     // ... (KSS/Display vars) ...
//      std::string kssStatus = "Initializing"; YawnDetector::YawnMetrics yawnMetrics = {}; my::HeadPoseTracker::HeadPoseResults headPoseResults = {}; std::vector<std::string> detectedObjects; int extractedTotalKSS = 1; int perclosKSS = 1, blinkKSS = 1, headposeKSS = 1, yawnKSS = 1, objdectdetectionKSS = 1; std::stringstream text_stream; int text_y = 0; const int line_height = 22; const int text_size = 12; const int status_text_size = 16; unsigned int status_color_uint = COLOR_GREEN;


//     // --- YOLO thread handle ---
//     std::thread yolo_worker_thread;

//     // --- Init models ---
//     // ... (Model init) ...
//      ret = init_post_process(); if (ret != 0) { /*...*/ return -1; } ret = init_face_analyzer(detection_model_path, landmark_model_path, iris_model_path, &app_ctx.face_ctx); if (ret != 0) { /*...*/ return -1; } ret = init_yolo11(yolo_model_path, &app_ctx.yolo_ctx); if (ret != 0) { /*...*/ return -1; } yolo_worker_thread = std::thread(yolo_worker_thread_func, &app_ctx.yolo_ctx);

//     // --- Init GStreamer input ---
//     // ... (GStreamer input init) ...
//      gst_init(nullptr, nullptr); GError* error = nullptr; GstElement* input_pipeline = gst_parse_launch(video_source, &error); if (!input_pipeline || error) { /*...*/ return -1; } appsink_ = gst_bin_get_by_name(GST_BIN(input_pipeline), "sink"); if (!appsink_) { /*...*/ return -1; } if (gst_element_set_state(input_pipeline, GST_STATE_PLAYING) == GST_STATE_CHANGE_FAILURE) { /*...*/ return -1; }

//     // --- Setup OpenCV window ---
//     cv::namedWindow("DMS Output", cv::WINDOW_NORMAL); cv::resizeWindow("DMS Output", 1920, 1080);

//     GstSample* sample; GstVideoInfo video_info; GstBuffer* gst_buffer; GstMapInfo map_info;
//     // bool first_frame = true; // Can remove this if only used for ROI def
//     bool saving_pipeline_caps_set = false; // <<< Flag to track if saving caps are set

//     // --- Buffer for the FIXED ROI Crop ---
//     image_buffer_t fixed_roi_crop_img;
//     memset(&fixed_roi_crop_img, 0, sizeof(image_buffer_t));

//     // --- Main Processing Loop ---
//     while (true) {
//         // --- Get Frame ---
//          sample = gst_app_sink_pull_sample(GST_APP_SINK(appsink_)); if (!sample) { std::cerr << "End of stream or error pulling sample." << std::endl; break; } gst_buffer = gst_sample_get_buffer(sample); GstCaps* caps = gst_sample_get_caps(sample); if (!gst_buffer || !caps || !gst_video_info_from_caps(&video_info, caps) || !gst_buffer_map(gst_buffer, &map_info, GST_MAP_READ)) { std::cerr << "Failed to get valid buffer/caps/map" << std::endl; if (gst_buffer && map_info.memory) gst_buffer_unmap(gst_buffer, &map_info); if (sample) gst_sample_unref(sample); continue; }

//         auto frame_start_time = std::chrono::high_resolution_clock::now();
//         current_frame_id++;

//         // --- Wrap GStreamer Data for the *full source image* ---
//         src_image.width = GST_VIDEO_INFO_WIDTH(&video_info);
//         src_image.height = GST_VIDEO_INFO_HEIGHT(&video_info);
//         src_image.width_stride = GST_VIDEO_INFO_PLANE_STRIDE(&video_info, 0);
//         src_image.height_stride = src_image.height;
//         // Determine format based on GStreamer info if possible, otherwise default
//         GstVideoFormat gst_format = GST_VIDEO_INFO_FORMAT(&video_info);


//         src_image.format = IMAGE_FORMAT_RGB888;
//         src_image.virt_addr = map_info.data;
//         src_image.size = map_info.size;
//         src_image.fd = -1;


//         // --- Set Saving Pipeline Caps (on first valid frame) ---
//         if (pipeline_ && appsrc_ && !saving_pipeline_caps_set) {
//              GstCaps* save_caps = gst_caps_new_simple("video/x-raw",
//                                              "format", G_TYPE_STRING, "BGR", // Input to saving pipeline is BGR
//                                              "width", G_TYPE_INT, src_image.width,
//                                              "height", G_TYPE_INT, src_image.height,
//                                              "framerate", GST_TYPE_FRACTION, GST_VIDEO_INFO_FPS_N(&video_info), GST_VIDEO_INFO_FPS_D(&video_info),
//                                              NULL);
//              if (save_caps) {
//                   printf("INFO: Setting saving pipeline caps to: %s\n", gst_caps_to_string(save_caps));
//                   g_object_set(G_OBJECT(appsrc_), "caps", save_caps, NULL);
//                   gst_caps_unref(save_caps);
//                   saving_pipeline_caps_set = true;
//              } else {
//                   std::cerr << "ERROR: Failed to create caps for saving pipeline!\n";
//                   // Consider stopping or disabling saving
//              }
//         }
//         // --- End Set Saving Caps ---


//         // --- Run Face Analysis on the FULL FRAME ---
//         ret = inference_face_analyzer(&app_ctx.face_ctx, &src_image, &face_results);
//         if (ret != 0) { printf("WARN: Face Analyzer Inference failed frame %ld, ret=%d\n", current_frame_id, ret); face_results.count = 0; }

//         // --- Reset frame-specific flags ---
//         driver_tracked_this_frame = false;
//         current_tracked_driver_idx = -1;
//         valid_object_roi = false; // Reset each frame

//         // --- Driver Identification and Tracking (Based on Full Frame Results) ---
//         if (!driver_identified_ever) { /* Identification */ int best_candidate_idx = -1; float max_area = 0.0f; cv::Rect temp_id_zone((int)(src_image.width * 0.40),(int)(src_image.height * 0.1),(int)(src_image.width * 0.55),(int)(src_image.height * 0.8)); temp_id_zone &= cv::Rect(0, 0, src_image.width, src_image.height); for (int i = 0; i < face_results.count; ++i) { cv::Point face_center((face_results.faces[i].box.left + face_results.faces[i].box.right) / 2, (face_results.faces[i].box.top + face_results.faces[i].box.bottom) / 2); if (temp_id_zone.contains(face_center) && face_results.faces[i].face_landmarks_valid) { float area = (float)(face_results.faces[i].box.right - face_results.faces[i].box.left) * (face_results.faces[i].box.bottom - face_results.faces[i].box.top); if (area > max_area) { max_area = area; best_candidate_idx = i; } } } if (best_candidate_idx != -1) { driver_identified_ever = true; driver_tracked_this_frame = true; current_tracked_driver_idx = best_candidate_idx; prev_driver_centroid = calculate_centroid(face_results.faces[best_candidate_idx].face_landmarks, NUM_FACE_LANDMARKS); driver_search_timeout_frames = 0; printf("INFO: Driver Identified (Index %d) at frame %ld\n", current_tracked_driver_idx, current_frame_id); kssStatus = "Calibrating..."; } else { kssStatus = "Searching Driver..."; } } else { /* Tracking */ int best_match_idx = -1; double min_dist = MAX_CENTROID_DISTANCE; for (int i = 0; i < face_results.count; ++i) { if (face_results.faces[i].face_landmarks_valid) { cv::Point current_centroid = calculate_centroid(face_results.faces[i].face_landmarks, NUM_FACE_LANDMARKS); if (prev_driver_centroid.x >= 0 && current_centroid.x >= 0) { double dist = cv::norm(current_centroid - prev_driver_centroid); if (dist < min_dist) { min_dist = dist; best_match_idx = i; } } } } if (best_match_idx != -1) { driver_tracked_this_frame = true; current_tracked_driver_idx = best_match_idx; prev_driver_centroid = calculate_centroid(face_results.faces[best_match_idx].face_landmarks, NUM_FACE_LANDMARKS); driver_search_timeout_frames = 0; if (!calibration_done) kssStatus = "Calibrating..."; } else { driver_tracked_this_frame = false; current_tracked_driver_idx = -1; prev_driver_centroid = cv::Point(-1, -1); driver_search_timeout_frames++; kssStatus = "Driver Lost..."; if (driver_search_timeout_frames > DRIVER_SEARCH_MAX_FRAMES) { driver_identified_ever = false; driver_search_timeout_frames = 0; printf("INFO: Driver track lost for %d frames. Reverting to search.\n", DRIVER_SEARCH_MAX_FRAMES); kssStatus = "Searching Driver..."; } } }


//         // --- Calibration or Normal Processing ---
//         if (!calibration_done) {
//             // --- Calibration Logic ---
//              bool head_pose_calibrated = false; bool eyes_consistently_valid = false; double elapsed_calib_seconds = 0.0; std::string calib_status_detail = ""; if (driver_tracked_this_frame && current_tracked_driver_idx != -1) { face_object_t *calib_face = &face_results.faces[current_tracked_driver_idx]; if (calib_face->face_landmarks_valid) { if (!calibration_timer_started) { /* Start timer, reset state */ calibration_start_time = std::chrono::steady_clock::now(); calibration_timer_started = true; printf("INFO: Calibration timer started (Driver Index %d).\n", current_tracked_driver_idx); consecutive_valid_eyes_frames = 0; consecutive_stable_ear_frames = 0; consecutive_stable_mouth_frames = 0; calib_left_ears.clear(); calib_right_ears.clear(); calib_mouth_dists.clear(); ear_calibrated = false; mouth_calibrated = false; } /* ... Run checks ... */ std::vector<cv::Point> calib_faceLandmarks = convert_landmarks_to_cvpoint(calib_face->face_landmarks, NUM_FACE_LANDMARKS); headPoseTracker.run(calib_faceLandmarks); head_pose_calibrated = headPoseTracker.isCalibrated(); if (!head_pose_calibrated) calib_status_detail += " (Head)"; bool eyes_valid_this_frame = calib_face->eye_landmarks_left_valid && calib_face->eye_landmarks_right_valid; if (eyes_valid_this_frame) consecutive_valid_eyes_frames++; else consecutive_valid_eyes_frames = 0; eyes_consistently_valid = (consecutive_valid_eyes_frames >= REQUIRED_VALID_EYES_FRAMES); if (!eyes_consistently_valid) calib_status_detail += " (Eyes Valid)"; if (!ear_calibrated && eyes_valid_this_frame) { const std::vector<int> L={33,160,158,133,153,144}, R={362,385,387,263,380,373}; float lE=calculate_ear_simple(calib_faceLandmarks,L), rE=calculate_ear_simple(calib_faceLandmarks,R); calib_left_ears.push_back(lE); calib_right_ears.push_back(rE); if (calib_left_ears.size()>CALIB_WINDOW_SIZE) calib_left_ears.pop_front(); if (calib_right_ears.size()>CALIB_WINDOW_SIZE) calib_right_ears.pop_front(); if (calib_left_ears.size()>=CALIB_WINDOW_SIZE) { float lS=calculate_stddev(calib_left_ears), rS=calculate_stddev(calib_right_ears); if (lS<EAR_STDDEV_THRESHOLD && rS<EAR_STDDEV_THRESHOLD) consecutive_stable_ear_frames++; else consecutive_stable_ear_frames=0; } else consecutive_stable_ear_frames=0; ear_calibrated=(consecutive_stable_ear_frames>=REQUIRED_STABLE_FRAMES); } else if(!eyes_valid_this_frame){ consecutive_stable_ear_frames=0; ear_calibrated=false; calib_left_ears.clear(); calib_right_ears.clear(); } if(!ear_calibrated) calib_status_detail+=" (EAR Stable)"; if (!mouth_calibrated) { double cM=calculate_mouth_dist_simple(calib_faceLandmarks); calib_mouth_dists.push_back(cM); if(calib_mouth_dists.size()>CALIB_WINDOW_SIZE) calib_mouth_dists.pop_front(); if(calib_mouth_dists.size()>=CALIB_WINDOW_SIZE){ double mS=calculate_stddev(calib_mouth_dists); if(mS<MOUTH_DIST_STDDEV_THRESHOLD) consecutive_stable_mouth_frames++; else consecutive_stable_mouth_frames=0; } else consecutive_stable_mouth_frames=0; mouth_calibrated=(consecutive_stable_mouth_frames>=REQUIRED_STABLE_FRAMES); } if(!mouth_calibrated) calib_status_detail+=" (Mouth Stable)"; auto now = std::chrono::steady_clock::now(); elapsed_calib_seconds = std::chrono::duration<double>(now - calibration_start_time).count(); if (elapsed_calib_seconds >= CALIBRATION_TIMEOUT_SECONDS) { if (head_pose_calibrated && eyes_consistently_valid && ear_calibrated && mouth_calibrated) { calibration_done = true; printf("INFO: Calibration Complete!\n"); /* Define Fixed ROI */ face_object_t *calib_driver_face = &face_results.faces[current_tracked_driver_idx]; box_rect_t calib_driver_box = calib_driver_face->box; int calib_box_w = calib_driver_box.right - calib_driver_box.left; int calib_box_h = calib_driver_box.bottom - calib_driver_box.top; if(calib_box_w > 0 && calib_box_h > 0) { int box_center_x = (calib_driver_box.left + calib_driver_box.right) / 2; int box_center_y = (calib_driver_box.top + calib_driver_box.bottom) / 2; const float width_expansion = 2.2f; const float height_expansion_up = 0.8f; const float height_expansion_down = 3.0f; int roi_width = static_cast<int>(calib_box_w * width_expansion); int roi_height = static_cast<int>(calib_box_h * (height_expansion_up + height_expansion_down)); int roi_size = std::max(roi_width, roi_height); roi_width = roi_size; roi_height = roi_size; if (roi_width < 640) roi_width = 640; int roi_x = box_center_x - roi_width / 2; int roi_y = box_center_y - static_cast<int>(calib_box_h * height_expansion_up); driver_object_roi.x = std::max(0, roi_x); driver_object_roi.y = std::max(0, roi_y); driver_object_roi.width = std::min(roi_width, src_image.width - driver_object_roi.x); driver_object_roi.height = std::min(roi_height, src_image.height - driver_object_roi.y); driver_roi_defined = (driver_object_roi.width > 0 && driver_object_roi.height > 0); printf("INFO: Fixed Driver Object ROI defined: [%d, %d, %d x %d]\n", driver_object_roi.x, driver_object_roi.y, driver_object_roi.width, driver_object_roi.height); } else { driver_roi_defined = false; printf("WARN: Could not define fixed ROI due to invalid calibration box.\n"); } } else { /* Timeout Failed */ calibration_timer_started = false; driver_identified_ever = false; driver_tracked_this_frame = false; current_tracked_driver_idx = -1; prev_driver_centroid = cv::Point(-1, -1); consecutive_valid_eyes_frames = 0; consecutive_stable_ear_frames = 0; consecutive_stable_mouth_frames = 0; ear_calibrated = false; mouth_calibrated = false; calib_left_ears.clear(); calib_right_ears.clear(); calib_mouth_dists.clear(); printf("WARN: Calibration time expired, criteria not met (H:%d, EV:%d, ES:%d, MS:%d). Retrying identification.\n", head_pose_calibrated, eyes_consistently_valid, ear_calibrated, mouth_calibrated); } } } else { calibration_timer_started = false; printf("WARN: Tracked driver landmarks not valid during calibration.\n"); } kssStatus = "Calibrating... " + std::to_string(static_cast<int>(elapsed_calib_seconds)) + "s" + calib_status_detail; if(driver_tracked_this_frame && current_tracked_driver_idx != -1) draw_rectangle(&src_image, face_results.faces[current_tracked_driver_idx].box.left, face_results.faces[current_tracked_driver_idx].box.top, face_results.faces[current_tracked_driver_idx].box.right - face_results.faces[current_tracked_driver_idx].box.left, face_results.faces[current_tracked_driver_idx].box.bottom - face_results.faces[current_tracked_driver_idx].box.top, COLOR_YELLOW, 2); } else { /* No driver tracked */ calibration_timer_started = false; consecutive_valid_eyes_frames = 0; consecutive_stable_ear_frames = 0; consecutive_stable_mouth_frames = 0; ear_calibrated = false; mouth_calibrated = false; calib_left_ears.clear(); calib_right_ears.clear(); calib_mouth_dists.clear(); if (driver_identified_ever) { kssStatus = "Calibration: Waiting for Driver Track..."; } else { kssStatus = "Calibration: Searching Driver..."; } for (int i = 0; i < face_results.count; ++i) { draw_rectangle(&src_image, face_results.faces[i].box.left, face_results.faces[i].box.top, face_results.faces[i].box.right - face_results.faces[i].box.left, face_results.faces[i].box.bottom - face_results.faces[i].box.top, COLOR_YELLOW, 1); } }
//             // Draw Calibration Status Text
//              draw_text(&src_image, kssStatus.c_str(), 10, 10, COLOR_YELLOW, status_text_size);

//         } else { // --- Normal Processing Phase (Calibration is Done) ---

//             // Default Status for this phase
//             kssStatus = "Normal"; // Assume normal unless KSS says otherwise or driver lost

//             if (!driver_roi_defined) {
//                 kssStatus = "ROI Error";
//                 // ... (Reset KSS values) ...
//                 printf("ERROR: Driver Object ROI was not defined. Skipping processing.\n"); extractedTotalKSS = 1; perclosKSS = 1; blinkKSS = 1; headposeKSS = 1; yawnKSS = 1; objdectdetectionKSS = 1; yawnMetrics = {}; headPoseResults = {}; detectedObjects.clear();
//             } else {
//                 // --- Crop to the FIXED Driver Object ROI ---
//                 // ... (Crop logic) ...
//                  if (fixed_roi_crop_img.virt_addr) { free(fixed_roi_crop_img.virt_addr); fixed_roi_crop_img.virt_addr = nullptr; } memset(&fixed_roi_crop_img, 0, sizeof(image_buffer_t)); box_rect_t fixed_roi_as_box = { driver_object_roi.x, driver_object_roi.y, driver_object_roi.x + driver_object_roi.width, driver_object_roi.y + driver_object_roi.height}; ret = crop_image_simple(&src_image, &fixed_roi_crop_img, fixed_roi_as_box);

//                 if (ret == 0 && fixed_roi_crop_img.virt_addr) {
//                     // --- Queue FIXED ROI CROP for YOLO ---
//                     // ... (YOLO queuing logic) ...
//                     auto yolo_input_image = std::make_shared<image_buffer_t>(); yolo_input_image->width = fixed_roi_crop_img.width; yolo_input_image->height = fixed_roi_crop_img.height; yolo_input_image->format = fixed_roi_crop_img.format; yolo_input_image->size = fixed_roi_crop_img.size; yolo_input_image->virt_addr = (unsigned char*)malloc(yolo_input_image->size); if (yolo_input_image->virt_addr) { memcpy(yolo_input_image->virt_addr, fixed_roi_crop_img.virt_addr, yolo_input_image->size); yolo_input_image->width_stride = 0; yolo_input_image->height_stride = 0; yolo_input_image->fd = -1; { std::unique_lock<std::mutex> lock(yolo_input_mutex); if (yolo_input_queue.size() < MAX_QUEUE_SIZE) yolo_input_queue.push({current_frame_id, yolo_input_image}); else { free(yolo_input_image->virt_addr); yolo_input_image->virt_addr = nullptr; } } } else { printf("ERROR: Failed alloc YOLO input copy (fixed roi crop).\n"); }
//                 } else { printf("ERROR: Failed to crop to FIXED ROI for YOLO frame %ld.\n", current_frame_id); }

//                 // --- Get YOLO Results ---
//                  bool yolo_result_received = false; { std::unique_lock<std::mutex> lock(yolo_output_mutex); if (!yolo_output_queue.empty()) { YoloOutputData data = yolo_output_queue.front(); yolo_output_queue.pop(); yolo_results = data.results; yolo_result_received = true; } }

//                 // --- Run Behavior Analysis & KSS (Conditional on Driver Tracked & In ROI) ---
//                 detectedObjects.clear();
//                 std::vector<std::string> driver_detectedObjects_final;
//                 valid_object_roi = driver_roi_defined;

//                 if (driver_tracked_this_frame && current_tracked_driver_idx != -1) {
//                     face_object_t *driver_face = &face_results.faces[current_tracked_driver_idx];
//                     cv::Point driver_center_full_frame( (driver_face->box.left + driver_face->box.right) / 2, (driver_face->box.top + driver_face->box.bottom) / 2 );

//                     if (driver_object_roi.contains(driver_center_full_frame)) { // Driver IN ROI
//                         // Filter YOLO results
//                         if (yolo_results.count > 0) { for (int j = 0; j < yolo_results.count; ++j) { object_detect_result* det = &yolo_results.results[j]; if (det->prop > 0.4) { driver_detectedObjects_final.push_back(coco_cls_to_name(det->cls_id)); } } }
//                         detectedObjects = driver_detectedObjects_final;

//                         // Run Behavior Modules
//                         if (driver_face->face_landmarks_valid) {
//                             std::vector<cv::Point> faceLandmarksCv = convert_landmarks_to_cvpoint(driver_face->face_landmarks, NUM_FACE_LANDMARKS);
//                             if (!faceLandmarksCv.empty()) {
//                                  headPoseResults = headPoseTracker.run(faceLandmarksCv); blinkDetector.run(faceLandmarksCv, src_image.width, src_image.height); yawnMetrics = yawnDetector.run(faceLandmarksCv, src_image.width, src_image.height); kssCalculator.setPerclos(blinkDetector.getPerclos()); int headPoseKSSValue = 1; if (headPoseResults.rows.size() >= 4) { for (const auto& row : headPoseResults.rows) { if (row.size() >= 2 && row[0] == "Head KSS") { try { headPoseKSSValue = std::stoi(row[1]); } catch (...) { headPoseKSSValue = 1; } break; } } } kssCalculator.setHeadPose(headPoseKSSValue); kssCalculator.setYawnMetrics(yawnMetrics.isYawning, yawnMetrics.yawnFrequency_5min, yawnMetrics.yawnDuration); kssCalculator.setBlinksLastMinute(blinkDetector.getBlinksInWindow());
//                             } else { kssStatus = "Landmark Error"; /* Reset Inputs */ headPoseResults = {}; yawnMetrics = {}; kssCalculator.setPerclos(0); kssCalculator.setHeadPose(1); kssCalculator.setYawnMetrics(false, 0, 0); kssCalculator.setBlinksLastMinute(0); }
//                         } else { kssStatus = "Driver Data Invalid"; /* Reset Inputs */ headPoseResults = {}; yawnMetrics = {}; kssCalculator.setPerclos(0); kssCalculator.setHeadPose(1); kssCalculator.setYawnMetrics(false, 0, 0); kssCalculator.setBlinksLastMinute(0); }

//                         // Calculate KSS
//                          double now_seconds = std::chrono::duration<double>(std::chrono::system_clock::now().time_since_epoch()).count(); kssCalculator.setDetectedObjects(driver_detectedObjects_final, now_seconds); auto kssBreakdownResults = kssCalculator.calculateCompositeKSS(); if (kssBreakdownResults.size() > 5 && kssBreakdownResults[5].size() == 2) { try { perclosKSS = std::stoi(kssBreakdownResults[0][1]); blinkKSS = std::stoi(kssBreakdownResults[1][1]); headposeKSS = std::stoi(kssBreakdownResults[2][1]); yawnKSS = std::stoi(kssBreakdownResults[3][1]); objdectdetectionKSS = std::stoi(kssBreakdownResults[4][1]); extractedTotalKSS = std::stoi(kssBreakdownResults[5][1]); } catch (...) { extractedTotalKSS = 1; } } else { extractedTotalKSS = 1; }
//                         std::string alertStatus = kssCalculator.getKSSAlertStatus(extractedTotalKSS);
//                         // Update status only if not already an error, or if there's an alert
//                         if (kssStatus == "Normal" || !alertStatus.empty()) { kssStatus = alertStatus.empty() ? "Normal" : alertStatus; }

//                     } else { // Driver tracked BUT outside fixed ROI
//                          kssStatus = "Driver Outside ROI";
//                          driver_tracked_this_frame = false; current_tracked_driver_idx = -1; extractedTotalKSS = 1; yawnMetrics = {}; headPoseResults = {}; kssCalculator.setPerclos(0.0); kssCalculator.setHeadPose(1); kssCalculator.setYawnMetrics(false, 0, 0); kssCalculator.setBlinksLastMinute(0); double now_seconds = std::chrono::duration<double>(std::chrono::system_clock::now().time_since_epoch()).count(); kssCalculator.setDetectedObjects({}, now_seconds); auto kssBreakdownResults = kssCalculator.calculateCompositeKSS(); if (kssBreakdownResults.size() > 5 && kssBreakdownResults[5].size() == 2) { extractedTotalKSS = std::stoi(kssBreakdownResults[5][1]); } else { extractedTotalKSS = 1;}
//                     }
//                 } else { // Driver NOT tracked in this frame
//                     kssStatus = "Driver Not Tracked";
//                     extractedTotalKSS = 1; yawnMetrics = {}; headPoseResults = {}; kssCalculator.setPerclos(0.0); kssCalculator.setHeadPose(1); kssCalculator.setYawnMetrics(false, 0, 0); kssCalculator.setBlinksLastMinute(0); double now_seconds = std::chrono::duration<double>(std::chrono::system_clock::now().time_since_epoch()).count(); kssCalculator.setDetectedObjects({}, now_seconds); auto kssBreakdownResults = kssCalculator.calculateCompositeKSS(); if (kssBreakdownResults.size() > 5 && kssBreakdownResults[5].size() == 2) { extractedTotalKSS = std::stoi(kssBreakdownResults[5][1]); } else { extractedTotalKSS = 1;}
//                 }
//             } // End if(driver_roi_defined) / else


//             // --- Drawing (Normal Processing Phase - Full Frame Coords) ---
//             // Draw Green box ONLY if driver is tracked AND INSIDE ROI
//             if (driver_tracked_this_frame && current_tracked_driver_idx != -1) {
//                 face_object_t *driver_face = &face_results.faces[current_tracked_driver_idx];
//                 cv::Point driver_center_full_frame( (driver_face->box.left + driver_face->box.right) / 2, (driver_face->box.top + driver_face->box.bottom) / 2 );
//                 if (driver_roi_defined && driver_object_roi.contains(driver_center_full_frame)) {
//                      draw_rectangle(&src_image, driver_face->box.left, driver_face->box.top, driver_face->box.right - driver_face->box.left, driver_face->box.bottom - driver_face->box.top, COLOR_GREEN, 2);
//                      // Draw Landmarks and Eye status ONLY if driver is valid and inside ROI
//                      if (driver_face->face_landmarks_valid) {
//                           // ---> COMMENT OUT this block to disable landmarks <---
                          
//                           for (int j = 0; j < NUM_FACE_LANDMARKS; ++j) { draw_circle(&src_image, driver_face->face_landmarks[j].x, driver_face->face_landmarks[j].y, 1, COLOR_CYAN, 1); }
//                           if (driver_face->eye_landmarks_left_valid) { for (int j=0; j<NUM_EYE_CONTOUR_LANDMARKS; ++j) draw_circle(&src_image, driver_face->eye_landmarks_left[j].x, driver_face->eye_landmarks_left[j].y, 1, COLOR_BLUE, 1); }
//                           if (driver_face->iris_landmarks_left_valid) { for (int j=0; j<NUM_IRIS_LANDMARKS; ++j) draw_circle(&src_image, driver_face->iris_landmarks_left[j].x, driver_face->iris_landmarks_left[j].y, 1, COLOR_ORANGE, 1); }
//                           if (driver_face->eye_landmarks_right_valid) { for (int j=0; j<NUM_EYE_CONTOUR_LANDMARKS; ++j) draw_circle(&src_image, driver_face->eye_landmarks_right[j].x, driver_face->eye_landmarks_right[j].y, 1, COLOR_BLUE, 1); }
//                           if (driver_face->iris_landmarks_right_valid) { for (int j=0; j<NUM_IRIS_LANDMARKS; ++j) draw_circle(&src_image, driver_face->iris_landmarks_right[j].x, driver_face->iris_landmarks_right[j].y, 1, COLOR_ORANGE, 1); }
                          

//                           const int LEFT_EYE_TEXT_ANCHOR_IDX = 33; const int RIGHT_EYE_TEXT_ANCHOR_IDX = 263;
            
//                      }
//                  } else { // Driver tracked but outside ROI - draw yellow box
//                       draw_rectangle(&src_image, driver_face->box.left, driver_face->box.top, driver_face->box.right - driver_face->box.left, driver_face->box.bottom - driver_face->box.top, COLOR_YELLOW, 1);
//                  }
//             }

//             // Draw Status Text (Common)
//              if (kssStatus == "Normal") status_color_uint = COLOR_GREEN; else if (extractedTotalKSS <= 7) status_color_uint = COLOR_BLUE; else status_color_uint = COLOR_RED; text_y = 10; if (!kssStatus.empty()) { draw_text(&src_image, kssStatus.c_str(), 10, text_y, status_color_uint, status_text_size); text_y += (int)(line_height * 1.4); } text_stream.str(""); text_stream << "PERCLOS: " << std::fixed << std::setprecision(2) << blinkDetector.getPerclos() << "%"; draw_text(&src_image, text_stream.str().c_str(), 10, text_y, COLOR_WHITE, text_size); text_y += line_height; text_stream.str(""); text_stream << "Blinks (Last Min): " << blinkDetector.getBlinksInWindow(); draw_text(&src_image, text_stream.str().c_str(), 10, text_y, COLOR_WHITE, text_size); text_y += line_height; if (headPoseResults.rows.size() >= 3) { std::string headpose_text = "Yaw:" + headPoseResults.rows[0][1] + " Pitch:" + headPoseResults.rows[1][1] + " Roll:" + headPoseResults.rows[2][1]; draw_text(&src_image, headpose_text.c_str(), 10, text_y, COLOR_WHITE, text_size); text_y += line_height; } else { draw_text(&src_image, "Head Pose: N/A", 10, text_y, COLOR_WHITE, text_size); text_y += line_height; } text_stream.str(""); text_stream << "Yawning: " << (yawnMetrics.isYawning ? "Yes" : "No"); draw_text(&src_image, text_stream.str().c_str(), 10, text_y, COLOR_WHITE, text_size); text_y += line_height; text_stream.str(""); text_stream << "Yawn Freq(5m): " << static_cast<int>(yawnMetrics.yawnFrequency_5min); draw_text(&src_image, text_stream.str().c_str(), 10, text_y, COLOR_WHITE, text_size); text_y += line_height; std::string detected_objects_text = ""; if (!detectedObjects.empty()) { detected_objects_text = "Detected: "; for (size_t j = 0; j < detectedObjects.size(); ++j) { detected_objects_text += detectedObjects[j]; if (j < detectedObjects.size() - 1) detected_objects_text += ", "; } draw_text(&src_image, detected_objects_text.c_str(), 10, text_y, COLOR_ORANGE, text_size); text_y += line_height; } text_stream.str(""); text_stream << "KSS Breakdown: P" << perclosKSS << " H" << headposeKSS << " Y" << yawnKSS << " O" << objdectdetectionKSS; draw_text(&src_image, text_stream.str().c_str(), 10, text_y, COLOR_GREEN, text_size); text_y += line_height; text_stream.str(""); text_stream << "Composite KSS: " << extractedTotalKSS; draw_text(&src_image, text_stream.str().c_str(), 10, text_y, status_color_uint, text_size); text_y += line_height;

//         } // End if (!calibration_done) / else

//         // --- Draw ROIs (Optional Debugging - FIXED ROI) ---
//         #ifdef DEBUG_DRAW_ROIS
//         if (driver_roi_defined) { // Draw the FIXED ROI if it has been defined
//              draw_rectangle(&src_image, driver_object_roi.x, driver_object_roi.y, driver_object_roi.width, driver_object_roi.height, COLOR_MAGENTA, 1); draw_text(&src_image, "Fixed Driver ROI", driver_object_roi.x + 5, driver_object_roi.y + 15, COLOR_MAGENTA, 10);
//         }
//         // Draw non-tracked faces (yellow box) - ALWAYS based on full frame results
//          for (int i = 0; i < face_results.count; ++i) { if (i != current_tracked_driver_idx) { draw_rectangle(&src_image, face_results.faces[i].box.left, face_results.faces[i].box.top, face_results.faces[i].box.right - face_results.faces[i].box.left, face_results.faces[i].box.bottom - face_results.faces[i].box.top, COLOR_YELLOW, 1); } }
//         #endif

//         // --- Draw Resource Monitoring Info ---
//          auto frame_processing_end_time = std::chrono::high_resolution_clock::now(); double frame_duration_ms = std::chrono::duration<double, std::milli>(frame_processing_end_time - frame_start_time).count(); calculateOverallFPS(frame_duration_ms, frame_times, overallFPS, max_time_records); getCPUUsage(currentCpuUsage, prevIdleTime, prevTotalTime); getTemperature(currentTemp); std::stringstream info_ss; info_ss << "FPS:" << std::fixed << std::setprecision(1) << overallFPS << "|CPU:" << currentCpuUsage << "%|T:" << currentTemp << "C"; draw_text(&src_image, info_ss.str().c_str(), 10, src_image.height - 20, COLOR_WHITE, 10);

//         // --- Display Frame ---
//         cv::Mat display_frame(src_image.height, src_image.width, CV_8UC3, map_info.data, src_image.width_stride);
//         cv::imshow("DMS Output", display_frame);

//         // --- Push Frame to Saving Pipeline ---
//          if (pipeline_ && appsrc_) { GstClockTime duration = gst_util_uint64_scale_int(1, GST_SECOND, video_info.fps_n > 0 ? video_info.fps_d * video_info.fps_n : 30); pushFrameToPipeline(map_info.data, map_info.size, src_image.width, src_image.height, duration); }

//         // --- Cleanup Frame Resources ---
//         if (fixed_roi_crop_img.virt_addr) { free(fixed_roi_crop_img.virt_addr); fixed_roi_crop_img.virt_addr = nullptr; }
//         gst_buffer_unmap(gst_buffer, &map_info);
//         gst_sample_unref(sample);

//         // --- Handle KeyPress ---
//         if (cv::waitKey(1) == 27) break; // Exit on ESC

//     } // End main loop

//     // --- Final Cleanup ---
//      if (fixed_roi_crop_img.virt_addr) { free(fixed_roi_crop_img.virt_addr); } printf("INFO: Cleaning up resources...\n"); cv::destroyAllWindows(); stop_yolo_worker.store(true); if (yolo_worker_thread.joinable()) yolo_worker_thread.join(); printf("INFO: YOLO thread joined.\n"); if (input_pipeline) { gst_element_set_state(input_pipeline, GST_STATE_NULL); gst_object_unref(appsink_); gst_object_unref(input_pipeline); printf("INFO: Input pipeline released.\n"); } if (pipeline_) { gst_element_send_event(pipeline_, gst_event_new_eos()); GstBus* bus = gst_element_get_bus(pipeline_); gst_bus_poll(bus, GST_MESSAGE_EOS, GST_CLOCK_TIME_NONE); gst_object_unref(bus); gst_element_set_state(pipeline_, GST_STATE_NULL); gst_object_unref(appsrc_); gst_object_unref(pipeline_); printf("INFO: Saving pipeline released.\n"); } gst_deinit(); printf("INFO: GStreamer deinitialized.\n"); release_face_analyzer(&app_ctx.face_ctx); release_yolo11(&app_ctx.yolo_ctx); deinit_post_process(); printf("INFO: RKNN models released.\n");

//     printf("Exiting main (ret = %d)\n", ret);
//     return ret;
// }






// NEW UI TESTING

// #include <stdint.h>
// #include <stdio.h>
// #include <stdlib.h>
// #include <string.h>
// #include <vector>
// #include <string>
// #include <sstream>
// #include <iomanip>
// #include <chrono>
// #include <stdexcept>
// #include <cmath>
// #include <opencv2/core/types.hpp> // Needed for cv::Rect, cv::Point
// #include <opencv2/opencv.hpp>
// #include <thread>
// #include <queue>
// #include <mutex>
// #include <atomic>
// #include <memory>
// #include <sys/sysinfo.h>
// #include <sys/stat.h>
// #include <unistd.h>
// #include <dirent.h>

// // Include project headers
// #include "face_analyzer/face_analyzer.h"
// #include "yolo_detector/yolo11.h"
// #include "behavior_analysis/BlinkDetector.hpp"
// #include "behavior_analysis/YawnDetector.hpp"
// #include "behavior_analysis/HeadPoseTracker.hpp"
// #include "behavior_analysis/KSSCalculator.hpp"
// #include "image_utils.h"         // Make sure crop_image_simple is declared here or implement it
// #include "file_utils.h"
// #include "image_drawing.h"

// // GStreamer Includes
// #include <gst/gst.h>
// #include <gst/video/video.h>
// #include <gst/app/gstappsink.h>
// #include <gst/allocators/gstdmabuf.h>
// #include <gst/app/gstappsrc.h> 

// // Resource Monitoring Includes
// #include <fstream>
// #include <numeric>
// #include <deque>

// // --- Debugging Flags ---
// #define DEBUG_DRAW_ROIS // Comment out to disable drawing ROIs

// // --- Define colors ---
// #ifndef COLOR_MAGENTA
// #define COLOR_MAGENTA (0xFF00FF) // Pinkish/Magenta
// #endif
// #ifndef COLOR_YELLOW
// #define COLOR_YELLOW (0x00FFFF) // Yellow
// #endif
// #ifndef COLOR_WHITE
// #define COLOR_WHITE (0xFFFFFF) // White
// #endif
// #ifndef COLOR_GREEN
// #define COLOR_GREEN (0x00FF00) // Green
// #endif
// #ifndef COLOR_RED
// #define COLOR_RED (0x0000FF) // Red
// #endif
// #ifndef COLOR_BLUE
// #define COLOR_BLUE (0xFF0000) // Blue
// #endif
// #ifndef COLOR_ORANGE
// #define COLOR_ORANGE (0x00A5FF) // Orange
// #endif
// #ifndef COLOR_CYAN
// #define COLOR_CYAN (0xFFFF00) // Cyan
// #endif


// // --- Application Context ---
// typedef struct app_context_t {
//     face_analyzer_app_context_t face_ctx;
//     yolo11_app_context_t        yolo_ctx;
// } app_context_t;

// // --- Global GStreamer elements ---
// static GstElement* pipeline_ = nullptr; // Saving pipeline
// static GstElement* appsrc_ = nullptr;   // Source for saving pipeline
// static GstElement* appsink_ = nullptr;  // Sink for input pipeline

// // --- Helper Functions ---
// // ... (Keep all helper functions: calculate_centroid, convert_landmarks_to_cvpoint, calculate_ear_simple, calculate_mouth_dist_simple, parse_head_pose_value, calculate_stddev) ...
// cv::Point calculate_centroid(const point_t landmarks[], int count) { if (count == 0) return cv::Point(-1, -1); long long sum_x = 0, sum_y = 0; int valid_points = 0; for (int i = 0; i < count; ++i) { if (landmarks[i].x > -10000 && landmarks[i].x < 10000 && landmarks[i].y > -10000 && landmarks[i].y < 10000) { sum_x += landmarks[i].x; sum_y += landmarks[i].y; valid_points++; } } if (valid_points == 0) return cv::Point(-1, -1); return cv::Point(static_cast<int>(sum_x / valid_points), static_cast<int>(sum_y / valid_points)); }
// std::vector<cv::Point> convert_landmarks_to_cvpoint(const point_t landmarks[], int count) { std::vector<cv::Point> cv_landmarks; if (count <= 0) return cv_landmarks; cv_landmarks.reserve(count); for (int i = 0; i < count; ++i) { if (landmarks[i].x > -10000 && landmarks[i].x < 10000 && landmarks[i].y > -10000 && landmarks[i].y < 10000) { cv_landmarks.emplace_back(cv::Point(landmarks[i].x, landmarks[i].y)); } } return cv_landmarks; }
// float calculate_ear_simple(const std::vector<cv::Point>& landmarks, const std::vector<int>& eye_points) { if (landmarks.empty()) return 1.0f; int max_idx = 0; for(int idx : eye_points) { if (idx > max_idx) max_idx = idx; } if (max_idx >= landmarks.size()) { return 1.0f; } try { cv::Point p1=landmarks.at(eye_points.at(0)); cv::Point p2=landmarks.at(eye_points.at(1)); cv::Point p3=landmarks.at(eye_points.at(2)); cv::Point p4=landmarks.at(eye_points.at(3)); cv::Point p5=landmarks.at(eye_points.at(4)); cv::Point p6=landmarks.at(eye_points.at(5)); double v1=cv::norm(p2-p6); double v2=cv::norm(p3-p5); double h=cv::norm(p1-p4); if(h<1e-6) return 1.0f; return static_cast<float>((v1+v2)/(2.0*h)); } catch(const std::out_of_range& oor) { std::cerr << "Out of Range error in calculate_ear_simple: " << oor.what() << std::endl; return 1.0f; } catch(...) { std::cerr << "Unknown exception in calculate_ear_simple" << std::endl; return 1.0f; } }
// double calculate_mouth_dist_simple(const std::vector<cv::Point>& landmarks) { if (landmarks.empty() || landmarks.size() <= 14) return 0.0; try { return cv::norm(landmarks.at(13)-landmarks.at(14)); } catch(const std::out_of_range& oor) { std::cerr << "Out of Range error in calculate_mouth_dist_simple: " << oor.what() << std::endl; return 0.0; } catch (...) { std::cerr << "Unknown exception in calculate_mouth_dist_simple" << std::endl; return 0.0; } }
// double parse_head_pose_value(const std::string& s) { try { std::string n=s; size_t d=n.find(" deg"); if(d!=std::string::npos) n=n.substr(0,d); size_t f=n.find_first_not_of(" \t"); if(f==std::string::npos) return 0.0; size_t l=n.find_last_not_of(" \t"); n=n.substr(f,l-f+1); if(n.empty()) return 0.0; return std::stod(n); } catch (const std::exception& e) { printf("WARN: Ex parse head pose '%s': %s\n", s.c_str(), e.what()); return 0.0; } catch (...) { printf("WARN: Unk ex parse head pose '%s'.\n", s.c_str()); return 0.0; } }
// template <typename T> T calculate_stddev(const std::deque<T>& data) { if (data.size() < 2) return T(0); T sum = std::accumulate(data.begin(), data.end(), T(0)); T mean = sum / data.size(); T sq_sum = std::inner_product(data.begin(), data.end(), data.begin(), T(0)); T variance = sq_sum / data.size() - mean * mean; return std::sqrt(std::max(T(0), variance)); }

// // --- Simple CPU Cropping Function ---
// static int crop_image_simple(image_buffer_t *src_img, image_buffer_t *dst_img, box_rect_t crop_box) {
//     if (!src_img || !src_img->virt_addr || !dst_img) {
//         printf("ERROR: crop_image_simple null input/output pointer.\n");
//         return -1;
//     }

//     int channels = 0;
//     if (src_img->format == IMAGE_FORMAT_RGB888) {
//         channels = 3;
//     }
// // Put preprocessor directives on separate lines
// #ifdef IMAGE_FORMAT_BGR888
//     else if (src_img->format == IMAGE_FORMAT_BGR888) {
//         channels = 3;
//     }
// #endif
//     // Add checks for other formats if needed, otherwise error out
//     else if (channels == 0) { // Check if format wasn't handled
//         printf("ERROR: crop_image_simple unsupported format %d\n", src_img->format);
//         return -1;
//     }

//     int src_w = src_img->width;
//     int src_h = src_img->height;
//     int crop_x = crop_box.left;
//     int crop_y = crop_box.top;
//     int crop_w = crop_box.right - crop_box.left;
//     int crop_h = crop_box.bottom - crop_box.top;

//     if (crop_w <= 0 || crop_h <= 0) {
//         printf("ERROR: crop ROI invalid size (%dx%d) from box [%d,%d,%d,%d]\n",
//                crop_w, crop_h, crop_box.left, crop_box.top, crop_box.right, crop_box.bottom);
//         return -1;
//     }

//     int x_start = std::max(0, crop_x);
//     int y_start = std::max(0, crop_y);
//     int x_end = std::min(src_w, crop_x + crop_w);
//     int y_end = std::min(src_h, crop_y + crop_h);
//     int valid_crop_w = x_end - x_start;
//     int valid_crop_h = y_end - y_start;

//     if (valid_crop_w <= 0 || valid_crop_h <= 0) {
//         printf("ERROR: Clamped crop ROI zero size. Original ROI [%d,%d,%d,%d], Clamped to src %dx%d -> [%d,%d,%d,%d]\n",
//                crop_box.left, crop_box.top, crop_box.right, crop_box.bottom,
//                src_w, src_h,
//                x_start, y_start, x_end, y_end);
//         return -1;
//     }

//     dst_img->width = crop_w;
//     dst_img->height = crop_h;
//     dst_img->format = src_img->format; // Keep original format
//     dst_img->size = crop_w * crop_h * channels; // Use calculated channels

//     // Allocate or check destination buffer
//     if (dst_img->virt_addr == NULL) {
//         dst_img->virt_addr = (unsigned char*)malloc(dst_img->size);
//         if (!dst_img->virt_addr) {
//             printf("ERROR: Failed alloc memory for crop (%d bytes)\n", dst_img->size);
//             return -1;
//         }
//     } else if (dst_img->size < (size_t)(crop_w * crop_h * channels)) {
//         // Optional: Realloc if needed, or return error
//         printf("ERROR: Dest buffer size %d too small for crop %d.\n", dst_img->size, crop_w * crop_h * channels);
//         // Consider reallocating:
//         // void* new_addr = realloc(dst_img->virt_addr, dst_img->size);
//         // if (!new_addr) { /* handle realloc failure */ return -1; }
//         // dst_img->virt_addr = (unsigned char*)new_addr;
//         return -1; // Current behavior is error
//     }

//     memset(dst_img->virt_addr, 0, dst_img->size); // Clear destination

//     unsigned char* src_data = src_img->virt_addr;
//     unsigned char* dst_data = dst_img->virt_addr;
//     // Use width_stride if provided, otherwise calculate based on width
//     size_t src_stride = src_img->width_stride > 0 ? src_img->width_stride : (size_t)src_w * channels;
//     size_t dst_stride = (size_t)crop_w * channels; // Destination is assumed packed
//     int dst_x_offset = x_start - crop_x;
//     int dst_y_offset = y_start - crop_y;

//     // Copy valid region row by row
//     for (int y = 0; y < valid_crop_h; ++y) {
//         unsigned char* src_row_ptr = src_data + (size_t)(y_start + y) * src_stride + (size_t)x_start * channels;
//         unsigned char* dst_row_ptr = dst_data + (size_t)(dst_y_offset + y) * dst_stride + (size_t)dst_x_offset * channels;
//         memcpy(dst_row_ptr, src_row_ptr, (size_t)valid_crop_w * channels);
//     }
//     return 0;
// }
// // --- End Helper Functions ---


// // --- YOLO Worker Thread ---
// struct YoloInputData { long frame_id; std::shared_ptr<image_buffer_t> image; }; struct YoloOutputData { long frame_id; object_detect_result_list results; }; std::queue<YoloInputData> yolo_input_queue; std::queue<YoloOutputData> yolo_output_queue; std::mutex yolo_input_mutex; std::mutex yolo_output_mutex; std::atomic<bool> stop_yolo_worker(false); const int MAX_QUEUE_SIZE = 5;
// void yolo_worker_thread_func(yolo11_app_context_t* yolo_ctx_ptr) { while (!stop_yolo_worker.load()) { YoloInputData input_data; bool got_data = false; { std::unique_lock<std::mutex> lock(yolo_input_mutex); if (!yolo_input_queue.empty()) { input_data = yolo_input_queue.front(); yolo_input_queue.pop(); got_data = true; } } if (got_data && input_data.image) { YoloOutputData output_data; output_data.frame_id = input_data.frame_id; memset(&output_data.results, 0, sizeof(output_data.results)); int ret = inference_yolo11(yolo_ctx_ptr, input_data.image.get(), &output_data.results); if (ret != 0) printf("WARN: YOLO Worker inference failed (frame %ld), ret=%d\n", input_data.frame_id, ret); { std::unique_lock<std::mutex> lock(yolo_output_mutex); if (yolo_output_queue.size() >= MAX_QUEUE_SIZE) yolo_output_queue.pop(); yolo_output_queue.push(output_data); } if (input_data.image->virt_addr) { free(input_data.image->virt_addr); input_data.image->virt_addr = nullptr; } } else { std::this_thread::sleep_for(std::chrono::milliseconds(5)); } } printf("YOLO Worker Thread Exiting.\n");}


// // --- GStreamer Saving Pipeline Setup ---
// // void setupPipeline() { gst_init(nullptr, nullptr); std::string dir = "/userdata/test_cpp/dms_gst"; if (access(dir.c_str(), W_OK) != 0) { std::cerr << "Directory " << dir << " is not writable or does not exist" << std::endl; return; } DIR* directory = opendir(dir.c_str()); if (!directory) { std::cerr << "Failed to open directory " << dir << std::endl; return; } int mkv_count = 0; struct dirent* entry; while ((entry = readdir(directory)) != nullptr) { std::string filename = entry->d_name; if (filename.find(".mkv") != std::string::npos) mkv_count++; } closedir(directory); std::string filepath = dir + "/dms_multi_" + std::to_string(mkv_count + 1) + ".mkv"; std::string pipeline_str = "appsrc name=source ! queue ! videoconvert ! video/x-raw,format=NV12 ! mpph265enc rc-mode=cbr bps=4000000 gop=30 qp-min=10 qp-max=51 ! h265parse ! matroskamux ! filesink location=" + filepath; std::cout << "Saving Pipeline: " << pipeline_str << std::endl; GError* error = nullptr; pipeline_ = gst_parse_launch(pipeline_str.c_str(), &error); if (!pipeline_ || error) { std::cerr << "Failed to create saving pipeline: " << (error ? error->message : "Unknown error") << std::endl; if (error) g_error_free(error); return; } appsrc_ = gst_bin_get_by_name(GST_BIN(pipeline_), "source"); if (!appsrc_) { std::cerr << "Failed to get appsrc" << std::endl; gst_object_unref(pipeline_); pipeline_ = nullptr; return; } GstCaps* caps = gst_caps_new_simple("video/x-raw", "format", G_TYPE_STRING, "BGR", "width", G_TYPE_INT, 1920, "height", G_TYPE_INT, 1080, "framerate", GST_TYPE_FRACTION, 60, 1, nullptr); g_object_set(G_OBJECT(appsrc_), "caps", caps, "format", GST_FORMAT_TIME, nullptr); gst_caps_unref(caps); if (gst_element_set_state(pipeline_, GST_STATE_PLAYING) == GST_STATE_CHANGE_FAILURE) { std::cerr << "Failed to set saving pipeline to playing" << std::endl; gst_object_unref(appsrc_); gst_object_unref(pipeline_); pipeline_ = nullptr; appsrc_ = nullptr; } }

// void setupPipeline() {
//     gst_init(nullptr, nullptr);
//     const std::string dir = "/userdata/test_cpp/dms_gst"; // Or your desired path

//     // Check if directory exists and is writable
//     struct stat st = {0};
//     if (stat(dir.c_str(), &st) == -1) {
//         if (mkdir(dir.c_str(), 0700) == -1) { // Create if it doesn't exist
//              std::cerr << "Error: Cannot create directory " << dir << ": " << strerror(errno) << std::endl;
//              return;
//         }
//         std::cout << "INFO: Created directory " << dir << std::endl;
//     } else if (access(dir.c_str(), W_OK) != 0) {
//         std::cerr << "Error: Directory " << dir << " is not writable." << std::endl;
//         return;
//     }


//     // Open directory and count .mkv files
//     DIR* directory = opendir(dir.c_str());
//     if (!directory) {
//         std::cerr << "Failed to open directory " << dir << "\n";
//         return;
//     }

//     int mkv_count = 0;
//     struct dirent* entry; // Use struct dirent*
//     while ((entry = readdir(directory)) != nullptr) { // Correct loop condition
//         if (std::string_view(entry->d_name).find(".mkv") != std::string_view::npos) {
//             ++mkv_count;
//         }
//     }
//     closedir(directory);

//     // Construct pipeline string
//     const std::string filepath = dir + "/dms_multi_" + std::to_string(mkv_count + 1) + ".mkv";
//     const std::string pipeline_str =
//         "appsrc name=source ! queue ! videoconvert ! video/x-raw,format=NV12 ! " // Assuming NV12 for encoder
//         "mpph265enc rc-mode=cbr bps=4000000 gop=30 qp-min=10 qp-max=51 ! "
//         "h265parse ! matroskamux ! filesink location=" + filepath;

//     std::cout << "Saving Pipeline: " << pipeline_str << "\n";

//     // Create pipeline
//     GError* error = nullptr;
//     pipeline_ = gst_parse_launch(pipeline_str.c_str(), &error);
//     if (!pipeline_ || error) {
//         std::cerr << "Failed to create saving pipeline: " << (error ? error->message : "Unknown error") << "\n";
//         if (error) {
//             g_error_free(error);
//         }
//         return;
//     }

//     // Get appsrc
//     appsrc_ = gst_bin_get_by_name(GST_BIN(pipeline_), "source");
//     if (!appsrc_) {
//         std::cerr << "Failed to get appsrc\n";
//         gst_object_unref(pipeline_);
//         pipeline_ = nullptr;
//         return;
//     }

//     // ***** REMOVE CAPS SETTING FROM HERE *****
//     // GstCaps* caps = gst_caps_new_simple(...);
//     // g_object_set(G_OBJECT(appsrc_), "caps", caps, "format", GST_FORMAT_TIME, nullptr);
//     // gst_caps_unref(caps);
//     // ******************************************

//      // Set appsrc properties needed BEFORE setting caps later
//      g_object_set(G_OBJECT(appsrc_),
//                   "stream-type", GST_APP_STREAM_TYPE_STREAM, // GST_APP_STREAM_TYPE_SEEKABLE if needed
//                   "format", GST_FORMAT_TIME,
//                   "is-live", FALSE, // Set to TRUE if it's a live source
//                   NULL);


//     // Start pipeline playing (it will wait for data and caps)
//     if (gst_element_set_state(pipeline_, GST_STATE_PLAYING) == GST_STATE_CHANGE_FAILURE) {
//         std::cerr << "Failed to set saving pipeline to playing state\n";
//         gst_object_unref(appsrc_);
//         gst_object_unref(pipeline_);
//         appsrc_ = nullptr;
//         pipeline_ = nullptr;
//     } else {
//         std::cout << "INFO: Saving pipeline created and set to PLAYING (waiting for caps/data).\n";
//     }
// }


// void pushFrameToPipeline(unsigned char* data, int size, int width, int height, GstClockTime duration) { if (!appsrc_) return; GstBuffer* buffer = gst_buffer_new_allocate(nullptr, size, nullptr); GstMapInfo map; if (!gst_buffer_map(buffer, &map, GST_MAP_WRITE)) { std::cerr << "Failed map buffer" << std::endl; gst_buffer_unref(buffer); return; } if (map.size != (guint)size) { std::cerr << "Buffer size mismatch: " << map.size << " vs " << size << std::endl; gst_buffer_unmap(buffer, &map); gst_buffer_unref(buffer); return; } memcpy(map.data, data, size); gst_buffer_unmap(buffer, &map); static GstClockTime timestamp = 0; GST_BUFFER_PTS(buffer) = timestamp; GST_BUFFER_DURATION(buffer) = duration; timestamp += GST_BUFFER_DURATION(buffer); GstFlowReturn ret; g_signal_emit_by_name(appsrc_, "push-buffer", buffer, &ret); if (ret != GST_FLOW_OK) std::cerr << "Failed push buffer, ret=" << ret << std::endl; gst_buffer_unref(buffer); }


// // --- Resource Monitoring Functions ---
// // ... (Keep resource monitoring functions as is) ...
// void calculateOverallFPS(double frame_duration_ms, std::deque<double>& times_deque, double& fps_variable, int max_records) { times_deque.push_back(frame_duration_ms); if (times_deque.size() > max_records) times_deque.pop_front(); if (!times_deque.empty()) { double sum = std::accumulate(times_deque.begin(), times_deque.end(), 0.0); double avg_time_ms = sum / times_deque.size(); fps_variable = (avg_time_ms > 0) ? (1000.0 / avg_time_ms) : 0.0; } else { fps_variable = 0.0; } }
// void getCPUUsage(double& cpu_usage_variable, long& prev_idle, long& prev_total) { std::ifstream file("/proc/stat"); if (!file.is_open()) return; std::string line; std::getline(file, line); file.close(); long user, nice, system, idle, iowait, irq, softirq, steal, guest, guest_nice; user = nice = system = idle = iowait = irq = softirq = steal = guest = guest_nice = 0; std::istringstream iss(line); std::string cpu_label; iss >> cpu_label >> user >> nice >> system >> idle >> iowait >> irq >> softirq >> steal >> guest >> guest_nice; long currentIdleTime = idle + iowait; long currentTotalTime = user + nice + system + idle + iowait + irq + softirq + steal; long diffIdle = currentIdleTime - prev_idle; long diffTotal = currentTotalTime - prev_total; if (diffTotal > 0) cpu_usage_variable = 100.0 * (double)(diffTotal - diffIdle) / diffTotal; prev_idle = currentIdleTime; prev_total = currentTotalTime; }
// void getTemperature(double& temp_variable) { const char* temp_paths[] = {"/sys/class/thermal/thermal_zone0/temp", "/sys/class/thermal/thermal_zone1/temp"}; bool temp_read = false; for (const char* path : temp_paths) { std::ifstream file(path); double temp_milliC = 0; if (file >> temp_milliC) { temp_variable = temp_milliC / 1000.0; temp_read = true; file.close(); break; } file.close(); } }


// /*-------------------------------------------
//                   Main Function
// -------------------------------------------*/
// int main(int argc, char **argv) {

//     // ... (Variable declarations: ret, frame_id, monitoring vars, model paths, video source) ...
//     int ret = 0; long current_frame_id = 0; std::deque<double> frame_times; const int max_time_records = 60; double overallFPS = 0.0; double currentCpuUsage = 0.0; long prevIdleTime = 0, prevTotalTime = 0; double currentTemp = 0.0;

//     // const char *detection_model_path = "../../model/faceD.rknn";
//     const char *detection_model_path = "../../model/rf.rknn";
//     const char *landmark_model_path  = "../../model/faceL.rknn";
//     const char *iris_model_path = "../../model/faceI.rknn";
//     const char *yolo_model_path = "../../model/od.rknn";

//     const char *video_source = "filesrc location=../../model/wingbody4.mkv ! decodebin ! queue ! videoconvert ! video/x-raw,format=BGR ! appsink name=sink sync=false";

//     // --- Initialization ---
//     setupPipeline(); // Setup saving pipeline (without caps)
//     // ... (Init contexts, buffers, behavior modules) ...
//     app_context_t app_ctx; memset(&app_ctx, 0, sizeof(app_context_t)); image_buffer_t src_image; memset(&src_image, 0, sizeof(image_buffer_t)); face_analyzer_result_t face_results; memset(&face_results, 0, sizeof(face_results)); object_detect_result_list yolo_results; memset(&yolo_results, 0, sizeof(yolo_results)); my::BlinkDetector blinkDetector; YawnDetector yawnDetector; my::HeadPoseTracker headPoseTracker; KSSCalculator kssCalculator;

//     // --- Calibration State Variables ---
//     // ... (Calibration vars) ...
//      bool calibration_done = false; std::chrono::steady_clock::time_point calibration_start_time; bool calibration_timer_started = false; int consecutive_valid_eyes_frames = 0; const int REQUIRED_VALID_EYES_FRAMES = 60; bool ear_calibrated = false; bool mouth_calibrated = false; std::deque<float> calib_left_ears; std::deque<float> calib_right_ears; std::deque<double> calib_mouth_dists; int consecutive_stable_ear_frames = 0; int consecutive_stable_mouth_frames = 0; const int CALIB_WINDOW_SIZE = 30; const float EAR_STDDEV_THRESHOLD = 0.04; const double MOUTH_DIST_STDDEV_THRESHOLD = 15.0; const int REQUIRED_STABLE_FRAMES = CALIB_WINDOW_SIZE + 5; const double CALIBRATION_TIMEOUT_SECONDS = 10.0;


//     // --- Driver ID & Tracking State ---
//     // ... (Driver ID vars) ...
//      bool driver_identified_ever = false; bool driver_tracked_this_frame = false; int current_tracked_driver_idx = -1; cv::Point prev_driver_centroid = cv::Point(-1, -1); const double MAX_CENTROID_DISTANCE = 150.0; int driver_search_timeout_frames = 0; const int DRIVER_SEARCH_MAX_FRAMES = 60;

//     // --- Fixed Driver Object ROI ---
//     // ... (Fixed ROI vars) ...
//      cv::Rect driver_object_roi; bool driver_roi_defined = false; bool valid_object_roi = false;

//     // --- State variables for KSS and display ---
//     // ... (KSS/Display vars) ...
//      std::string kssStatus = "Initializing"; YawnDetector::YawnMetrics yawnMetrics = {}; my::HeadPoseTracker::HeadPoseResults headPoseResults = {}; std::vector<std::string> detectedObjects; int extractedTotalKSS = 1; int perclosKSS = 1, blinkKSS = 1, headposeKSS = 1, yawnKSS = 1, objdectdetectionKSS = 1; std::stringstream text_stream; int text_y = 0; const int line_height = 22; const int text_size = 12; const int status_text_size = 16; unsigned int status_color_uint = COLOR_GREEN;


//     // --- YOLO thread handle ---
//     std::thread yolo_worker_thread;

//     // --- Init models ---
//     // ... (Model init) ...
//      ret = init_post_process(); if (ret != 0) { /*...*/ return -1; } ret = init_face_analyzer(detection_model_path, landmark_model_path, iris_model_path, &app_ctx.face_ctx); if (ret != 0) { /*...*/ return -1; } ret = init_yolo11(yolo_model_path, &app_ctx.yolo_ctx); if (ret != 0) { /*...*/ return -1; } yolo_worker_thread = std::thread(yolo_worker_thread_func, &app_ctx.yolo_ctx);

//     // --- Init GStreamer input ---
//     // ... (GStreamer input init) ...
//      gst_init(nullptr, nullptr); GError* error = nullptr; GstElement* input_pipeline = gst_parse_launch(video_source, &error); if (!input_pipeline || error) { /*...*/ return -1; } appsink_ = gst_bin_get_by_name(GST_BIN(input_pipeline), "sink"); if (!appsink_) { /*...*/ return -1; } if (gst_element_set_state(input_pipeline, GST_STATE_PLAYING) == GST_STATE_CHANGE_FAILURE) { /*...*/ return -1; }

//     // --- Setup OpenCV window ---
//     cv::namedWindow("DMS Output", cv::WINDOW_NORMAL); cv::resizeWindow("DMS Output", 1920, 1080);

//     GstSample* sample; GstVideoInfo video_info; GstBuffer* gst_buffer; GstMapInfo map_info;
//     // bool first_frame = true; // Can remove this if only used for ROI def
//     bool saving_pipeline_caps_set = false; // <<< Flag to track if saving caps are set

//     // --- Buffer for the FIXED ROI Crop ---
//     image_buffer_t fixed_roi_crop_img;
//     memset(&fixed_roi_crop_img, 0, sizeof(image_buffer_t));

//     // --- Main Processing Loop ---
//     while (true) {
//         // --- Get Frame ---
//          sample = gst_app_sink_pull_sample(GST_APP_SINK(appsink_)); if (!sample) { std::cerr << "End of stream or error pulling sample." << std::endl; break; } gst_buffer = gst_sample_get_buffer(sample); GstCaps* caps = gst_sample_get_caps(sample); if (!gst_buffer || !caps || !gst_video_info_from_caps(&video_info, caps) || !gst_buffer_map(gst_buffer, &map_info, GST_MAP_READ)) { std::cerr << "Failed to get valid buffer/caps/map" << std::endl; if (gst_buffer && map_info.memory) gst_buffer_unmap(gst_buffer, &map_info); if (sample) gst_sample_unref(sample); continue; }

//         auto frame_start_time = std::chrono::high_resolution_clock::now();
//         current_frame_id++;

//         // --- Wrap GStreamer Data for the *full source image* ---
//         src_image.width = GST_VIDEO_INFO_WIDTH(&video_info);
//         src_image.height = GST_VIDEO_INFO_HEIGHT(&video_info);
//         src_image.width_stride = GST_VIDEO_INFO_PLANE_STRIDE(&video_info, 0);
//         src_image.height_stride = src_image.height;
//         // Determine format based on GStreamer info if possible, otherwise default
//         GstVideoFormat gst_format = GST_VIDEO_INFO_FORMAT(&video_info);


//         src_image.format = IMAGE_FORMAT_RGB888;
//         src_image.virt_addr = map_info.data;
//         src_image.size = map_info.size;
//         src_image.fd = -1;


//         // --- Set Saving Pipeline Caps (on first valid frame) ---
//         if (pipeline_ && appsrc_ && !saving_pipeline_caps_set) {
//              GstCaps* save_caps = gst_caps_new_simple("video/x-raw",
//                                              "format", G_TYPE_STRING, "BGR", // Input to saving pipeline is BGR
//                                              "width", G_TYPE_INT, src_image.width,
//                                              "height", G_TYPE_INT, src_image.height,
//                                              "framerate", GST_TYPE_FRACTION, GST_VIDEO_INFO_FPS_N(&video_info), GST_VIDEO_INFO_FPS_D(&video_info),
//                                              NULL);
//              if (save_caps) {
//                   printf("INFO: Setting saving pipeline caps to: %s\n", gst_caps_to_string(save_caps));
//                   g_object_set(G_OBJECT(appsrc_), "caps", save_caps, NULL);
//                   gst_caps_unref(save_caps);
//                   saving_pipeline_caps_set = true;
//              } else {
//                   std::cerr << "ERROR: Failed to create caps for saving pipeline!\n";
//                   // Consider stopping or disabling saving
//              }
//         }
//         // --- End Set Saving Caps ---


//         // --- Run Face Analysis on the FULL FRAME ---
//         ret = inference_face_analyzer(&app_ctx.face_ctx, &src_image, &face_results);
//         if (ret != 0) { printf("WARN: Face Analyzer Inference failed frame %ld, ret=%d\n", current_frame_id, ret); face_results.count = 0; }

//         // --- Reset frame-specific flags ---
//         driver_tracked_this_frame = false;
//         current_tracked_driver_idx = -1;
//         valid_object_roi = false; // Reset each frame

//         // --- Driver Identification and Tracking (Based on Full Frame Results) ---
//         if (!driver_identified_ever) { /* Identification */ int best_candidate_idx = -1; float max_area = 0.0f; cv::Rect temp_id_zone((int)(src_image.width * 0.40),(int)(src_image.height * 0.1),(int)(src_image.width * 0.55),(int)(src_image.height * 0.8)); temp_id_zone &= cv::Rect(0, 0, src_image.width, src_image.height); for (int i = 0; i < face_results.count; ++i) { cv::Point face_center((face_results.faces[i].box.left + face_results.faces[i].box.right) / 2, (face_results.faces[i].box.top + face_results.faces[i].box.bottom) / 2); if (temp_id_zone.contains(face_center) && face_results.faces[i].face_landmarks_valid) { float area = (float)(face_results.faces[i].box.right - face_results.faces[i].box.left) * (face_results.faces[i].box.bottom - face_results.faces[i].box.top); if (area > max_area) { max_area = area; best_candidate_idx = i; } } } if (best_candidate_idx != -1) { driver_identified_ever = true; driver_tracked_this_frame = true; current_tracked_driver_idx = best_candidate_idx; prev_driver_centroid = calculate_centroid(face_results.faces[best_candidate_idx].face_landmarks, NUM_FACE_LANDMARKS); driver_search_timeout_frames = 0; printf("INFO: Driver Identified (Index %d) at frame %ld\n", current_tracked_driver_idx, current_frame_id); kssStatus = "Calibrating..."; } else { kssStatus = "Searching Driver..."; } } else { /* Tracking */ int best_match_idx = -1; double min_dist = MAX_CENTROID_DISTANCE; for (int i = 0; i < face_results.count; ++i) { if (face_results.faces[i].face_landmarks_valid) { cv::Point current_centroid = calculate_centroid(face_results.faces[i].face_landmarks, NUM_FACE_LANDMARKS); if (prev_driver_centroid.x >= 0 && current_centroid.x >= 0) { double dist = cv::norm(current_centroid - prev_driver_centroid); if (dist < min_dist) { min_dist = dist; best_match_idx = i; } } } } if (best_match_idx != -1) { driver_tracked_this_frame = true; current_tracked_driver_idx = best_match_idx; prev_driver_centroid = calculate_centroid(face_results.faces[best_match_idx].face_landmarks, NUM_FACE_LANDMARKS); driver_search_timeout_frames = 0; if (!calibration_done) kssStatus = "Calibrating..."; } else { driver_tracked_this_frame = false; current_tracked_driver_idx = -1; prev_driver_centroid = cv::Point(-1, -1); driver_search_timeout_frames++; kssStatus = "Driver Lost..."; if (driver_search_timeout_frames > DRIVER_SEARCH_MAX_FRAMES) { driver_identified_ever = false; driver_search_timeout_frames = 0; printf("INFO: Driver track lost for %d frames. Reverting to search.\n", DRIVER_SEARCH_MAX_FRAMES); kssStatus = "Searching Driver..."; } } }


//         // --- Calibration or Normal Processing ---
//         // if (!calibration_done) {
//         //     // --- Calibration Logic ---
//         //      bool head_pose_calibrated = false; bool eyes_consistently_valid = false; double elapsed_calib_seconds = 0.0; std::string calib_status_detail = ""; if (driver_tracked_this_frame && current_tracked_driver_idx != -1) { face_object_t *calib_face = &face_results.faces[current_tracked_driver_idx]; if (calib_face->face_landmarks_valid) { if (!calibration_timer_started) { /* Start timer, reset state */ calibration_start_time = std::chrono::steady_clock::now(); calibration_timer_started = true; printf("INFO: Calibration timer started (Driver Index %d).\n", current_tracked_driver_idx); consecutive_valid_eyes_frames = 0; consecutive_stable_ear_frames = 0; consecutive_stable_mouth_frames = 0; calib_left_ears.clear(); calib_right_ears.clear(); calib_mouth_dists.clear(); ear_calibrated = false; mouth_calibrated = false; } /* ... Run checks ... */ std::vector<cv::Point> calib_faceLandmarks = convert_landmarks_to_cvpoint(calib_face->face_landmarks, NUM_FACE_LANDMARKS); headPoseTracker.run(calib_faceLandmarks); head_pose_calibrated = headPoseTracker.isCalibrated(); if (!head_pose_calibrated) calib_status_detail += " (Head)"; bool eyes_valid_this_frame = calib_face->eye_landmarks_left_valid && calib_face->eye_landmarks_right_valid; if (eyes_valid_this_frame) consecutive_valid_eyes_frames++; else consecutive_valid_eyes_frames = 0; eyes_consistently_valid = (consecutive_valid_eyes_frames >= REQUIRED_VALID_EYES_FRAMES); if (!eyes_consistently_valid) calib_status_detail += " (Eyes Valid)"; if (!ear_calibrated && eyes_valid_this_frame) { const std::vector<int> L={33,160,158,133,153,144}, R={362,385,387,263,380,373}; float lE=calculate_ear_simple(calib_faceLandmarks,L), rE=calculate_ear_simple(calib_faceLandmarks,R); calib_left_ears.push_back(lE); calib_right_ears.push_back(rE); if (calib_left_ears.size()>CALIB_WINDOW_SIZE) calib_left_ears.pop_front(); if (calib_right_ears.size()>CALIB_WINDOW_SIZE) calib_right_ears.pop_front(); if (calib_left_ears.size()>=CALIB_WINDOW_SIZE) { float lS=calculate_stddev(calib_left_ears), rS=calculate_stddev(calib_right_ears); if (lS<EAR_STDDEV_THRESHOLD && rS<EAR_STDDEV_THRESHOLD) consecutive_stable_ear_frames++; else consecutive_stable_ear_frames=0; } else consecutive_stable_ear_frames=0; ear_calibrated=(consecutive_stable_ear_frames>=REQUIRED_STABLE_FRAMES); } else if(!eyes_valid_this_frame){ consecutive_stable_ear_frames=0; ear_calibrated=false; calib_left_ears.clear(); calib_right_ears.clear(); } if(!ear_calibrated) calib_status_detail+=" (EAR Stable)"; if (!mouth_calibrated) { double cM=calculate_mouth_dist_simple(calib_faceLandmarks); calib_mouth_dists.push_back(cM); if(calib_mouth_dists.size()>CALIB_WINDOW_SIZE) calib_mouth_dists.pop_front(); if(calib_mouth_dists.size()>=CALIB_WINDOW_SIZE){ double mS=calculate_stddev(calib_mouth_dists); if(mS<MOUTH_DIST_STDDEV_THRESHOLD) consecutive_stable_mouth_frames++; else consecutive_stable_mouth_frames=0; } else consecutive_stable_mouth_frames=0; mouth_calibrated=(consecutive_stable_mouth_frames>=REQUIRED_STABLE_FRAMES); } if(!mouth_calibrated) calib_status_detail+=" (Mouth Stable)"; auto now = std::chrono::steady_clock::now(); elapsed_calib_seconds = std::chrono::duration<double>(now - calibration_start_time).count(); if (elapsed_calib_seconds >= CALIBRATION_TIMEOUT_SECONDS) { if (head_pose_calibrated && eyes_consistently_valid && ear_calibrated && mouth_calibrated) { calibration_done = true; printf("INFO: Calibration Complete!\n"); /* Define Fixed ROI */ face_object_t *calib_driver_face = &face_results.faces[current_tracked_driver_idx]; box_rect_t calib_driver_box = calib_driver_face->box; int calib_box_w = calib_driver_box.right - calib_driver_box.left; int calib_box_h = calib_driver_box.bottom - calib_driver_box.top; if(calib_box_w > 0 && calib_box_h > 0) { int box_center_x = (calib_driver_box.left + calib_driver_box.right) / 2; int box_center_y = (calib_driver_box.top + calib_driver_box.bottom) / 2; const float width_expansion = 2.2f; const float height_expansion_up = 0.8f; const float height_expansion_down = 3.0f; int roi_width = static_cast<int>(calib_box_w * width_expansion); int roi_height = static_cast<int>(calib_box_h * (height_expansion_up + height_expansion_down)); int roi_size = std::max(roi_width, roi_height); roi_width = roi_size; roi_height = roi_size; if (roi_width < 640) roi_width = 640; int roi_x = box_center_x - roi_width / 2; int roi_y = box_center_y - static_cast<int>(calib_box_h * height_expansion_up); driver_object_roi.x = std::max(0, roi_x); driver_object_roi.y = std::max(0, roi_y); driver_object_roi.width = std::min(roi_width, src_image.width - driver_object_roi.x); driver_object_roi.height = std::min(roi_height, src_image.height - driver_object_roi.y); driver_roi_defined = (driver_object_roi.width > 0 && driver_object_roi.height > 0); printf("INFO: Fixed Driver Object ROI defined: [%d, %d, %d x %d]\n", driver_object_roi.x, driver_object_roi.y, driver_object_roi.width, driver_object_roi.height); } else { driver_roi_defined = false; printf("WARN: Could not define fixed ROI due to invalid calibration box.\n"); } } else { /* Timeout Failed */ calibration_timer_started = false; driver_identified_ever = false; driver_tracked_this_frame = false; current_tracked_driver_idx = -1; prev_driver_centroid = cv::Point(-1, -1); consecutive_valid_eyes_frames = 0; consecutive_stable_ear_frames = 0; consecutive_stable_mouth_frames = 0; ear_calibrated = false; mouth_calibrated = false; calib_left_ears.clear(); calib_right_ears.clear(); calib_mouth_dists.clear(); printf("WARN: Calibration time expired, criteria not met (H:%d, EV:%d, ES:%d, MS:%d). Retrying identification.\n", head_pose_calibrated, eyes_consistently_valid, ear_calibrated, mouth_calibrated); } } } else { calibration_timer_started = false; printf("WARN: Tracked driver landmarks not valid during calibration.\n"); } kssStatus = "Calibrating... " + std::to_string(static_cast<int>(elapsed_calib_seconds)) + "s" + calib_status_detail; if(driver_tracked_this_frame && current_tracked_driver_idx != -1) draw_rectangle(&src_image, face_results.faces[current_tracked_driver_idx].box.left, face_results.faces[current_tracked_driver_idx].box.top, face_results.faces[current_tracked_driver_idx].box.right - face_results.faces[current_tracked_driver_idx].box.left, face_results.faces[current_tracked_driver_idx].box.bottom - face_results.faces[current_tracked_driver_idx].box.top, COLOR_YELLOW, 2); } else { /* No driver tracked */ calibration_timer_started = false; consecutive_valid_eyes_frames = 0; consecutive_stable_ear_frames = 0; consecutive_stable_mouth_frames = 0; ear_calibrated = false; mouth_calibrated = false; calib_left_ears.clear(); calib_right_ears.clear(); calib_mouth_dists.clear(); if (driver_identified_ever) { kssStatus = "Calibration: Waiting for Driver Track..."; } else { kssStatus = "Calibration: Searching Driver..."; } for (int i = 0; i < face_results.count; ++i) { draw_rectangle(&src_image, face_results.faces[i].box.left, face_results.faces[i].box.top, face_results.faces[i].box.right - face_results.faces[i].box.left, face_results.faces[i].box.bottom - face_results.faces[i].box.top, COLOR_YELLOW, 1); } }
//         //     // Draw Calibration Status Text
//         //      draw_text(&src_image, kssStatus.c_str(), 10, 10, COLOR_YELLOW, status_text_size);

//         // } 
//         if (!calibration_done) {
//             // --- Calibration Logic ---
//              bool head_pose_calibrated = false; bool eyes_consistently_valid = false; double elapsed_calib_seconds = 0.0; std::string calib_status_detail = "";
//              kssStatus = "Initializing"; // Default status during calibration attempts

//              if (driver_tracked_this_frame && current_tracked_driver_idx != -1) { // Calibration requires a tracked driver
//                  face_object_t *calib_face = &face_results.faces[current_tracked_driver_idx];
//                  if (calib_face->face_landmarks_valid) {
//                     // ... (Start timer logic, run calibration checks as before) ...
//                      if (!calibration_timer_started) { /* Start timer, reset state */ calibration_start_time = std::chrono::steady_clock::now(); calibration_timer_started = true; printf("INFO: Calibration timer started (Driver Index %d).\n", current_tracked_driver_idx); consecutive_valid_eyes_frames = 0; consecutive_stable_ear_frames = 0; consecutive_stable_mouth_frames = 0; calib_left_ears.clear(); calib_right_ears.clear(); calib_mouth_dists.clear(); ear_calibrated = false; mouth_calibrated = false; }
//                      std::vector<cv::Point> calib_faceLandmarks = convert_landmarks_to_cvpoint(calib_face->face_landmarks, NUM_FACE_LANDMARKS);
//                      if (!calib_faceLandmarks.empty()) { // Check if conversion yielded points
//                          headPoseTracker.run(calib_faceLandmarks); head_pose_calibrated = headPoseTracker.isCalibrated(); if (!head_pose_calibrated) calib_status_detail += " (Head)"; bool eyes_valid_this_frame = calib_face->eye_landmarks_left_valid && calib_face->eye_landmarks_right_valid; if (eyes_valid_this_frame) consecutive_valid_eyes_frames++; else consecutive_valid_eyes_frames = 0; eyes_consistently_valid = (consecutive_valid_eyes_frames >= REQUIRED_VALID_EYES_FRAMES); if (!eyes_consistently_valid) calib_status_detail += " (Eyes Valid)"; if (!ear_calibrated && eyes_valid_this_frame) { const std::vector<int> L={33,160,158,133,153,144}, R={362,385,387,263,380,373}; float lE=calculate_ear_simple(calib_faceLandmarks,L), rE=calculate_ear_simple(calib_faceLandmarks,R); calib_left_ears.push_back(lE); calib_right_ears.push_back(rE); if (calib_left_ears.size()>CALIB_WINDOW_SIZE) calib_left_ears.pop_front(); if (calib_right_ears.size()>CALIB_WINDOW_SIZE) calib_right_ears.pop_front(); if (calib_left_ears.size()>=CALIB_WINDOW_SIZE) { float lS=calculate_stddev(calib_left_ears), rS=calculate_stddev(calib_right_ears); if (lS<EAR_STDDEV_THRESHOLD && rS<EAR_STDDEV_THRESHOLD) consecutive_stable_ear_frames++; else consecutive_stable_ear_frames=0; } else consecutive_stable_ear_frames=0; ear_calibrated=(consecutive_stable_ear_frames>=REQUIRED_STABLE_FRAMES); } else if(!eyes_valid_this_frame){ consecutive_stable_ear_frames=0; ear_calibrated=false; calib_left_ears.clear(); calib_right_ears.clear(); } if(!ear_calibrated) calib_status_detail+=" (EAR Stable)"; if (!mouth_calibrated) { double cM=calculate_mouth_dist_simple(calib_faceLandmarks); calib_mouth_dists.push_back(cM); if(calib_mouth_dists.size()>CALIB_WINDOW_SIZE) calib_mouth_dists.pop_front(); if(calib_mouth_dists.size()>=CALIB_WINDOW_SIZE){ double mS=calculate_stddev(calib_mouth_dists); if(mS<MOUTH_DIST_STDDEV_THRESHOLD) consecutive_stable_mouth_frames++; else consecutive_stable_mouth_frames=0; } else consecutive_stable_mouth_frames=0; mouth_calibrated=(consecutive_stable_mouth_frames>=REQUIRED_STABLE_FRAMES); } if(!mouth_calibrated) calib_status_detail+=" (Mouth Stable)"; auto now = std::chrono::steady_clock::now(); elapsed_calib_seconds = std::chrono::duration<double>(now - calibration_start_time).count();
//                     } else {
//                         // Handle case where landmark conversion failed
//                          head_pose_calibrated = false; // Cannot calibrate head pose
//                          eyes_consistently_valid = false; // Cannot check eye validity
//                          ear_calibrated = false; // Cannot check EAR
//                          mouth_calibrated = false; // Cannot check mouth
//                          calibration_timer_started = false; // Reset timer
//                          printf("WARN: Landmark conversion failed during calibration.\n");
//                          calib_status_detail += " (Lmk Err)";
//                     }


//                      // ... (Check for completion or timeout, define fixed ROI logic remains the same) ...
//                      if (elapsed_calib_seconds >= CALIBRATION_TIMEOUT_SECONDS) { /* ... timeout/completion logic ... */ if (head_pose_calibrated && eyes_consistently_valid && ear_calibrated && mouth_calibrated) { calibration_done = true; printf("INFO: Calibration Complete!\n"); /* Define Fixed ROI */ face_object_t *calib_driver_face = &face_results.faces[current_tracked_driver_idx]; box_rect_t calib_driver_box = calib_driver_face->box; int calib_box_w = calib_driver_box.right - calib_driver_box.left; int calib_box_h = calib_driver_box.bottom - calib_driver_box.top; if(calib_box_w > 0 && calib_box_h > 0) { int box_center_x = (calib_driver_box.left + calib_driver_box.right) / 2; int box_center_y = (calib_driver_box.top + calib_driver_box.bottom) / 2; const float width_expansion = 2.2f; const float height_expansion_up = 0.8f; const float height_expansion_down = 3.0f; int roi_width = static_cast<int>(calib_box_w * width_expansion); int roi_height = static_cast<int>(calib_box_h * (height_expansion_up + height_expansion_down)); int roi_size = std::max(roi_width, roi_height); roi_width = roi_size; roi_height = roi_size; if (roi_width < 640) {roi_width = 640; roi_height = 640;} int roi_x = box_center_x - roi_width / 2; int roi_y = box_center_y - static_cast<int>(calib_box_h * height_expansion_up); driver_object_roi.x = std::max(0, roi_x); driver_object_roi.y = std::max(0, roi_y); driver_object_roi.width = std::min(roi_width, src_image.width - driver_object_roi.x); driver_object_roi.height = std::min(roi_height, src_image.height - driver_object_roi.y); driver_roi_defined = (driver_object_roi.width > 0 && driver_object_roi.height > 0); printf("INFO: Fixed Driver Object ROI defined: [%d, %d, %d x %d]\n", driver_object_roi.x, driver_object_roi.y, driver_object_roi.width, driver_object_roi.height); } else { driver_roi_defined = false; printf("WARN: Could not define fixed ROI due to invalid calibration box.\n"); } } else { /* Timeout Failed */ calibration_timer_started = false; driver_identified_ever = false; driver_tracked_this_frame = false; current_tracked_driver_idx = -1; prev_driver_centroid = cv::Point(-1, -1); consecutive_valid_eyes_frames = 0; consecutive_stable_ear_frames = 0; consecutive_stable_mouth_frames = 0; ear_calibrated = false; mouth_calibrated = false; calib_left_ears.clear(); calib_right_ears.clear(); calib_mouth_dists.clear(); printf("WARN: Calibration time expired, criteria not met (H:%d, EV:%d, ES:%d, MS:%d). Retrying identification.\n", head_pose_calibrated, eyes_consistently_valid, ear_calibrated, mouth_calibrated); } }

//                  } else { calibration_timer_started = false; printf("WARN: Tracked driver landmarks not valid during calibration.\n"); kssStatus = "Calibration: Landmark Invalid"; } // Update status

//                  // Update overall calibration status text
//                  if (calibration_timer_started && !calibration_done) {
//                      kssStatus = "Calibrating... " + std::to_string(static_cast<int>(elapsed_calib_seconds)) + "s" + calib_status_detail;
//                  }

//                  // Draw yellow box around the face being calibrated
//                  draw_rectangle(&src_image, calib_face->box.left, calib_face->box.top, calib_face->box.right - calib_face->box.left, calib_face->box.bottom - calib_face->box.top, COLOR_YELLOW, 2);

//                  // +++++++++++++ ADDED: Draw Calibration Reference Points +++++++++++++
//                  if (calib_face->face_landmarks_valid) {
//                      const int calib_indices[] = {
//                          my::HeadPoseTracker::LandmarkIndex::LEFT_EYE,
//                          my::HeadPoseTracker::LandmarkIndex::RIGHT_EYE,
//                          my::HeadPoseTracker::LandmarkIndex::FOREHEAD,
//                          my::HeadPoseTracker::LandmarkIndex::MOUTH_CENTER,
//                          my::HeadPoseTracker::LandmarkIndex::NOSE_TIP
//                      };
//                      for (int idx : calib_indices) {
//                          if (idx < NUM_FACE_LANDMARKS) { // Basic bounds check
//                              draw_circle(&src_image,
//                                          calib_face->face_landmarks[idx].x,
//                                          calib_face->face_landmarks[idx].y,
//                                          3,         // Make calibration points slightly larger
//                                          COLOR_RED, // Use Red for visibility
//                                          -1);       // Filled circle
//                          }
//                      }
//                  }
//                  // ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

//              } else { // No driver tracked during calibration phase
//                  // ... (Reset calibration state variables as before) ...
//                   calibration_timer_started = false; consecutive_valid_eyes_frames = 0; consecutive_stable_ear_frames = 0; consecutive_stable_mouth_frames = 0; ear_calibrated = false; mouth_calibrated = false; calib_left_ears.clear(); calib_right_ears.clear(); calib_mouth_dists.clear();
//                  if (driver_identified_ever) { kssStatus = "Calibration: Waiting for Driver Track..."; } else { kssStatus = "Calibration: Searching Driver..."; }
//                  // Draw boxes for all detected faces during search
//                  for (int i = 0; i < face_results.count; ++i) { draw_rectangle(&src_image, face_results.faces[i].box.left, face_results.faces[i].box.top, face_results.faces[i].box.right - face_results.faces[i].box.left, face_results.faces[i].box.bottom - face_results.faces[i].box.top, COLOR_YELLOW, 1); }
//              }
//              // Draw the overall calibration status text determined above
//              draw_text(&src_image, kssStatus.c_str(), 10, 10, COLOR_YELLOW, status_text_size);

//         }
        
//         else { // --- Normal Processing Phase (Calibration is Done) ---

//             // Default Status for this phase
//             kssStatus = "Normal"; // Assume normal unless KSS says otherwise or driver lost

//             if (!driver_roi_defined) {
//                 kssStatus = "ROI Error";
//                 // ... (Reset KSS values) ...
//                 printf("ERROR: Driver Object ROI was not defined. Skipping processing.\n"); extractedTotalKSS = 1; perclosKSS = 1; blinkKSS = 1; headposeKSS = 1; yawnKSS = 1; objdectdetectionKSS = 1; yawnMetrics = {}; headPoseResults = {}; detectedObjects.clear();
//             } else {
//                 // --- Crop to the FIXED Driver Object ROI ---
//                 // ... (Crop logic) ...
//                  if (fixed_roi_crop_img.virt_addr) { free(fixed_roi_crop_img.virt_addr); fixed_roi_crop_img.virt_addr = nullptr; } memset(&fixed_roi_crop_img, 0, sizeof(image_buffer_t)); box_rect_t fixed_roi_as_box = { driver_object_roi.x, driver_object_roi.y, driver_object_roi.x + driver_object_roi.width, driver_object_roi.y + driver_object_roi.height}; ret = crop_image_simple(&src_image, &fixed_roi_crop_img, fixed_roi_as_box);

//                 if (ret == 0 && fixed_roi_crop_img.virt_addr) {
//                     // --- Queue FIXED ROI CROP for YOLO ---
//                     // ... (YOLO queuing logic) ...
//                     auto yolo_input_image = std::make_shared<image_buffer_t>(); yolo_input_image->width = fixed_roi_crop_img.width; yolo_input_image->height = fixed_roi_crop_img.height; yolo_input_image->format = fixed_roi_crop_img.format; yolo_input_image->size = fixed_roi_crop_img.size; yolo_input_image->virt_addr = (unsigned char*)malloc(yolo_input_image->size); if (yolo_input_image->virt_addr) { memcpy(yolo_input_image->virt_addr, fixed_roi_crop_img.virt_addr, yolo_input_image->size); yolo_input_image->width_stride = 0; yolo_input_image->height_stride = 0; yolo_input_image->fd = -1; { std::unique_lock<std::mutex> lock(yolo_input_mutex); if (yolo_input_queue.size() < MAX_QUEUE_SIZE) yolo_input_queue.push({current_frame_id, yolo_input_image}); else { free(yolo_input_image->virt_addr); yolo_input_image->virt_addr = nullptr; } } } else { printf("ERROR: Failed alloc YOLO input copy (fixed roi crop).\n"); }
//                 } else { printf("ERROR: Failed to crop to FIXED ROI for YOLO frame %ld.\n", current_frame_id); }

//                 // --- Get YOLO Results ---
//                  bool yolo_result_received = false; { std::unique_lock<std::mutex> lock(yolo_output_mutex); if (!yolo_output_queue.empty()) { YoloOutputData data = yolo_output_queue.front(); yolo_output_queue.pop(); yolo_results = data.results; yolo_result_received = true; } }

//                 // --- Run Behavior Analysis & KSS (Conditional on Driver Tracked & In ROI) ---
//                 detectedObjects.clear();
//                 std::vector<std::string> driver_detectedObjects_final;
//                 valid_object_roi = driver_roi_defined;

//                 if (driver_tracked_this_frame && current_tracked_driver_idx != -1) {
//                     face_object_t *driver_face = &face_results.faces[current_tracked_driver_idx];
//                     cv::Point driver_center_full_frame( (driver_face->box.left + driver_face->box.right) / 2, (driver_face->box.top + driver_face->box.bottom) / 2 );

//                     if (driver_object_roi.contains(driver_center_full_frame)) { // Driver IN ROI
//                         // Filter YOLO results
//                         if (yolo_results.count > 0) { for (int j = 0; j < yolo_results.count; ++j) { object_detect_result* det = &yolo_results.results[j]; if (det->prop > 0.4) { driver_detectedObjects_final.push_back(coco_cls_to_name(det->cls_id)); } } }
//                         detectedObjects = driver_detectedObjects_final;

//                         // Run Behavior Modules
//                         if (driver_face->face_landmarks_valid) {
//                             std::vector<cv::Point> faceLandmarksCv = convert_landmarks_to_cvpoint(driver_face->face_landmarks, NUM_FACE_LANDMARKS);
//                             if (!faceLandmarksCv.empty()) {
//                                  headPoseResults = headPoseTracker.run(faceLandmarksCv); blinkDetector.run(faceLandmarksCv, src_image.width, src_image.height); yawnMetrics = yawnDetector.run(faceLandmarksCv, src_image.width, src_image.height); kssCalculator.setPerclos(blinkDetector.getPerclos()); int headPoseKSSValue = 1; if (headPoseResults.rows.size() >= 4) { for (const auto& row : headPoseResults.rows) { if (row.size() >= 2 && row[0] == "Head KSS") { try { headPoseKSSValue = std::stoi(row[1]); } catch (...) { headPoseKSSValue = 1; } break; } } } kssCalculator.setHeadPose(headPoseKSSValue); kssCalculator.setYawnMetrics(yawnMetrics.isYawning, yawnMetrics.yawnFrequency_5min, yawnMetrics.yawnDuration); kssCalculator.setBlinksLastMinute(blinkDetector.getBlinksInWindow());
//                             } else { kssStatus = "Landmark Error"; /* Reset Inputs */ headPoseResults = {}; yawnMetrics = {}; kssCalculator.setPerclos(0); kssCalculator.setHeadPose(1); kssCalculator.setYawnMetrics(false, 0, 0); kssCalculator.setBlinksLastMinute(0); }
//                         } else { kssStatus = "Driver Data Invalid"; /* Reset Inputs */ headPoseResults = {}; yawnMetrics = {}; kssCalculator.setPerclos(0); kssCalculator.setHeadPose(1); kssCalculator.setYawnMetrics(false, 0, 0); kssCalculator.setBlinksLastMinute(0); }

//                         // Calculate KSS
//                          double now_seconds = std::chrono::duration<double>(std::chrono::system_clock::now().time_since_epoch()).count(); kssCalculator.setDetectedObjects(driver_detectedObjects_final, now_seconds); auto kssBreakdownResults = kssCalculator.calculateCompositeKSS(); if (kssBreakdownResults.size() > 5 && kssBreakdownResults[5].size() == 2) { try { perclosKSS = std::stoi(kssBreakdownResults[0][1]); blinkKSS = std::stoi(kssBreakdownResults[1][1]); headposeKSS = std::stoi(kssBreakdownResults[2][1]); yawnKSS = std::stoi(kssBreakdownResults[3][1]); objdectdetectionKSS = std::stoi(kssBreakdownResults[4][1]); extractedTotalKSS = std::stoi(kssBreakdownResults[5][1]); } catch (...) { extractedTotalKSS = 1; } } else { extractedTotalKSS = 1; }
//                         std::string alertStatus = kssCalculator.getKSSAlertStatus(extractedTotalKSS);
//                         // Update status only if not already an error, or if there's an alert
//                         if (kssStatus == "Normal" || !alertStatus.empty()) { kssStatus = alertStatus.empty() ? "Normal" : alertStatus; }

//                     } else { // Driver tracked BUT outside fixed ROI
//                          kssStatus = "Driver Outside ROI";
//                          driver_tracked_this_frame = false; current_tracked_driver_idx = -1; extractedTotalKSS = 1; yawnMetrics = {}; headPoseResults = {}; kssCalculator.setPerclos(0.0); kssCalculator.setHeadPose(1); kssCalculator.setYawnMetrics(false, 0, 0); kssCalculator.setBlinksLastMinute(0); double now_seconds = std::chrono::duration<double>(std::chrono::system_clock::now().time_since_epoch()).count(); kssCalculator.setDetectedObjects({}, now_seconds); auto kssBreakdownResults = kssCalculator.calculateCompositeKSS(); if (kssBreakdownResults.size() > 5 && kssBreakdownResults[5].size() == 2) { extractedTotalKSS = std::stoi(kssBreakdownResults[5][1]); } else { extractedTotalKSS = 1;}
//                     }
//                 } else { // Driver NOT tracked in this frame
//                     kssStatus = "Driver Not Tracked";
//                     extractedTotalKSS = 1; yawnMetrics = {}; headPoseResults = {}; kssCalculator.setPerclos(0.0); kssCalculator.setHeadPose(1); kssCalculator.setYawnMetrics(false, 0, 0); kssCalculator.setBlinksLastMinute(0); double now_seconds = std::chrono::duration<double>(std::chrono::system_clock::now().time_since_epoch()).count(); kssCalculator.setDetectedObjects({}, now_seconds); auto kssBreakdownResults = kssCalculator.calculateCompositeKSS(); if (kssBreakdownResults.size() > 5 && kssBreakdownResults[5].size() == 2) { extractedTotalKSS = std::stoi(kssBreakdownResults[5][1]); } else { extractedTotalKSS = 1;}
//                 }
//             } // End if(driver_roi_defined) / else


//             // --- Drawing (Normal Processing Phase - Full Frame Coords) ---
//             // Draw Green box ONLY if driver is tracked AND INSIDE ROI
//             if (driver_tracked_this_frame && current_tracked_driver_idx != -1) {
//                 face_object_t *driver_face = &face_results.faces[current_tracked_driver_idx];
//                 cv::Point driver_center_full_frame( (driver_face->box.left + driver_face->box.right) / 2, (driver_face->box.top + driver_face->box.bottom) / 2 );
//                 if (driver_roi_defined && driver_object_roi.contains(driver_center_full_frame)) {
//                      draw_rectangle(&src_image, driver_face->box.left, driver_face->box.top, driver_face->box.right - driver_face->box.left, driver_face->box.bottom - driver_face->box.top, COLOR_GREEN, 2);
//                      // Draw Landmarks and Eye status ONLY if driver is valid and inside ROI
//                     //  if (driver_face->face_landmarks_valid) {
//                     //       // ---> COMMENT OUT this block to disable landmarks <---
                          
//                     //       for (int j = 0; j < NUM_FACE_LANDMARKS; ++j) { draw_circle(&src_image, driver_face->face_landmarks[j].x, driver_face->face_landmarks[j].y, 1, COLOR_CYAN, 1); }
//                     //       if (driver_face->eye_landmarks_left_valid) { for (int j=0; j<NUM_EYE_CONTOUR_LANDMARKS; ++j) draw_circle(&src_image, driver_face->eye_landmarks_left[j].x, driver_face->eye_landmarks_left[j].y, 1, COLOR_BLUE, 1); }
//                     //       if (driver_face->iris_landmarks_left_valid) { for (int j=0; j<NUM_IRIS_LANDMARKS; ++j) draw_circle(&src_image, driver_face->iris_landmarks_left[j].x, driver_face->iris_landmarks_left[j].y, 1, COLOR_ORANGE, 1); }
//                     //       if (driver_face->eye_landmarks_right_valid) { for (int j=0; j<NUM_EYE_CONTOUR_LANDMARKS; ++j) draw_circle(&src_image, driver_face->eye_landmarks_right[j].x, driver_face->eye_landmarks_right[j].y, 1, COLOR_BLUE, 1); }
//                     //       if (driver_face->iris_landmarks_right_valid) { for (int j=0; j<NUM_IRIS_LANDMARKS; ++j) draw_circle(&src_image, driver_face->iris_landmarks_right[j].x, driver_face->iris_landmarks_right[j].y, 1, COLOR_ORANGE, 1); }
                          

//                     //       const int LEFT_EYE_TEXT_ANCHOR_IDX = 33; const int RIGHT_EYE_TEXT_ANCHOR_IDX = 263;
            
//                     //  }
//                  } 
//                  else { // Driver tracked but outside ROI - draw yellow box
//                       draw_rectangle(&src_image, driver_face->box.left, driver_face->box.top, driver_face->box.right - driver_face->box.left, driver_face->box.bottom - driver_face->box.top, COLOR_YELLOW, 1);
//                  }
//             }

//             // Draw Status Text (Common)
//             if (kssStatus == "Normal") status_color_uint = COLOR_GREEN; else if (extractedTotalKSS <= 7) status_color_uint = COLOR_BLUE; else status_color_uint = COLOR_RED;

//             text_y = 10; // Reset Y position for text

//             // Draw Main Status
//             if (!kssStatus.empty()) {
//                  draw_text(&src_image, kssStatus.c_str(), 10, text_y, status_color_uint, status_text_size);
//                  text_y += (int)(line_height * 1.4); // Extra space after main status
//             }

//             // --- Draw Behavior Metrics ---
//             text_stream.str(""); text_stream << "PERCLOS: " << std::fixed << std::setprecision(2) << blinkDetector.getPerclos() << "%";
//             draw_text(&src_image, text_stream.str().c_str(), 10, text_y, COLOR_WHITE, text_size); text_y += line_height;

//             text_stream.str(""); text_stream << "Blinks (Last Min): " << blinkDetector.getBlinksInWindow();
//             draw_text(&src_image, text_stream.str().c_str(), 10, text_y, COLOR_WHITE, text_size); text_y += line_height;

//             if (headPoseResults.rows.size() >= 3) { std::string headpose_text = "Yaw:" + headPoseResults.rows[0][1] + " Pitch:" + headPoseResults.rows[1][1] + " Roll:" + headPoseResults.rows[2][1]; draw_text(&src_image, headpose_text.c_str(), 10, text_y, COLOR_WHITE, text_size); } else { draw_text(&src_image, "Head Pose: N/A", 10, text_y, COLOR_WHITE, text_size); } text_y += line_height;

//             text_stream.str(""); text_stream << "Yawning: " << (yawnMetrics.isYawning ? "Yes" : "No");
//             draw_text(&src_image, text_stream.str().c_str(), 10, text_y, COLOR_WHITE, text_size); text_y += line_height;

//             text_stream.str(""); text_stream << "Yawn Freq(5m): " << static_cast<int>(yawnMetrics.yawnFrequency_5min);
//             draw_text(&src_image, text_stream.str().c_str(), 10, text_y, COLOR_WHITE, text_size); text_y += line_height;

//             std::string detected_objects_text = ""; if (!detectedObjects.empty()) { detected_objects_text = "Detected: "; for (size_t j = 0; j < detectedObjects.size(); ++j) { detected_objects_text += detectedObjects[j]; if (j < detectedObjects.size() - 1) detected_objects_text += ", "; } draw_text(&src_image, detected_objects_text.c_str(), 10, text_y, COLOR_ORANGE, text_size); text_y += line_height; }

//             // --- Draw KSS Breakdown ---
//             text_stream.str(""); text_stream << "KSS Breakdown: P" << perclosKSS << " H" << headposeKSS << " Y" << yawnKSS << " O" << objdectdetectionKSS; // Removed BlinkCount KSS for consistency
//             draw_text(&src_image, text_stream.str().c_str(), 10, text_y, COLOR_GREEN, text_size); text_y += line_height;

//             text_stream.str(""); text_stream << "Composite KSS: " << extractedTotalKSS;
//             draw_text(&src_image, text_stream.str().c_str(), 10, text_y, status_color_uint, text_size); text_y += line_height;

//             // +++++++++++++++ ADDED: Timers and Counts +++++++++++++++
//             text_y += 5; // Add a small gap

//             // PERCLOS Timer
//             double current_time_sec_perf = std::chrono::duration<double>(std::chrono::system_clock::now().time_since_epoch()).count();
//             double perclos_start = blinkDetector.getPerclosWindowStartTime();
//             double perclos_elapsed = current_time_sec_perf - perclos_start;
//             double perclos_remaining = std::max(0.0, 60.0 - perclos_elapsed);
//             text_stream.str(""); text_stream << "PERCLOS Window: " << std::fixed << std::setprecision(0) << perclos_remaining << "s";
//             draw_text(&src_image, text_stream.str().c_str(), 10, text_y, COLOR_CYAN, text_size); text_y += line_height;

//             // Head Pose Counts (within last 5 min)
//             text_stream.str(""); text_stream << "Head Turns(5m): >=15: " << headPoseTracker.getHeadTurnCount15()
//                                             << " | >=30: " << headPoseTracker.getHeadTurnCount30()
//                                             << " | >=45: " << headPoseTracker.getHeadTurnCount45();
//             draw_text(&src_image, text_stream.str().c_str(), 10, text_y, COLOR_CYAN, text_size); text_y += line_height;

//              // Yawn Counts (within last 5 min)
//             text_stream.str(""); text_stream << "Yawns(5m): >=2s: " << yawnDetector.getYawnCount2s()
//                                             << " | >=3s: " << yawnDetector.getYawnCount3s()
//                                             << " | >=4s: " << yawnDetector.getYawnCount4s();
//             draw_text(&src_image, text_stream.str().c_str(), 10, text_y, COLOR_CYAN, text_size); text_y += line_height;

//             // Object Detection Consecutive Frames & Event Count (Example: Mobile L3 10min)
//             text_stream.str(""); text_stream << "Obj Cons Frames: M:" << kssCalculator.getConsecutiveMobileFrames()
//                                             << " E:" << kssCalculator.getConsecutiveEatDrinkFrames()
//                                             << " S:" << kssCalculator.getConsecutiveSmokeFrames();
//             draw_text(&src_image, text_stream.str().c_str(), 10, text_y, COLOR_CYAN, text_size); text_y += line_height;

//             // Example showing Mobile >=3s events in last 10 min
//             // text_stream.str(""); text_stream << "Mobile Ev(>=3s/10m): " << kssCalculator.getMobileEventsL3_10m(current_time_sec_perf);
//             // draw_text(&src_image, text_stream.str().c_str(), 10, text_y, COLOR_ORANGE, text_size); text_y += line_height;

//             // +++++++++++++++++++++++++++++++++++++++++++++++++++++++++


//         } // End if (!calibration_done) / else

//         // --- Draw ROIs (Optional Debugging - FIXED ROI) ---
//         #ifdef DEBUG_DRAW_ROIS
//         if (driver_roi_defined) { /* Draw Fixed ROI */ draw_rectangle(&src_image, driver_object_roi.x, driver_object_roi.y, driver_object_roi.width, driver_object_roi.height, COLOR_MAGENTA, 1); draw_text(&src_image, "Driver ROI", driver_object_roi.x + 5, driver_object_roi.y + 15, COLOR_MAGENTA, 10); }
//         // Draw non-tracked faces (yellow box) - ALWAYS based on full frame results
//          for (int i = 0; i < face_results.count; ++i) { if (i != current_tracked_driver_idx) { draw_rectangle(&src_image, face_results.faces[i].box.left, face_results.faces[i].box.top, face_results.faces[i].box.right - face_results.faces[i].box.left, face_results.faces[i].box.bottom - face_results.faces[i].box.top, COLOR_YELLOW, 1); } }
//         #endif

//         // --- Draw Resource Monitoring Info ---
//          auto frame_processing_end_time = std::chrono::high_resolution_clock::now(); double frame_duration_ms = std::chrono::duration<double, std::milli>(frame_processing_end_time - frame_start_time).count(); calculateOverallFPS(frame_duration_ms, frame_times, overallFPS, max_time_records); getCPUUsage(currentCpuUsage, prevIdleTime, prevTotalTime); getTemperature(currentTemp); std::stringstream info_ss; info_ss << "FPS:" << std::fixed << std::setprecision(1) << overallFPS << "|CPU:" << currentCpuUsage << "%|T:" << currentTemp << "C"; draw_text(&src_image, info_ss.str().c_str(), 10, src_image.height - 20, COLOR_WHITE, 10);

//         // --- Display Frame ---
//         cv::Mat display_frame(src_image.height, src_image.width, CV_8UC3, map_info.data, src_image.width_stride);
//         cv::imshow("DMS Output", display_frame);

//         // --- Push Frame to Saving Pipeline ---
//          if (pipeline_ && appsrc_ && saving_pipeline_caps_set) { GstClockTime duration = gst_util_uint64_scale_int(1, GST_SECOND, video_info.fps_n > 0 ? video_info.fps_d * video_info.fps_n : 30); pushFrameToPipeline(map_info.data, map_info.size, src_image.width, src_image.height, duration); }

//         // --- Cleanup Frame Resources ---
//         if (fixed_roi_crop_img.virt_addr) { free(fixed_roi_crop_img.virt_addr); fixed_roi_crop_img.virt_addr = nullptr; }
//         gst_buffer_unmap(gst_buffer, &map_info);
//         gst_sample_unref(sample);

//         // --- Handle KeyPress ---
//         if (cv::waitKey(1) == 27) break; // Exit on ESC

//     } // End main loop

//     // --- Final Cleanup ---
//     // ... (Cleanup logic) ...
//     if (fixed_roi_crop_img.virt_addr) { free(fixed_roi_crop_img.virt_addr); } printf("INFO: Cleaning up resources...\n"); cv::destroyAllWindows(); stop_yolo_worker.store(true); if (yolo_worker_thread.joinable()) yolo_worker_thread.join(); printf("INFO: YOLO thread joined.\n"); if (input_pipeline) { gst_element_set_state(input_pipeline, GST_STATE_NULL); gst_object_unref(appsink_); gst_object_unref(input_pipeline); printf("INFO: Input pipeline released.\n"); } if (pipeline_) { gst_element_send_event(pipeline_, gst_event_new_eos()); GstBus* bus = gst_element_get_bus(pipeline_); gst_bus_poll(bus, GST_MESSAGE_EOS, GST_CLOCK_TIME_NONE); gst_object_unref(bus); gst_element_set_state(pipeline_, GST_STATE_NULL); gst_object_unref(appsrc_); gst_object_unref(pipeline_); printf("INFO: Saving pipeline released.\n"); } gst_deinit(); printf("INFO: GStreamer deinitialized.\n"); release_face_analyzer(&app_ctx.face_ctx); release_yolo11(&app_ctx.yolo_ctx); deinit_post_process(); printf("INFO: RKNN models released.\n");

//     printf("Exiting main (ret = %d)\n", ret);
//     return ret;
// }






































//  CALIBRATION REFERENCE FOR EYE+MOUTH


#include <stdint.h>
#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#include <vector>
#include <string>
#include <sstream>
#include <iomanip>
#include <chrono>
#include <stdexcept>
#include <cmath>
#include <opencv2/core/types.hpp> // Needed for cv::Rect, cv::Point
#include <opencv2/opencv.hpp>
#include <thread>
#include <queue>
#include <mutex>
#include <atomic>
#include <memory>
#include <sys/sysinfo.h>
#include <sys/stat.h>
#include <unistd.h>
#include <dirent.h>
#include <cerrno> // For strerror
#include <string_view> // For string_view

// Include project headers
#include "face_analyzer/face_analyzer.h"
#include "yolo_detector/yolo11.h"
#include "behavior_analysis/BlinkDetector.hpp"
#include "behavior_analysis/YawnDetector.hpp"
#include "behavior_analysis/HeadPoseTracker.hpp"
#include "behavior_analysis/KSSCalculator.hpp"
#include "image_utils.h"         // Make sure crop_image_simple is declared here or implement it
#include "file_utils.h"
#include "image_drawing.h"

// GStreamer Includes
#include <gst/gst.h>
#include <gst/video/video.h>
#include <gst/app/gstappsink.h>
#include <gst/allocators/gstdmabuf.h>
#include <gst/app/gstappsrc.h>

// Resource Monitoring Includes
#include <fstream>
#include <numeric>
#include <deque>
#include <algorithm> // For std::sort, std::max

// --- Debugging Flags ---
#define DEBUG_DRAW_ROIS // Comment out to disable drawing ROIs

// --- Define colors ---
#ifndef COLOR_MAGENTA
#define COLOR_MAGENTA (0xFF00FF) // Pinkish/Magenta
#endif
#ifndef COLOR_YELLOW
#define COLOR_YELLOW (0x00FFFF) // Yellow
#endif
#ifndef COLOR_WHITE
#define COLOR_WHITE (0xFFFFFF) // White
#endif
#ifndef COLOR_GREEN
#define COLOR_GREEN (0x00FF00) // Green
#endif
#ifndef COLOR_RED
#define COLOR_RED (0x0000FF) // Red
#endif
#ifndef COLOR_BLUE
#define COLOR_BLUE (0xFF0000) // Blue
#endif
#ifndef COLOR_ORANGE
#define COLOR_ORANGE (0x00A5FF) // Orange
#endif
#ifndef COLOR_CYAN
#define COLOR_CYAN (0xFFFF00) // Cyan
#endif


// --- Application Context ---
typedef struct app_context_t {
    face_analyzer_app_context_t face_ctx;
    yolo11_app_context_t        yolo_ctx;
} app_context_t;

// --- Global GStreamer elements ---
static GstElement* pipeline_ = nullptr; // Saving pipeline
static GstElement* appsrc_ = nullptr;   // Source for saving pipeline
static GstElement* appsink_ = nullptr;  // Sink for input pipeline

// --- Helper Functions ---
cv::Point calculate_centroid(const point_t landmarks[], int count) { if (count == 0) return cv::Point(-1, -1); long long sum_x = 0, sum_y = 0; int valid_points = 0; for (int i = 0; i < count; ++i) { if (landmarks[i].x > -10000 && landmarks[i].x < 10000 && landmarks[i].y > -10000 && landmarks[i].y < 10000) { sum_x += landmarks[i].x; sum_y += landmarks[i].y; valid_points++; } } if (valid_points == 0) return cv::Point(-1, -1); return cv::Point(static_cast<int>(sum_x / valid_points), static_cast<int>(sum_y / valid_points)); }
std::vector<cv::Point> convert_landmarks_to_cvpoint(const point_t landmarks[], int count) { std::vector<cv::Point> cv_landmarks; if (count <= 0) return cv_landmarks; cv_landmarks.reserve(count); for (int i = 0; i < count; ++i) { if (landmarks[i].x > -10000 && landmarks[i].x < 10000 && landmarks[i].y > -10000 && landmarks[i].y < 10000) { cv_landmarks.emplace_back(cv::Point(landmarks[i].x, landmarks[i].y)); } } return cv_landmarks; }
float calculate_ear_simple(const std::vector<cv::Point>& landmarks, const std::vector<int>& eye_points) { if (landmarks.empty()) return 1.0f; int max_idx = 0; for(int idx : eye_points) { if (idx > max_idx) max_idx = idx; } if (max_idx >= landmarks.size()) { return 1.0f; } try { cv::Point p1=landmarks.at(eye_points.at(0)); cv::Point p2=landmarks.at(eye_points.at(1)); cv::Point p3=landmarks.at(eye_points.at(2)); cv::Point p4=landmarks.at(eye_points.at(3)); cv::Point p5=landmarks.at(eye_points.at(4)); cv::Point p6=landmarks.at(eye_points.at(5)); double v1=cv::norm(p2-p6); double v2=cv::norm(p3-p5); double h=cv::norm(p1-p4); if(h<1e-6) return 1.0f; return static_cast<float>((v1+v2)/(2.0*h)); } catch(const std::out_of_range& oor) { std::cerr << "Out of Range error in calculate_ear_simple: " << oor.what() << std::endl; return 1.0f; } catch(...) { std::cerr << "Unknown exception in calculate_ear_simple" << std::endl; return 1.0f; } }
double calculate_mouth_dist_simple(const std::vector<cv::Point>& landmarks) { if (landmarks.empty() || landmarks.size() <= 14) return 0.0; try { return cv::norm(landmarks.at(13)-landmarks.at(14)); } catch(const std::out_of_range& oor) { std::cerr << "Out of Range error in calculate_mouth_dist_simple: " << oor.what() << std::endl; return 0.0; } catch (...) { std::cerr << "Unknown exception in calculate_mouth_dist_simple" << std::endl; return 0.0; } }
double parse_head_pose_value(const std::string& s) { try { std::string n=s; size_t d=n.find(" deg"); if(d!=std::string::npos) n=n.substr(0,d); size_t f=n.find_first_not_of(" \t"); if(f==std::string::npos) return 0.0; size_t l=n.find_last_not_of(" \t"); n=n.substr(f,l-f+1); if(n.empty()) return 0.0; return std::stod(n); } catch (const std::exception& e) { printf("WARN: Ex parse head pose '%s': %s\n", s.c_str(), e.what()); return 0.0; } catch (...) { printf("WARN: Unk ex parse head pose '%s'.\n", s.c_str()); return 0.0; } }
template <typename T> T calculate_stddev(const std::deque<T>& data) { if (data.size() < 2) return T(0); T sum = std::accumulate(data.begin(), data.end(), T(0)); T mean = sum / data.size(); T sq_sum = std::inner_product(data.begin(), data.end(), data.begin(), T(0)); T variance = sq_sum / data.size() - mean * mean; return std::sqrt(std::max(T(0), variance)); }


// --- Simple CPU Cropping Function (Corrected Preprocessor Directives) ---
static int crop_image_simple(image_buffer_t *src_img, image_buffer_t *dst_img, box_rect_t crop_box) {
    if (!src_img || !src_img->virt_addr || !dst_img) {
        printf("ERROR: crop_image_simple null input/output pointer.\n");
        return -1;
    }

    int channels = 0;
    if (src_img->format == IMAGE_FORMAT_RGB888) {
        channels = 3;
    }
// Put preprocessor directives on separate lines
#ifdef IMAGE_FORMAT_BGR888
    else if (src_img->format == IMAGE_FORMAT_BGR888) {
        channels = 3;
    }
#endif
    // Add checks for other formats if needed, otherwise error out
    else if (channels == 0) { // Check if format wasn't handled
        printf("ERROR: crop_image_simple unsupported format %d\n", src_img->format);
        return -1;
    }

    // --- Rest of the function remains the same ---
    int src_w = src_img->width; int src_h = src_img->height; int crop_x = crop_box.left; int crop_y = crop_box.top; int crop_w = crop_box.right - crop_box.left; int crop_h = crop_box.bottom - crop_box.top; if (crop_w <= 0 || crop_h <= 0) { printf("ERROR: crop ROI invalid size (%dx%d) from box [%d,%d,%d,%d]\n", crop_w, crop_h, crop_box.left, crop_box.top, crop_box.right, crop_box.bottom); return -1; } int x_start = std::max(0, crop_x); int y_start = std::max(0, crop_y); int x_end = std::min(src_w, crop_x + crop_w); int y_end = std::min(src_h, crop_y + crop_h); int valid_crop_w = x_end - x_start; int valid_crop_h = y_end - y_start; if (valid_crop_w <= 0 || valid_crop_h <= 0) { printf("ERROR: Clamped crop ROI zero size. Original ROI [%d,%d,%d,%d], Clamped to src %dx%d -> [%d,%d,%d,%d]\n", crop_box.left, crop_box.top, crop_box.right, crop_box.bottom, src_w, src_h, x_start, y_start, x_end, y_end); return -1; } dst_img->width = crop_w; dst_img->height = crop_h; dst_img->format = src_img->format; dst_img->size = crop_w * crop_h * channels; if (dst_img->virt_addr == NULL) { dst_img->virt_addr = (unsigned char*)malloc(dst_img->size); if (!dst_img->virt_addr) { printf("ERROR: Failed alloc memory for crop (%zu bytes)\n", dst_img->size); return -1; } } else if (dst_img->size < (size_t)(crop_w * crop_h * channels)) { printf("ERROR: Dest buffer size %d too small for crop %d.\n", dst_img->size, crop_w * crop_h * channels); return -1; } memset(dst_img->virt_addr, 0, dst_img->size); unsigned char* src_data = src_img->virt_addr; unsigned char* dst_data = dst_img->virt_addr; size_t src_stride = src_img->width_stride > 0 ? src_img->width_stride : (size_t)src_w * channels; size_t dst_stride = (size_t)crop_w * channels; int dst_x_offset = x_start - crop_x; int dst_y_offset = y_start - crop_y; for (int y = 0; y < valid_crop_h; ++y) { unsigned char* src_row_ptr = src_data + (size_t)(y_start + y) * src_stride + (size_t)x_start * channels; unsigned char* dst_row_ptr = dst_data + (size_t)(dst_y_offset + y) * dst_stride + (size_t)dst_x_offset * channels; memcpy(dst_row_ptr, src_row_ptr, (size_t)valid_crop_w * channels); } return 0;
}
// --- End Helper Functions ---


// --- YOLO Worker Thread ---
struct YoloInputData { long frame_id; std::shared_ptr<image_buffer_t> image; }; struct YoloOutputData { long frame_id; object_detect_result_list results; }; std::queue<YoloInputData> yolo_input_queue; std::queue<YoloOutputData> yolo_output_queue; std::mutex yolo_input_mutex; std::mutex yolo_output_mutex; std::atomic<bool> stop_yolo_worker(false); const int MAX_QUEUE_SIZE = 5;
void yolo_worker_thread_func(yolo11_app_context_t* yolo_ctx_ptr) { while (!stop_yolo_worker.load()) { YoloInputData input_data; bool got_data = false; { std::unique_lock<std::mutex> lock(yolo_input_mutex); if (!yolo_input_queue.empty()) { input_data = yolo_input_queue.front(); yolo_input_queue.pop(); got_data = true; } } if (got_data && input_data.image) { YoloOutputData output_data; output_data.frame_id = input_data.frame_id; memset(&output_data.results, 0, sizeof(output_data.results)); int ret = inference_yolo11(yolo_ctx_ptr, input_data.image.get(), &output_data.results); if (ret != 0) printf("WARN: YOLO Worker inference failed (frame %ld), ret=%d\n", input_data.frame_id, ret); { std::unique_lock<std::mutex> lock(yolo_output_mutex); if (yolo_output_queue.size() >= MAX_QUEUE_SIZE) yolo_output_queue.pop(); yolo_output_queue.push(output_data); } if (input_data.image->virt_addr) { free(input_data.image->virt_addr); input_data.image->virt_addr = nullptr; } } else { std::this_thread::sleep_for(std::chrono::milliseconds(5)); } } printf("YOLO Worker Thread Exiting.\n");}


// --- GStreamer Saving Pipeline Setup ---
// void setupPipeline() { /* ... (keep as is) ... */ gst_init(nullptr, nullptr); const std::string dir = "/userdata/test_cpp/dms_gst"; struct stat st = {0}; if (stat(dir.c_str(), &st) == -1) { if (mkdir(dir.c_str(), 0700) == -1) { std::cerr << "Error: Cannot create directory " << dir << ": " << strerror(errno) << std::endl; return; } std::cout << "INFO: Created directory " << dir << std::endl; } else if (access(dir.c_str(), W_OK) != 0) { std::cerr << "Error: Directory " << dir << " is not writable." << std::endl; return; } DIR* directory = opendir(dir.c_str()); if (!directory) { std::cerr << "Failed to open directory " << dir << "\n"; return; } int mkv_count = 0; struct dirent* entry; while ((entry = readdir(directory)) != nullptr) { if (std::string_view(entry->d_name).find(".mkv") != std::string_view::npos) { ++mkv_count; } } closedir(directory); const std::string filepath = dir + "/dms_multi_" + std::to_string(mkv_count + 1) + ".mkv"; const std::string pipeline_str = "appsrc name=source ! queue ! videoconvert ! video/x-raw,format=NV12 ! mpph265enc rc-mode=cbr bps=4000000 gop=30 qp-min=10 qp-max=51 ! h265parse ! matroskamux ! filesink location=" + filepath; std::cout << "Saving Pipeline: " << pipeline_str << "\n"; GError* error = nullptr; pipeline_ = gst_parse_launch(pipeline_str.c_str(), &error); if (!pipeline_ || error) { std::cerr << "Failed to create saving pipeline: " << (error ? error->message : "Unknown error") << "\n"; if (error) { g_error_free(error); } return; } appsrc_ = gst_bin_get_by_name(GST_BIN(pipeline_), "source"); if (!appsrc_) { std::cerr << "Failed to get appsrc\n"; gst_object_unref(pipeline_); pipeline_ = nullptr; return; } g_object_set(G_OBJECT(appsrc_), "stream-type", GST_APP_STREAM_TYPE_STREAM, "format", GST_FORMAT_TIME, "is-live", FALSE, NULL); if (gst_element_set_state(pipeline_, GST_STATE_PLAYING) == GST_STATE_CHANGE_FAILURE) { std::cerr << "Failed to set saving pipeline to playing state\n"; gst_object_unref(appsrc_); gst_object_unref(pipeline_); appsrc_ = nullptr; pipeline_ = nullptr; } else { std::cout << "INFO: Saving pipeline created and set to PLAYING (waiting for caps/data).\n"; } }

// void pushFrameToPipeline(unsigned char* data, int size, int width, int height, GstClockTime duration) { if (!appsrc_) return; GstBuffer* buffer = gst_buffer_new_allocate(nullptr, size, nullptr); GstMapInfo map; if (!gst_buffer_map(buffer, &map, GST_MAP_WRITE)) { std::cerr << "Failed map buffer" << std::endl; gst_buffer_unref(buffer); return; } if (map.size != (guint)size) { std::cerr << "Buffer size mismatch: " << map.size << " vs " << size << std::endl; gst_buffer_unmap(buffer, &map); gst_buffer_unref(buffer); return; } memcpy(map.data, data, size); gst_buffer_unmap(buffer, &map); static GstClockTime timestamp = 0; GST_BUFFER_PTS(buffer) = timestamp; GST_BUFFER_DURATION(buffer) = duration; timestamp += GST_BUFFER_DURATION(buffer); GstFlowReturn ret; g_signal_emit_by_name(appsrc_, "push-buffer", buffer, &ret); if (ret != GST_FLOW_OK) std::cerr << "Failed push buffer, ret=" << ret << std::endl; gst_buffer_unref(buffer); }


void setupPipeline() {
    gst_init(nullptr, nullptr);
    const std::string dir = "/userdata/test_cpp/dms_gst";
    // ... (directory checking/creation logic remains the same) ...
    struct stat st = {0}; if (stat(dir.c_str(), &st) == -1) { if (mkdir(dir.c_str(), 0700) == -1) { std::cerr << "Error: Cannot create directory " << dir << ": " << strerror(errno) << std::endl; return; } std::cout << "INFO: Created directory " << dir << std::endl; } else if (access(dir.c_str(), W_OK) != 0) { std::cerr << "Error: Directory " << dir << " is not writable." << std::endl; return; } DIR* directory = opendir(dir.c_str()); if (!directory) { std::cerr << "Failed to open directory " << dir << "\n"; return; } int mkv_count = 0; struct dirent* entry; while ((entry = readdir(directory)) != nullptr) { if (std::string_view(entry->d_name).find(".mkv") != std::string_view::npos) { ++mkv_count; } } closedir(directory); const std::string filepath = dir + "/dms_multi_" + std::to_string(mkv_count + 1) + ".mkv";

    const std::string pipeline_str = "appsrc name=source ! queue ! videoconvert ! video/x-raw,format=NV12 ! mpph265enc rc-mode=cbr bps=4000000 gop=30 qp-min=10 qp-max=51 ! h265parse ! matroskamux ! filesink location=" + filepath;
    std::cout << "Saving Pipeline: " << pipeline_str << "\n";
    GError* error = nullptr;
    pipeline_ = gst_parse_launch(pipeline_str.c_str(), &error);
    if (!pipeline_ || error) {
        std::cerr << "Failed to create saving pipeline: " << (error ? error->message : "Unknown error") << "\n";
        if (error) { g_error_free(error); }
        return;
    }
    appsrc_ = gst_bin_get_by_name(GST_BIN(pipeline_), "source");
    if (!appsrc_) {
        std::cerr << "Failed to get appsrc\n";
        gst_object_unref(pipeline_);
        pipeline_ = nullptr;
        return;
    }

    // Configure appsrc properties
    g_object_set(G_OBJECT(appsrc_),
                 "stream-type", GST_APP_STREAM_TYPE_STREAM, // Use STREAM for continuous data
                 "format", GST_FORMAT_TIME,
                 "is-live", FALSE,         // Important for filesrc input, allows seeking/rate changes potentially
                 "do-timestamp", TRUE,     // <--- ADD THIS LINE: Let appsrc create PTS
                 NULL);

    // Setting caps will be done dynamically in the main loop now

    if (gst_element_set_state(pipeline_, GST_STATE_PLAYING) == GST_STATE_CHANGE_FAILURE) {
        std::cerr << "Failed to set saving pipeline to playing state\n";
        gst_object_unref(appsrc_);
        gst_object_unref(pipeline_);
        appsrc_ = nullptr;
        pipeline_ = nullptr;
    } else {
        std::cout << "INFO: Saving pipeline created and set to PLAYING (waiting for caps/data).\n";
    }
}

void pushFrameToPipeline(unsigned char* data, int size, int width, int height, GstClockTime duration) { // Keep duration parameter
    if (!appsrc_) return;
    GstBuffer* buffer = gst_buffer_new_allocate(nullptr, size, nullptr);
    GstMapInfo map;
    if (!gst_buffer_map(buffer, &map, GST_MAP_WRITE)) {
        std::cerr << "Failed map buffer" << std::endl;
        gst_buffer_unref(buffer);
        return;
    }
    if (map.size != (guint)size) {
        std::cerr << "Buffer size mismatch: " << map.size << " vs " << size << std::endl;
        gst_buffer_unmap(buffer, &map);
        gst_buffer_unref(buffer);
        return;
    }
    memcpy(map.data, data, size);
    gst_buffer_unmap(buffer, &map);

    // REMOVE Manual PTS setting and static timestamp increment
    // static GstClockTime timestamp = 0;
    // GST_BUFFER_PTS(buffer) = timestamp;
    // timestamp += GST_BUFFER_DURATION(buffer);

    // KEEP Duration setting (useful metadata for encoder/muxer)
    GST_BUFFER_DURATION(buffer) = duration;

    GstFlowReturn ret;
    g_signal_emit_by_name(appsrc_, "push-buffer", buffer, &ret);
    if (ret != GST_FLOW_OK) {
        // Handle potential downstream errors (e.g., queue full, EOS)
        std::cerr << "Failed push buffer, ret=" << gst_flow_get_name(ret) << std::endl;
        if (ret == GST_FLOW_ERROR || ret == GST_FLOW_NOT_LINKED || ret == GST_FLOW_FLUSHING) {
           // Consider stopping the pipeline or handling the error appropriately
        }
    }
    gst_buffer_unref(buffer);
}



// --- Resource Monitoring Functions ---
// ... (Keep resource monitoring functions as is) ...
void calculateOverallFPS(double frame_duration_ms, std::deque<double>& times_deque, double& fps_variable, int max_records) { times_deque.push_back(frame_duration_ms); if (times_deque.size() > max_records) times_deque.pop_front(); if (!times_deque.empty()) { double sum = std::accumulate(times_deque.begin(), times_deque.end(), 0.0); double avg_time_ms = sum / times_deque.size(); fps_variable = (avg_time_ms > 0) ? (1000.0 / avg_time_ms) : 0.0; } else { fps_variable = 0.0; } }
void getCPUUsage(double& cpu_usage_variable, long& prev_idle, long& prev_total) { std::ifstream file("/proc/stat"); if (!file.is_open()) return; std::string line; std::getline(file, line); file.close(); long user, nice, system, idle, iowait, irq, softirq, steal, guest, guest_nice; user = nice = system = idle = iowait = irq = softirq = steal = guest = guest_nice = 0; std::istringstream iss(line); std::string cpu_label; iss >> cpu_label >> user >> nice >> system >> idle >> iowait >> irq >> softirq >> steal >> guest >> guest_nice; long currentIdleTime = idle + iowait; long currentTotalTime = user + nice + system + idle + iowait + irq + softirq + steal; long diffIdle = currentIdleTime - prev_idle; long diffTotal = currentTotalTime - prev_total; if (diffTotal > 0) cpu_usage_variable = 100.0 * (double)(diffTotal - diffIdle) / diffTotal; prev_idle = currentIdleTime; prev_total = currentTotalTime; }
void getTemperature(double& temp_variable) { const char* temp_paths[] = {"/sys/class/thermal/thermal_zone0/temp", "/sys/class/thermal/thermal_zone1/temp"}; bool temp_read = false; for (const char* path : temp_paths) { std::ifstream file(path); double temp_milliC = 0; if (file >> temp_milliC) { temp_variable = temp_milliC / 1000.0; temp_read = true; file.close(); break; } file.close(); } }


/*-------------------------------------------
                  Main Function
-------------------------------------------*/
int main(int argc, char **argv) {

    // ... (Variable declarations: ret, frame_id, monitoring vars, model paths, video source) ...
    int ret = 0; long current_frame_id = 0; std::deque<double> frame_times; const int max_time_records = 60; double overallFPS = 0.0; double currentCpuUsage = 0.0; long prevIdleTime = 0, prevTotalTime = 0; double currentTemp = 0.0;
    const char *detection_model_path = "../../model/rf.rknn"; const char *landmark_model_path = "../../model/faceL.rknn"; const char *iris_model_path = "../../model/faceI.rknn"; const char *yolo_model_path = "../../model/od.rknn";
    const char *video_source = "filesrc location=../../model/gunsan7.mkv ! decodebin ! queue ! videoconvert ! video/x-raw,format=BGR ! appsink name=sink sync=false";
    // const char *video_source = "v4l2src device=/dev/video0 ! queue ! videoconvert ! video/x-raw,format=BGR,width=1920,height=1080,framerate=30/1 ! appsink name=sink sync=false";

    // --- Initialization ---
    setupPipeline(); // Setup saving pipeline (without caps)
    app_context_t app_ctx; memset(&app_ctx, 0, sizeof(app_context_t)); image_buffer_t src_image; memset(&src_image, 0, sizeof(image_buffer_t)); face_analyzer_result_t face_results; memset(&face_results, 0, sizeof(face_results)); object_detect_result_list yolo_results; memset(&yolo_results, 0, sizeof(yolo_results)); my::BlinkDetector blinkDetector; YawnDetector yawnDetector; my::HeadPoseTracker headPoseTracker; KSSCalculator kssCalculator;

    // --- Calibration State Variables ---
     bool calibration_done = false; std::chrono::steady_clock::time_point calibration_start_time; bool calibration_timer_started = false; int consecutive_valid_eyes_frames = 0; const int REQUIRED_VALID_EYES_FRAMES = 60; bool ear_calibrated = false; bool mouth_calibrated = false; std::deque<float> calib_left_ears; std::deque<float> calib_right_ears; std::deque<double> calib_mouth_dists; int consecutive_stable_ear_frames = 0; int consecutive_stable_mouth_frames = 0; const int CALIB_WINDOW_SIZE = 30; const float EAR_STDDEV_THRESHOLD = 0.04; const double MOUTH_DIST_STDDEV_THRESHOLD = 15.0; const int REQUIRED_STABLE_FRAMES = CALIB_WINDOW_SIZE + 5; const double CALIBRATION_TIMEOUT_SECONDS = 10.0;

    // --- Driver ID & Tracking State ---
     bool driver_identified_ever = false; bool driver_tracked_this_frame = false; int current_tracked_driver_idx = -1; cv::Point prev_driver_centroid = cv::Point(-1, -1); const double MAX_CENTROID_DISTANCE = 150.0; int driver_search_timeout_frames = 0; const int DRIVER_SEARCH_MAX_FRAMES = 60;

    // --- Fixed Driver Object ROI ---
     cv::Rect driver_object_roi; bool driver_roi_defined = false; bool valid_object_roi = false;

    // --- State variables for KSS and display ---
     std::string kssStatus = "Initializing"; YawnDetector::YawnMetrics yawnMetrics = {}; my::HeadPoseTracker::HeadPoseResults headPoseResults = {}; std::vector<std::string> detectedObjects; int extractedTotalKSS = 1; int perclosKSS = 1, blinkKSS = 1, headposeKSS = 1, yawnKSS = 1, objdectdetectionKSS = 1; std::stringstream text_stream; int text_y = 0; const int line_height = 22; const int text_size = 12; const int status_text_size = 16; unsigned int status_color_uint = COLOR_GREEN;

    // --- YOLO thread handle ---
    std::thread yolo_worker_thread;

    // --- Init models ---
     ret = init_post_process(); if (ret != 0) { /*...*/ return -1; } ret = init_face_analyzer(detection_model_path, landmark_model_path, iris_model_path, &app_ctx.face_ctx); if (ret != 0) { /*...*/ return -1; } ret = init_yolo11(yolo_model_path, &app_ctx.yolo_ctx); if (ret != 0) { /*...*/ return -1; } yolo_worker_thread = std::thread(yolo_worker_thread_func, &app_ctx.yolo_ctx);

    // --- Init GStreamer input ---
     gst_init(nullptr, nullptr); GError* error = nullptr; GstElement* input_pipeline = gst_parse_launch(video_source, &error); if (!input_pipeline || error) { /*...*/ return -1; } appsink_ = gst_bin_get_by_name(GST_BIN(input_pipeline), "sink"); if (!appsink_) { /*...*/ return -1; } if (gst_element_set_state(input_pipeline, GST_STATE_PLAYING) == GST_STATE_CHANGE_FAILURE) { /*...*/ return -1; }

    // --- Setup OpenCV window ---
    cv::namedWindow("DMS Output", cv::WINDOW_NORMAL); cv::resizeWindow("DMS Output", 1920, 1080);

    GstSample* sample; GstVideoInfo video_info; GstBuffer* gst_buffer; GstMapInfo map_info;
    // bool first_frame = true; // Removed, using saving_pipeline_caps_set instead
    bool saving_pipeline_caps_set = false;

    // --- Buffer for the FIXED ROI Crop ---
    image_buffer_t fixed_roi_crop_img;
    memset(&fixed_roi_crop_img, 0, sizeof(image_buffer_t));


    // --- Main Processing Loop ---
    while (true) {
        // --- Get Frame ---
         sample = gst_app_sink_pull_sample(GST_APP_SINK(appsink_)); if (!sample) { std::cerr << "End of stream or error pulling sample." << std::endl; break; } gst_buffer = gst_sample_get_buffer(sample); GstCaps* caps = gst_sample_get_caps(sample); if (!gst_buffer || !caps || !gst_video_info_from_caps(&video_info, caps) || !gst_buffer_map(gst_buffer, &map_info, GST_MAP_READ)) { std::cerr << "Failed to get valid buffer/caps/map" << std::endl; if (gst_buffer && map_info.memory) gst_buffer_unmap(gst_buffer, &map_info); if (sample) gst_sample_unref(sample); continue; }

        auto frame_start_time = std::chrono::high_resolution_clock::now();
        current_frame_id++;

        // --- Wrap GStreamer Data for the *full source image* ---
        src_image.width = GST_VIDEO_INFO_WIDTH(&video_info);
        src_image.height = GST_VIDEO_INFO_HEIGHT(&video_info);
        src_image.width_stride = GST_VIDEO_INFO_PLANE_STRIDE(&video_info, 0);
        src_image.height_stride = src_image.height;
        // Determine format based on GStreamer info if possible, otherwise default
        GstVideoFormat gst_format = GST_VIDEO_INFO_FORMAT(&video_info);
        #ifdef IMAGE_FORMAT_BGR888
            src_image.format = IMAGE_FORMAT_BGR888;
        #else
            src_image.format = IMAGE_FORMAT_RGB888;
        #endif
        src_image.virt_addr = map_info.data;
        src_image.size = map_info.size;
        src_image.fd = -1;


        // --- Set Saving Pipeline Caps (on first valid frame) ---
        // if (pipeline_ && appsrc_ && !saving_pipeline_caps_set) {
        //      GstCaps* save_caps = gst_caps_new_simple("video/x-raw",
        //                                      "format", G_TYPE_STRING, "BGR", // Input to saving pipeline is BGR
        //                                      "width", G_TYPE_INT, src_image.width,
        //                                      "height", G_TYPE_INT, src_image.height,
        //                                      "framerate", GST_TYPE_FRACTION, GST_VIDEO_INFO_FPS_N(&video_info), GST_VIDEO_INFO_FPS_D(&video_info),
        //                                      NULL);
        //      if (save_caps) {
        //           printf("INFO: Setting saving pipeline caps to: %s\n", gst_caps_to_string(save_caps));
        //           g_object_set(G_OBJECT(appsrc_), "caps", save_caps, NULL);
        //           gst_caps_unref(save_caps);
        //           saving_pipeline_caps_set = true;
        //      } else {
        //           std::cerr << "ERROR: Failed to create caps for saving pipeline!\n";
        //      }
        // }

        if (pipeline_ && appsrc_ && !saving_pipeline_caps_set && video_info.fps_n > 0) { // Check if framerate is valid
            GstCaps* save_caps = gst_caps_new_simple("video/x-raw",
                                            "format", G_TYPE_STRING, "BGR", // Input to saving pipeline is BGR
                                            "width", G_TYPE_INT, src_image.width,
                                            "height", G_TYPE_INT, src_image.height,
                                            "framerate", GST_TYPE_FRACTION, video_info.fps_n, video_info.fps_d, // USE ACTUAL FRAMERATE FROM INPUT
                                            NULL);
            if (save_caps) {
                 printf("INFO: Setting saving pipeline caps to: %s\n", gst_caps_to_string(save_caps));
                 g_object_set(G_OBJECT(appsrc_), "caps", save_caps, NULL);
                 gst_caps_unref(save_caps);
                 saving_pipeline_caps_set = true; // Set caps only once
            } else {
                 std::cerr << "ERROR: Failed to create caps for saving pipeline!\n";
            }
        }
        // --- End Set Saving Caps ---


        // --- Run Face Analysis on the FULL FRAME ---
        ret = inference_face_analyzer(&app_ctx.face_ctx, &src_image, &face_results);
        if (ret != 0) { printf("WARN: Face Analyzer Inference failed frame %ld, ret=%d\n", current_frame_id, ret); face_results.count = 0; }

        // --- Reset frame-specific flags ---
        driver_tracked_this_frame = false;
        current_tracked_driver_idx = -1;
        valid_object_roi = false; // Reset each frame

        // --- Driver Identification and Tracking (Based on Full Frame Results) ---
        if (!driver_identified_ever) { /* Identification */ int best_candidate_idx = -1; float max_area = 0.0f; cv::Rect temp_id_zone((int)(src_image.width * 0.40),(int)(src_image.height * 0.1),(int)(src_image.width * 0.55),(int)(src_image.height * 0.8)); temp_id_zone &= cv::Rect(0, 0, src_image.width, src_image.height); for (int i = 0; i < face_results.count; ++i) { cv::Point face_center((face_results.faces[i].box.left + face_results.faces[i].box.right) / 2, (face_results.faces[i].box.top + face_results.faces[i].box.bottom) / 2); if (temp_id_zone.contains(face_center) && face_results.faces[i].face_landmarks_valid) { float area = (float)(face_results.faces[i].box.right - face_results.faces[i].box.left) * (face_results.faces[i].box.bottom - face_results.faces[i].box.top); if (area > max_area) { max_area = area; best_candidate_idx = i; } } } if (best_candidate_idx != -1) { driver_identified_ever = true; driver_tracked_this_frame = true; current_tracked_driver_idx = best_candidate_idx; prev_driver_centroid = calculate_centroid(face_results.faces[best_candidate_idx].face_landmarks, NUM_FACE_LANDMARKS); driver_search_timeout_frames = 0; printf("INFO: Driver Identified (Index %d) at frame %ld\n", current_tracked_driver_idx, current_frame_id); kssStatus = "Calibrating..."; } else { kssStatus = "Searching Driver..."; } } else { /* Tracking */ int best_match_idx = -1; double min_dist = MAX_CENTROID_DISTANCE; for (int i = 0; i < face_results.count; ++i) { if (face_results.faces[i].face_landmarks_valid) { cv::Point current_centroid = calculate_centroid(face_results.faces[i].face_landmarks, NUM_FACE_LANDMARKS); if (prev_driver_centroid.x >= 0 && current_centroid.x >= 0) { double dist = cv::norm(current_centroid - prev_driver_centroid); if (dist < min_dist) { min_dist = dist; best_match_idx = i; } } } } if (best_match_idx != -1) { driver_tracked_this_frame = true; current_tracked_driver_idx = best_match_idx; prev_driver_centroid = calculate_centroid(face_results.faces[best_match_idx].face_landmarks, NUM_FACE_LANDMARKS); driver_search_timeout_frames = 0; if (!calibration_done) kssStatus = "Calibrating..."; } else { driver_tracked_this_frame = false; current_tracked_driver_idx = -1; prev_driver_centroid = cv::Point(-1, -1); driver_search_timeout_frames++; kssStatus = "Driver Lost..."; if (driver_search_timeout_frames > DRIVER_SEARCH_MAX_FRAMES) { driver_identified_ever = false; driver_search_timeout_frames = 0; printf("INFO: Driver track lost for %d frames. Reverting to search.\n", DRIVER_SEARCH_MAX_FRAMES); kssStatus = "Searching Driver..."; } } }

        // --- Calibration or Normal Processing ---
        if (!calibration_done) {
            // --- Calibration Logic ---
             bool head_pose_calibrated = false; bool eyes_consistently_valid = false; double elapsed_calib_seconds = 0.0; std::string calib_status_detail = "";
            // kssStatus = "Initializing"; // Status is set based on tracking above

             if (driver_tracked_this_frame && current_tracked_driver_idx != -1) {
                 face_object_t *calib_face = &face_results.faces[current_tracked_driver_idx];
                 if (calib_face->face_landmarks_valid) {
                     // ... (Start timer logic, run calibration checks) ...
                     if (!calibration_timer_started) { /* Start timer, reset state */ calibration_start_time = std::chrono::steady_clock::now(); calibration_timer_started = true; printf("INFO: Calibration timer started (Driver Index %d).\n", current_tracked_driver_idx); consecutive_valid_eyes_frames = 0; consecutive_stable_ear_frames = 0; consecutive_stable_mouth_frames = 0; calib_left_ears.clear(); calib_right_ears.clear(); calib_mouth_dists.clear(); ear_calibrated = false; mouth_calibrated = false; } std::vector<cv::Point> calib_faceLandmarks = convert_landmarks_to_cvpoint(calib_face->face_landmarks, NUM_FACE_LANDMARKS); if (!calib_faceLandmarks.empty()) { headPoseTracker.run(calib_faceLandmarks); head_pose_calibrated = headPoseTracker.isCalibrated(); if (!head_pose_calibrated) calib_status_detail += " (Head)"; bool eyes_valid_this_frame = calib_face->eye_landmarks_left_valid && calib_face->eye_landmarks_right_valid; if (eyes_valid_this_frame) consecutive_valid_eyes_frames++; else consecutive_valid_eyes_frames = 0; eyes_consistently_valid = (consecutive_valid_eyes_frames >= REQUIRED_VALID_EYES_FRAMES); if (!eyes_consistently_valid) calib_status_detail += " (Eyes Valid)"; if (!ear_calibrated && eyes_valid_this_frame) { const std::vector<int> L={33,160,158,133,153,144}, R={362,385,387,263,380,373}; float lE=calculate_ear_simple(calib_faceLandmarks,L), rE=calculate_ear_simple(calib_faceLandmarks,R); calib_left_ears.push_back(lE); calib_right_ears.push_back(rE); if (calib_left_ears.size()>CALIB_WINDOW_SIZE) calib_left_ears.pop_front(); if (calib_right_ears.size()>CALIB_WINDOW_SIZE) calib_right_ears.pop_front(); if (calib_left_ears.size()>=CALIB_WINDOW_SIZE) { float lS=calculate_stddev(calib_left_ears), rS=calculate_stddev(calib_right_ears); if (lS<EAR_STDDEV_THRESHOLD && rS<EAR_STDDEV_THRESHOLD) consecutive_stable_ear_frames++; else consecutive_stable_ear_frames=0; } else consecutive_stable_ear_frames=0; ear_calibrated=(consecutive_stable_ear_frames>=REQUIRED_STABLE_FRAMES); } else if(!eyes_valid_this_frame){ consecutive_stable_ear_frames=0; ear_calibrated=false; calib_left_ears.clear(); calib_right_ears.clear(); } if(!ear_calibrated) calib_status_detail+=" (EAR Stable)"; if (!mouth_calibrated) { double cM=calculate_mouth_dist_simple(calib_faceLandmarks); calib_mouth_dists.push_back(cM); if(calib_mouth_dists.size()>CALIB_WINDOW_SIZE) calib_mouth_dists.pop_front(); if(calib_mouth_dists.size()>=CALIB_WINDOW_SIZE){ double mS=calculate_stddev(calib_mouth_dists); if(mS<MOUTH_DIST_STDDEV_THRESHOLD) consecutive_stable_mouth_frames++; else consecutive_stable_mouth_frames=0; } else consecutive_stable_mouth_frames=0; mouth_calibrated=(consecutive_stable_mouth_frames>=REQUIRED_STABLE_FRAMES); } if(!mouth_calibrated) calib_status_detail+=" (Mouth Stable)"; } else { calib_status_detail += " (Lmk Convert Err)"; calibration_timer_started = false; } auto now = std::chrono::steady_clock::now(); elapsed_calib_seconds = calibration_timer_started ? std::chrono::duration<double>(now - calibration_start_time).count() : 0.0;

                     // Check for completion or timeout
                    if (calibration_timer_started && elapsed_calib_seconds >= CALIBRATION_TIMEOUT_SECONDS) {
                         if (head_pose_calibrated && eyes_consistently_valid && ear_calibrated && mouth_calibrated) {
                             calibration_done = true; printf("INFO: Calibration Complete!\n");
                             // Define Fixed ROI
                             face_object_t *calib_driver_face = &face_results.faces[current_tracked_driver_idx]; box_rect_t calib_driver_box = calib_driver_face->box; int calib_box_w = calib_driver_box.right - calib_driver_box.left; int calib_box_h = calib_driver_box.bottom - calib_driver_box.top; if(calib_box_w > 0 && calib_box_h > 0) { int box_center_x = (calib_driver_box.left + calib_driver_box.right) / 2; int box_center_y = (calib_driver_box.top + calib_driver_box.bottom) / 2; const float width_expansion = 2.2f; const float height_expansion_up = 0.8f; const float height_expansion_down = 3.0f; int roi_width = static_cast<int>(calib_box_w * width_expansion); int roi_height = static_cast<int>(calib_box_h * (height_expansion_up + height_expansion_down)); int roi_size = std::max(roi_width, roi_height); roi_width = roi_size; roi_height = roi_size; if (roi_width < 640) {roi_width = 640; roi_height = 640;} int roi_x = box_center_x - roi_width / 2; int roi_y = box_center_y - static_cast<int>(calib_box_h * height_expansion_up); driver_object_roi.x = std::max(0, roi_x); driver_object_roi.y = std::max(0, roi_y); driver_object_roi.width = std::min(roi_width, src_image.width - driver_object_roi.x); driver_object_roi.height = std::min(roi_height, src_image.height - driver_object_roi.y); driver_roi_defined = (driver_object_roi.width > 0 && driver_object_roi.height > 0); printf("INFO: Fixed Driver Object ROI defined: [%d, %d, %d x %d]\n", driver_object_roi.x, driver_object_roi.y, driver_object_roi.width, driver_object_roi.height); } else { driver_roi_defined = false; printf("WARN: Could not define fixed ROI due to invalid calibration box.\n"); }
                         } else { // Timeout Failed
                             calibration_timer_started = false; driver_identified_ever = false; driver_tracked_this_frame = false; current_tracked_driver_idx = -1; prev_driver_centroid = cv::Point(-1, -1); consecutive_valid_eyes_frames = 0; consecutive_stable_ear_frames = 0; consecutive_stable_mouth_frames = 0; ear_calibrated = false; mouth_calibrated = false; calib_left_ears.clear(); calib_right_ears.clear(); calib_mouth_dists.clear(); printf("WARN: Calibration time expired, criteria not met (H:%d, EV:%d, ES:%d, MS:%d). Retrying identification.\n", head_pose_calibrated, eyes_consistently_valid, ear_calibrated, mouth_calibrated); kssStatus = "Searching Driver...";
                         }
                     }
                 } else { calibration_timer_started = false; printf("WARN: Tracked driver landmarks not valid during calibration.\n"); kssStatus = "Calibration: Landmark Invalid"; } // Update status

                 // Update overall calibration status text (only if timer started)
                 if (calibration_timer_started && !calibration_done) {
                     kssStatus = "Calibrating... " + std::to_string(static_cast<int>(elapsed_calib_seconds)) + "s" + calib_status_detail;
                 }

                 // Draw yellow box around the face being calibrated
                 if (driver_tracked_this_frame && current_tracked_driver_idx != -1) {
                      draw_rectangle(&src_image, face_results.faces[current_tracked_driver_idx].box.left, face_results.faces[current_tracked_driver_idx].box.top, face_results.faces[current_tracked_driver_idx].box.right - face_results.faces[current_tracked_driver_idx].box.left, face_results.faces[current_tracked_driver_idx].box.bottom - face_results.faces[current_tracked_driver_idx].box.top, COLOR_YELLOW, 2);
                      // +++++++++++++++ DRAW CALIBRATION POINTS +++++++++++++++
                      if (calib_face->face_landmarks_valid) {
                           const int calib_indices[] = { 33, 263, 10, 13, 1 }; // Using LandmarkIndex enum values directly if available
                           for (int idx : calib_indices) {
                                if (idx < NUM_FACE_LANDMARKS) {
                                    draw_circle(&src_image, calib_face->face_landmarks[idx].x, calib_face->face_landmarks[idx].y, 3, COLOR_RED, -1);
                                }
                           }
                      }
                      // +++++++++++++++++++++++++++++++++++++++++++++++++++++++
                 }
             } else { // No driver tracked during calibration phase
                 // ... (Reset calibration state as before) ...
                 calibration_timer_started = false; consecutive_valid_eyes_frames = 0; consecutive_stable_ear_frames = 0; consecutive_stable_mouth_frames = 0; ear_calibrated = false; mouth_calibrated = false; calib_left_ears.clear(); calib_right_ears.clear(); calib_mouth_dists.clear();
                 if (driver_identified_ever) { kssStatus = "Calibration: Waiting for Driver Track..."; } else { kssStatus = "Calibration: Searching Driver..."; }
                 // Draw boxes for all detected faces during search
                 for (int i = 0; i < face_results.count; ++i) { draw_rectangle(&src_image, face_results.faces[i].box.left, face_results.faces[i].box.top, face_results.faces[i].box.right - face_results.faces[i].box.left, face_results.faces[i].box.bottom - face_results.faces[i].box.top, COLOR_YELLOW, 1); }
             }
             // Draw the overall calibration status text determined above
             draw_text(&src_image, kssStatus.c_str(), 10, 10, COLOR_YELLOW, status_text_size);

        } else { // --- Normal Processing Phase (Calibration is Done) ---
            // ... (Rest of normal processing logic, including drawing) ...
            kssStatus = "Normal"; // Default assumes normal unless KSS says otherwise or driver lost
            if (!driver_roi_defined) { kssStatus = "ROI Error"; /* Reset KSS */ printf("ERROR: Driver Object ROI was not defined. Skipping processing.\n"); extractedTotalKSS = 1; perclosKSS = 1; blinkKSS = 1; headposeKSS = 1; yawnKSS = 1; objdectdetectionKSS = 1; yawnMetrics = {}; headPoseResults = {}; detectedObjects.clear(); } else { /* Crop and Queue for YOLO */ if (fixed_roi_crop_img.virt_addr) { free(fixed_roi_crop_img.virt_addr); fixed_roi_crop_img.virt_addr = nullptr; } memset(&fixed_roi_crop_img, 0, sizeof(image_buffer_t)); box_rect_t fixed_roi_as_box = { driver_object_roi.x, driver_object_roi.y, driver_object_roi.x + driver_object_roi.width, driver_object_roi.y + driver_object_roi.height}; ret = crop_image_simple(&src_image, &fixed_roi_crop_img, fixed_roi_as_box); if (ret == 0 && fixed_roi_crop_img.virt_addr) { auto yolo_input_image = std::make_shared<image_buffer_t>(); /* copy data */ yolo_input_image->width = fixed_roi_crop_img.width; yolo_input_image->height = fixed_roi_crop_img.height; yolo_input_image->format = fixed_roi_crop_img.format; yolo_input_image->size = fixed_roi_crop_img.size; yolo_input_image->virt_addr = (unsigned char*)malloc(yolo_input_image->size); if (yolo_input_image->virt_addr) { memcpy(yolo_input_image->virt_addr, fixed_roi_crop_img.virt_addr, yolo_input_image->size); yolo_input_image->width_stride = 0; yolo_input_image->height_stride = 0; yolo_input_image->fd = -1; { std::unique_lock<std::mutex> lock(yolo_input_mutex); if (yolo_input_queue.size() < MAX_QUEUE_SIZE) yolo_input_queue.push({current_frame_id, yolo_input_image}); else { free(yolo_input_image->virt_addr); yolo_input_image->virt_addr = nullptr; } } } else { printf("ERROR: Failed alloc YOLO input copy (fixed roi crop).\n"); } } else { printf("ERROR: Failed to crop to FIXED ROI for YOLO frame %ld.\n", current_frame_id); } bool yolo_result_received = false; { std::unique_lock<std::mutex> lock(yolo_output_mutex); if (!yolo_output_queue.empty()) { YoloOutputData data = yolo_output_queue.front(); yolo_output_queue.pop(); yolo_results = data.results; yolo_result_received = true; } } detectedObjects.clear(); std::vector<std::string> driver_detectedObjects_final; valid_object_roi = driver_roi_defined; if (driver_tracked_this_frame && current_tracked_driver_idx != -1) { face_object_t *driver_face = &face_results.faces[current_tracked_driver_idx]; cv::Point driver_center_full_frame( (driver_face->box.left + driver_face->box.right) / 2, (driver_face->box.top + driver_face->box.bottom) / 2 ); if (driver_object_roi.contains(driver_center_full_frame)) { /* Filter YOLO Results */ if (yolo_results.count > 0) { for (int j = 0; j < yolo_results.count; ++j) { object_detect_result* det = &yolo_results.results[j]; if (det->prop > 0.4) { driver_detectedObjects_final.push_back(coco_cls_to_name(det->cls_id)); } } } detectedObjects = driver_detectedObjects_final; /* Run Behavior Modules */ if (driver_face->face_landmarks_valid) { std::vector<cv::Point> faceLandmarksCv = convert_landmarks_to_cvpoint(driver_face->face_landmarks, NUM_FACE_LANDMARKS); if (!faceLandmarksCv.empty()) { headPoseResults = headPoseTracker.run(faceLandmarksCv); blinkDetector.run(faceLandmarksCv, src_image.width, src_image.height); yawnMetrics = yawnDetector.run(faceLandmarksCv, src_image.width, src_image.height); kssCalculator.setPerclos(blinkDetector.getPerclos()); int headPoseKSSValue = 1; if (headPoseResults.rows.size() >= 4) { for (const auto& row : headPoseResults.rows) { if (row.size() >= 2 && row[0] == "Head KSS") { try { headPoseKSSValue = std::stoi(row[1]); } catch (...) { headPoseKSSValue = 1; } break; } } } kssCalculator.setHeadPose(headPoseKSSValue); kssCalculator.setYawnMetrics(yawnMetrics.isYawning, yawnMetrics.yawnFrequency_5min, yawnMetrics.yawnDuration); kssCalculator.setBlinksLastMinute(blinkDetector.getBlinksInWindow()); } else { kssStatus = "Landmark Error"; /* Reset Inputs */ headPoseResults = {}; yawnMetrics = {}; kssCalculator.setPerclos(0); kssCalculator.setHeadPose(1); kssCalculator.setYawnMetrics(false, 0, 0); kssCalculator.setBlinksLastMinute(0); } } else { kssStatus = "Driver Data Invalid"; /* Reset Inputs */ headPoseResults = {}; yawnMetrics = {}; kssCalculator.setPerclos(0); kssCalculator.setHeadPose(1); kssCalculator.setYawnMetrics(false, 0, 0); kssCalculator.setBlinksLastMinute(0); } /* Calculate KSS */ double now_seconds = std::chrono::duration<double>(std::chrono::system_clock::now().time_since_epoch()).count(); kssCalculator.setDetectedObjects(driver_detectedObjects_final, now_seconds); auto kssBreakdownResults = kssCalculator.calculateCompositeKSS(); if (kssBreakdownResults.size() > 5 && kssBreakdownResults[5].size() == 2) { try { perclosKSS = std::stoi(kssBreakdownResults[0][1]); blinkKSS = std::stoi(kssBreakdownResults[1][1]); headposeKSS = std::stoi(kssBreakdownResults[2][1]); yawnKSS = std::stoi(kssBreakdownResults[3][1]); objdectdetectionKSS = std::stoi(kssBreakdownResults[4][1]); extractedTotalKSS = std::stoi(kssBreakdownResults[5][1]); } catch (...) { extractedTotalKSS = 1; } } else { extractedTotalKSS = 1; } std::string alertStatus = kssCalculator.getKSSAlertStatus(extractedTotalKSS); if (kssStatus == "Normal" || !alertStatus.empty()) { kssStatus = alertStatus.empty() ? "Normal" : alertStatus; } } else { kssStatus = "Driver Outside ROI"; /* Reset state */ driver_tracked_this_frame = false; current_tracked_driver_idx = -1; extractedTotalKSS = 1; yawnMetrics = {}; headPoseResults = {}; kssCalculator.setPerclos(0.0); kssCalculator.setHeadPose(1); kssCalculator.setYawnMetrics(false, 0, 0); kssCalculator.setBlinksLastMinute(0); double now_seconds = std::chrono::duration<double>(std::chrono::system_clock::now().time_since_epoch()).count(); kssCalculator.setDetectedObjects({}, now_seconds); auto kssBreakdownResults = kssCalculator.calculateCompositeKSS(); if (kssBreakdownResults.size() > 5 && kssBreakdownResults[5].size() == 2) { extractedTotalKSS = std::stoi(kssBreakdownResults[5][1]); } else { extractedTotalKSS = 1;} } } else { kssStatus = "Driver Not Tracked"; /* Reset KSS inputs */ extractedTotalKSS = 1; yawnMetrics = {}; headPoseResults = {}; kssCalculator.setPerclos(0.0); kssCalculator.setHeadPose(1); kssCalculator.setYawnMetrics(false, 0, 0); kssCalculator.setBlinksLastMinute(0); double now_seconds = std::chrono::duration<double>(std::chrono::system_clock::now().time_since_epoch()).count(); kssCalculator.setDetectedObjects({}, now_seconds); auto kssBreakdownResults = kssCalculator.calculateCompositeKSS(); if (kssBreakdownResults.size() > 5 && kssBreakdownResults[5].size() == 2) { extractedTotalKSS = std::stoi(kssBreakdownResults[5][1]); } else { extractedTotalKSS = 1;} } }

             // --- Drawing (Normal Processing Phase) ---
             // ... (Draw driver box/landmarks/eye status if tracked & IN ROI) ...
             if (driver_tracked_this_frame && current_tracked_driver_idx != -1) { face_object_t *driver_face = &face_results.faces[current_tracked_driver_idx]; cv::Point driver_center_full_frame( (driver_face->box.left + driver_face->box.right) / 2, (driver_face->box.top + driver_face->box.bottom) / 2 ); if (driver_roi_defined && driver_object_roi.contains(driver_center_full_frame)) { draw_rectangle(&src_image, driver_face->box.left, driver_face->box.top, driver_face->box.right - driver_face->box.left, driver_face->box.bottom - driver_face->box.top, COLOR_GREEN, 2); if (driver_face->face_landmarks_valid) { /* Landmark drawing commented out */ const int LEFT_EYE_TEXT_ANCHOR_IDX = 33; const int RIGHT_EYE_TEXT_ANCHOR_IDX = 263; if (LEFT_EYE_TEXT_ANCHOR_IDX < NUM_FACE_LANDMARKS) { point_t la=driver_face->face_landmarks[LEFT_EYE_TEXT_ANCHOR_IDX]; std::string ls=blinkDetector.isLeftEyeClosed()?"CLOSED":"OPEN"; unsigned int lc=blinkDetector.isLeftEyeClosed()?COLOR_RED:COLOR_GREEN; draw_text(&src_image, ls.c_str(), la.x - 30, la.y - 25, lc, 14); text_stream.str(""); text_stream << "L:" << std::fixed << std::setprecision(2) << blinkDetector.getLeftEARValue(); draw_text(&src_image, text_stream.str().c_str(), la.x - 30, la.y - 10, COLOR_WHITE, 10); } if (RIGHT_EYE_TEXT_ANCHOR_IDX < NUM_FACE_LANDMARKS) { point_t ra=driver_face->face_landmarks[RIGHT_EYE_TEXT_ANCHOR_IDX]; std::string rs=blinkDetector.isRightEyeClosed()?"CLOSED":"OPEN"; unsigned int rc=blinkDetector.isRightEyeClosed()?COLOR_RED:COLOR_GREEN; draw_text(&src_image, rs.c_str(), ra.x - 30, ra.y - 25, rc, 14); text_stream.str(""); text_stream << "R:" << std::fixed << std::setprecision(2) << blinkDetector.getRightEARValue(); draw_text(&src_image, text_stream.str().c_str(), ra.x - 30, ra.y - 10, COLOR_WHITE, 10); } } } else { draw_rectangle(&src_image, driver_face->box.left, driver_face->box.top, driver_face->box.right - driver_face->box.left, driver_face->box.bottom - driver_face->box.top, COLOR_YELLOW, 1); } }
             // ... (Draw status text, KSS breakdown, timers, counts etc.) ...
             if (kssStatus == "Normal") status_color_uint = COLOR_GREEN; else if (extractedTotalKSS <= 7) status_color_uint = COLOR_BLUE; else status_color_uint = COLOR_RED; text_y = 10; if (!kssStatus.empty() && kssStatus.find("Calibrating...") == std::string::npos) { draw_text(&src_image, kssStatus.c_str(), 10, text_y, status_color_uint, status_text_size); text_y += (int)(line_height * 1.4); } else { text_y += (int)(line_height * 1.4); } /* Rest of text drawing */ text_stream.str(""); text_stream << "PERCLOS: " << std::fixed << std::setprecision(2) << blinkDetector.getPerclos() << "%"; draw_text(&src_image, text_stream.str().c_str(), 10, text_y, COLOR_WHITE, text_size); text_y += line_height; text_stream.str(""); text_stream << "Blinks (Last Min): " << blinkDetector.getBlinksInWindow(); draw_text(&src_image, text_stream.str().c_str(), 10, text_y, COLOR_WHITE, text_size); text_y += line_height; if (headPoseResults.rows.size() >= 3) { std::string headpose_text = "Yaw:" + headPoseResults.rows[0][1] + " Pitch:" + headPoseResults.rows[1][1] + " Roll:" + headPoseResults.rows[2][1]; draw_text(&src_image, headpose_text.c_str(), 10, text_y, COLOR_WHITE, text_size); text_y += line_height; } else { draw_text(&src_image, "Head Pose: N/A", 10, text_y, COLOR_WHITE, text_size); text_y += line_height; } text_stream.str(""); text_stream << "Yawning: " << (yawnMetrics.isYawning ? "Yes" : "No"); draw_text(&src_image, text_stream.str().c_str(), 10, text_y, COLOR_WHITE, text_size); text_y += line_height; text_stream.str(""); text_stream << "Yawn Freq(5m): " << static_cast<int>(yawnMetrics.yawnFrequency_5min); draw_text(&src_image, text_stream.str().c_str(), 10, text_y, COLOR_WHITE, text_size); text_y += line_height; std::string detected_objects_text = ""; if (!detectedObjects.empty()) { detected_objects_text = "Detected: "; for (size_t j = 0; j < detectedObjects.size(); ++j) { detected_objects_text += detectedObjects[j]; if (j < detectedObjects.size() - 1) detected_objects_text += ", "; } draw_text(&src_image, detected_objects_text.c_str(), 10, text_y, COLOR_ORANGE, text_size); text_y += line_height; } text_stream.str(""); text_stream << "KSS Breakdown: P" << perclosKSS << " H" << headposeKSS << " Y" << yawnKSS << " O" << objdectdetectionKSS; draw_text(&src_image, text_stream.str().c_str(), 10, text_y, COLOR_GREEN, text_size); text_y += line_height; text_stream.str(""); text_stream << "Composite KSS: " << extractedTotalKSS; draw_text(&src_image, text_stream.str().c_str(), 10, text_y, status_color_uint, text_size); text_y += line_height; text_y += 5; double current_time_sec_perf = std::chrono::duration<double>(std::chrono::system_clock::now().time_since_epoch()).count(); double perclos_start = blinkDetector.getPerclosWindowStartTime(); double perclos_elapsed = current_time_sec_perf - perclos_start; double perclos_remaining = std::max(0.0, 60.0 - perclos_elapsed); text_stream.str(""); text_stream << "PERCLOS Window: " << std::fixed << std::setprecision(0) << perclos_remaining << "s"; draw_text(&src_image, text_stream.str().c_str(), 10, text_y, COLOR_CYAN, text_size); text_y += line_height; text_stream.str(""); text_stream << "Head Turns(5m): >=15: " << headPoseTracker.getHeadTurnCount15() << " | >=30: " << headPoseTracker.getHeadTurnCount30() << " | >=45: " << headPoseTracker.getHeadTurnCount45(); draw_text(&src_image, text_stream.str().c_str(), 10, text_y, COLOR_CYAN, text_size); text_y += line_height; text_stream.str(""); text_stream << "Yawns(5m): >=2s: " << yawnDetector.getYawnCount2s() << " | >=3s: " << yawnDetector.getYawnCount3s() << " | >=4s: " << yawnDetector.getYawnCount4s(); draw_text(&src_image, text_stream.str().c_str(), 10, text_y, COLOR_CYAN, text_size); text_y += line_height; text_stream.str(""); text_stream << "Obj Cons Frames: M:" << kssCalculator.getConsecutiveMobileFrames() << " E:" << kssCalculator.getConsecutiveEatDrinkFrames() << " S:" << kssCalculator.getConsecutiveSmokeFrames(); draw_text(&src_image, text_stream.str().c_str(), 10, text_y, COLOR_CYAN, text_size); text_y += line_height;

        } // End if (!calibration_done) / else

        // --- Draw ROIs ---
        #ifdef DEBUG_DRAW_ROIS
        if (driver_roi_defined) { /* Draw Fixed ROI */ draw_rectangle(&src_image, driver_object_roi.x, driver_object_roi.y, driver_object_roi.width, driver_object_roi.height, COLOR_MAGENTA, 1); draw_text(&src_image, "Fixed Driver ROI", driver_object_roi.x + 5, driver_object_roi.y + 15, COLOR_MAGENTA, 10); }
        // Draw non-tracked faces
         for (int i = 0; i < face_results.count; ++i) { if (i != current_tracked_driver_idx) { draw_rectangle(&src_image, face_results.faces[i].box.left, face_results.faces[i].box.top, face_results.faces[i].box.right - face_results.faces[i].box.left, face_results.faces[i].box.bottom - face_results.faces[i].box.top, COLOR_YELLOW, 1); } }
        #endif

        // --- Draw Resource Monitoring Info ---
        auto frame_processing_end_time = std::chrono::high_resolution_clock::now(); double frame_duration_ms = std::chrono::duration<double, std::milli>(frame_processing_end_time - frame_start_time).count(); calculateOverallFPS(frame_duration_ms, frame_times, overallFPS, max_time_records); getCPUUsage(currentCpuUsage, prevIdleTime, prevTotalTime); getTemperature(currentTemp); std::stringstream info_ss; info_ss << "FPS:" << std::fixed << std::setprecision(1) << overallFPS << "|CPU:" << currentCpuUsage << "%|T:" << currentTemp << "C"; draw_text(&src_image, info_ss.str().c_str(), 10, src_image.height - 20, COLOR_WHITE, 10);

        // --- Display Frame ---
        cv::Mat display_frame(src_image.height, src_image.width, CV_8UC3, map_info.data, src_image.width_stride);
        cv::imshow("DMS Output", display_frame);

        // --- Push Frame to Saving Pipeline ---
        //  if (pipeline_ && appsrc_ && saving_pipeline_caps_set) { GstClockTime duration = gst_util_uint64_scale_int(1, GST_SECOND, video_info.fps_n > 0 ? video_info.fps_d * video_info.fps_n : 30); pushFrameToPipeline(map_info.data, map_info.size, src_image.width, src_image.height, duration); }

        if (pipeline_ && appsrc_ && saving_pipeline_caps_set) { // Check flag
            // Calculate duration based on INPUT video info
            GstClockTime duration = gst_util_uint64_scale_int(GST_SECOND, video_info.fps_d, video_info.fps_n);

            pushFrameToPipeline(map_info.data, map_info.size, src_image.width, src_image.height, duration); // Pass duration
        }

        // --- Cleanup Frame Resources ---
        if (fixed_roi_crop_img.virt_addr) { free(fixed_roi_crop_img.virt_addr); fixed_roi_crop_img.virt_addr = nullptr; }
        gst_buffer_unmap(gst_buffer, &map_info);
        gst_sample_unref(sample);

        // --- Handle KeyPress ---
        if (cv::waitKey(1) == 27) break; // Exit on ESC

    } // End main loop

    // --- Final Cleanup ---
    if (fixed_roi_crop_img.virt_addr) { free(fixed_roi_crop_img.virt_addr); } printf("INFO: Cleaning up resources...\n"); cv::destroyAllWindows(); stop_yolo_worker.store(true); if (yolo_worker_thread.joinable()) yolo_worker_thread.join(); printf("INFO: YOLO thread joined.\n"); if (input_pipeline) { gst_element_set_state(input_pipeline, GST_STATE_NULL); gst_object_unref(appsink_); gst_object_unref(input_pipeline); printf("INFO: Input pipeline released.\n"); } if (pipeline_) { gst_element_send_event(pipeline_, gst_event_new_eos()); GstBus* bus = gst_element_get_bus(pipeline_); gst_bus_poll(bus, GST_MESSAGE_EOS, GST_CLOCK_TIME_NONE); gst_object_unref(bus); gst_element_set_state(pipeline_, GST_STATE_NULL); gst_object_unref(appsrc_); gst_object_unref(pipeline_); printf("INFO: Saving pipeline released.\n"); } gst_deinit(); printf("INFO: GStreamer deinitialized.\n"); release_face_analyzer(&app_ctx.face_ctx); release_yolo11(&app_ctx.yolo_ctx); deinit_post_process(); printf("INFO: RKNN models released.\n");

    printf("Exiting main (ret = %d)\n", ret);
    return ret;
}

# File: YawnDetector.hpp
// #ifndef YAWNDETECTOR_HPP
// #define YAWNDETECTOR_HPP

// #include <iostream>
// #include <vector>
// #include <map>
// #include <deque>
// #include <chrono>
// #include <opencv2/opencv.hpp>

// class YawnDetector {
// public:
//     YawnDetector();

//     // ++ New struct to store yawn details ++
//     struct YawnEvent {
//         double timestamp; // Time the yawn ENDED (in seconds since epoch)
//         double duration;  // Duration of the yawn (in seconds)
//     };

//     struct YawnMetrics {
//         bool isYawning;       // Is the mouth currently open in a yawn state?
//         // double yawnCount;    // Removed, KSS is now based on event history
//         double yawnFrequency_5min; // Calculated count meeting >=2s duration in last 5min (for KSS logic)
//         double yawnDuration;    // Duration of the last completed valid yawn (for display)
//         int yawnKSS;
//     };

//     YawnMetrics run(const std::vector<cv::Point>& faceLandmarks, int frame_width, int frame_height);


//     // ADDED: Getters for yawn counts used in KSS calculation
//     // NEW FOR UI
//     int getYawnCount2s() const { return count_ge_2_sec_last_calc; }
//     int getYawnCount3s() const { return count_ge_3_sec_last_calc; }
//     int getYawnCount4s() const { return count_ge_4_sec_last_calc; }

// private:
//     int calculateYawnKSS(double current_time_seconds); // Needs current time
//     double calculatePixelDistance(cv::Point landmark1, cv::Point landmark2, int frame_width, int frame_height);
//     // Removed calculateYawnFrequency

// private:
//     // Constants
//     const int MOUTH_TOP;
//     const int MOUTH_BOTTOM;
//     const double YAWN_THRESHOLD;
//     const double MIN_YAWN_DURATION; // Still needed to validate *any* yawn
//     const double YAWN_COOLDOWN;
//     const double KSS_TIME_WINDOW_SECONDS; // Window for KSS calculation (300s = 5 min)

//     // ++ Data Storage: Stores events of completed yawns ++
//     std::deque<YawnEvent> completed_yawns_history;

//     // State Variables
//     bool is_yawning_now;
//     time_t yawn_start_time; // Keep using time_t for start time recording
//     double last_valid_yawn_end_time; // Time the last *valid* yawn ended
//     int yawn_kss;
//     // double yawn_frequency; // Removed
//     double last_yawn_duration; // Keep for metrics struct
//     bool processing_detected_yawn;

//     // Hysteresis Variables
//     int frames_below_threshold;
//     const int FRAMES_HYSTERESIS;

//     // NEW FOR UI
//     int count_ge_2_sec_last_calc = 0;
//     int count_ge_3_sec_last_calc = 0;
//     int count_ge_4_sec_last_calc = 0;
// };

// #endif // YAWNDETECTOR_HPP









// Dynamic CALIBRATION
// File: behavior_analysis/YawnDetector.hpp
#ifndef YAWNDETECTOR_HPP
#define YAWNDETECTOR_HPP

// ... (includes remain the same) ...
#include <iostream>
#include <vector>
#include <map>
#include <deque>
#include <chrono>
#include <opencv2/opencv.hpp>
#include <numeric>   // For std::accumulate
#include <algorithm> // For std::sort

double calculate_median(std::deque<double>& data);

class YawnDetector {
public:
    YawnDetector(); // Default threshold in constructor

    // ... (YawnEvent, YawnMetrics structs remain the same) ...
     struct YawnEvent { double timestamp; double duration; }; struct YawnMetrics { bool isYawning; double yawnFrequency_5min; double yawnDuration; int yawnKSS; };

    YawnMetrics run(const std::vector<cv::Point>& faceLandmarks, int frame_width, int frame_height);

    // +++ ADDED: Setter for personalized threshold +++
    void setPersonalizedYawnThreshold(double threshold);

    // Getters for KSS counts remain
    int getYawnCount2s() const { return count_ge_2_sec_last_calc; }
    int getYawnCount3s() const { return count_ge_3_sec_last_calc; }
    int getYawnCount4s() const { return count_ge_4_sec_last_calc; }


private:
    int calculateYawnKSS(double current_time_seconds);
    double calculatePixelDistance(cv::Point landmark1, cv::Point landmark2, int frame_width, int frame_height);

    // Constants
    const int MOUTH_TOP;
    const int MOUTH_BOTTOM;
    // const double YAWN_THRESHOLD; // REMOVED const
    const double MIN_YAWN_DURATION;
    const double YAWN_COOLDOWN;
    const double KSS_TIME_WINDOW_SECONDS;

    // ++ Use non-const member for threshold ++
    double personalized_yawn_threshold; // Default set in constructor

    std::deque<YawnEvent> completed_yawns_history;

    // State Variables
    bool is_yawning_now;
    time_t yawn_start_time;
    double last_valid_yawn_end_time;
    int yawn_kss;
    double last_yawn_duration;
    bool processing_detected_yawn;
    int count_ge_2_sec_last_calc = 0;
    int count_ge_3_sec_last_calc = 0;
    int count_ge_4_sec_last_calc = 0;

    // Hysteresis Variables
    int frames_below_threshold;
    const int FRAMES_HYSTERESIS;
};

#endif // YAWNDETECTOR_HPP

# File: KSSCalculator.cpp
#include "KSSCalculator.hpp"
#include <algorithm>
#include <vector>
#include <cmath>
#include <iostream>
#include <set>
#include <chrono> // Need this for getting current time

// Constructor remains the same
KSSCalculator::KSSCalculator() :
    blinkKSS(1),
    blinkCountKSS(1),
    headPoseKSS(1),
    yawnKSS(1),
    objectDetectionKSS(0),
    perclos(0.0),
    blinksLastMinute(0),
    isYawning(false),
    yawnFrequency(0.0),
    yawnDuration(0.0)
    {}

// Setters remain the same
void KSSCalculator::setPerclos(double perclos) { this->perclos = perclos; }
void KSSCalculator::setHeadPose(int headPoseKSS) { this->headPoseKSS = std::max(1, headPoseKSS); }
void KSSCalculator::setYawnMetrics(bool isYawning, double yawnFrequency, double yawnDuration) { this->isYawning = isYawning; this->yawnFrequency = yawnFrequency; this->yawnDuration = yawnDuration; }
void KSSCalculator::setBlinksLastMinute(int count) { this->blinksLastMinute = count; }
void KSSCalculator::setDetectedObjects(const std::vector<std::string>& currentFrameObjects, double currentTimeSeconds) { /* ... implementation remains the same ... */
    // Use a set for efficient lookup of objects detected in *this* frame
    std::set<std::string> detectedSet(currentFrameObjects.begin(), currentFrameObjects.end());

    // --- Update Mobile Phone ---
    if (detectedSet.count("mobile")) {
        consecutiveMobileFrames++;
        // Trigger events when thresholds are crossed *for the first time* during this continuous detection
        if (consecutiveMobileFrames >= DURATION_THRESHOLD_3S && !triggeredMobileLevel3) {
            mobileEvents.push_back({currentTimeSeconds, 3});
            triggeredMobileLevel3 = true; // Prevent re-triggering level 3
        }
        if (consecutiveMobileFrames >= DURATION_THRESHOLD_2S && !triggeredMobileLevel2) {
             mobileEvents.push_back({currentTimeSeconds, 2});
             triggeredMobileLevel2 = true;
        }
        if (consecutiveMobileFrames >= DURATION_THRESHOLD_1S && !triggeredMobileLevel1) {
             mobileEvents.push_back({currentTimeSeconds, 1});
             triggeredMobileLevel1 = true;
        }
    } else {
        // Reset counter and triggers when mobile is *not* detected
        consecutiveMobileFrames = 0;
        triggeredMobileLevel1 = false;
        triggeredMobileLevel2 = false;
        triggeredMobileLevel3 = false;
    }

    // --- Update Eating/Drinking ---
    bool eatDrinkDetected = detectedSet.count("eating") || detectedSet.count("drinking");
    if (eatDrinkDetected) {
        consecutiveEatDrinkFrames++;
        if (consecutiveEatDrinkFrames >= DURATION_THRESHOLD_3S && !triggeredEatDrinkLevel3) {
            eatDrinkEvents.push_back({currentTimeSeconds, 3});
            triggeredEatDrinkLevel3 = true;
        }
         if (consecutiveEatDrinkFrames >= DURATION_THRESHOLD_2S && !triggeredEatDrinkLevel2) {
             eatDrinkEvents.push_back({currentTimeSeconds, 2});
             triggeredEatDrinkLevel2 = true;
        }
        if (consecutiveEatDrinkFrames >= DURATION_THRESHOLD_1S && !triggeredEatDrinkLevel1) {
             eatDrinkEvents.push_back({currentTimeSeconds, 1});
             triggeredEatDrinkLevel1 = true;
        }
    } else {
        consecutiveEatDrinkFrames = 0;
        triggeredEatDrinkLevel1 = false;
        triggeredEatDrinkLevel2 = false;
        triggeredEatDrinkLevel3 = false;
    }

    // --- Update Smoking ---
    if (detectedSet.count("cigarette")) { // Assuming class name is "cigarette"
        consecutiveSmokeFrames++;
         if (consecutiveSmokeFrames >= DURATION_THRESHOLD_3S && !triggeredSmokeLevel3) {
            smokeEvents.push_back({currentTimeSeconds, 3});
            triggeredSmokeLevel3 = true;
        }
        if (consecutiveSmokeFrames >= DURATION_THRESHOLD_2S && !triggeredSmokeLevel2) {
             smokeEvents.push_back({currentTimeSeconds, 2});
             triggeredSmokeLevel2 = true;
        }
        if (consecutiveSmokeFrames >= DURATION_THRESHOLD_1S && !triggeredSmokeLevel1) {
             smokeEvents.push_back({currentTimeSeconds, 1});
             triggeredSmokeLevel1 = true;
        }
    } else {
        consecutiveSmokeFrames = 0;
        triggeredSmokeLevel1 = false;
        triggeredSmokeLevel2 = false;
        triggeredSmokeLevel3 = false;
    }

    // --- Clean up old events (Optional but recommended) ---
    // Remove events older than the largest window (10 minutes) + a small buffer
    double cleanupThreshold = currentTimeSeconds - (WINDOW_10_MIN + 10.0);
    while (!mobileEvents.empty() && mobileEvents.front().timestamp < cleanupThreshold) mobileEvents.pop_front();
    while (!eatDrinkEvents.empty() && eatDrinkEvents.front().timestamp < cleanupThreshold) eatDrinkEvents.pop_front();
    while (!smokeEvents.empty() && smokeEvents.front().timestamp < cleanupThreshold) smokeEvents.pop_front();
}

// --- Helper to count events ---
int KSSCalculator::countEventsInWindow(const std::deque<ObjectEvent>& eventQueue, double windowSeconds, double currentTimeSeconds, int minDurationLevel) {
    int count = 0;
    double windowStartTime = currentTimeSeconds - windowSeconds;
    // Iterate from newest to oldest (deque allows efficient front access)
    for (const auto& event : eventQueue) {
        if (event.timestamp >= windowStartTime && event.durationLevel >= minDurationLevel) {
            count++;
        } else if (event.timestamp < windowStartTime) {
             // break; // Optimization - uncomment if sure about order
        }
    }
    return count;
}


// **** MODIFIED IMPLEMENTATION ****
std::vector<std::vector<std::string>> KSSCalculator::calculateCompositeKSS() {
    // Get current time once for consistency
    double now = std::chrono::duration<double>(std::chrono::system_clock::now().time_since_epoch()).count();

    // Calculate individual components (store in member variables as before)
    blinkKSS = calculateBlinkKSS();
    blinkCountKSS = calculateBlinkCountKSS();
    yawnKSS = calculateYawnKSS();
    objectDetectionKSS = calculateObjectDetectionKSS(now);
    // headPoseKSS is assumed to be already set via setHeadPose()

    // Calculate the total score
    // int totalKSS = blinkKSS + blinkCountKSS + headPoseKSS + yawnKSS + objectDetectionKSS;
    int totalKSS = blinkKSS + headPoseKSS + yawnKSS + objectDetectionKSS;

    // Create the result vector
    std::vector<std::vector<std::string>> kssBreakdown;

    // Populate the vector
    kssBreakdown.push_back({"PERCLOS KSS", std::to_string(blinkKSS)});
    kssBreakdown.push_back({"Blink Count KSS", std::to_string(blinkCountKSS)});
    kssBreakdown.push_back({"Head Pose KSS", std::to_string(headPoseKSS)});
    kssBreakdown.push_back({"Yawn KSS", std::to_string(yawnKSS)});
    kssBreakdown.push_back({"Object KSS", std::to_string(objectDetectionKSS)});
    kssBreakdown.push_back({"Composite KSS", std::to_string(totalKSS)}); // Add the total

    return kssBreakdown;
}
// **** END MODIFIED IMPLEMENTATION ****


// Individual calculation functions remain the same
int KSSCalculator::calculateBlinkKSS() {
    // Based on PERCLOS
    if (perclos < 5.0) {
        return 0;
    }else if (perclos >= 5.0 && perclos < 10.0) { // Use >= for start of range
        return 1;
    } else if (perclos >= 10.0 && perclos <= 20.0) {
        return 4;
    } else if (perclos > 25.0 && perclos <= 35.0) {
        return 7;
    } else if (perclos > 35.0){ // perclos > 30.0
        return 9;
    }
}

int KSSCalculator::calculateBlinkCountKSS() {
    if (blinksLastMinute > 30) {
        return 7;
    }
    else if (blinksLastMinute > 20 && blinksLastMinute <= 30) {
        return 4;
    }
    else if (blinksLastMinute > 10 && blinksLastMinute <= 20) {
        return 1;
    }
    else{
        return 0;
    }
}


// int KSSCalculator::calculateYawnKSS() {
//     if (yawnFrequency < 1.0) {
//         return 0;
//     } else if (yawnFrequency > 1.0 && yawnFrequency < 3.0) {
//         return 1;
//     }else if (yawnFrequency > 3.0 && yawnFrequency < 5.0) {
//         return 4;
//     } else if (yawnFrequency > 5.0 && yawnFrequency < 7.0) {
//         return 7;
//     } else if(yawnFrequency >= 7.0) { 
//         return 9;
//     }
// }

int KSSCalculator::calculateYawnKSS() {
    if (yawnFrequency > 7.0) {
        return 9;
    } else if (yawnFrequency > 5.0) {
        return 7;
    } else if (yawnFrequency > 3.0) {
        return 4;
    } else if (yawnFrequency > 1.0) {
        return 1;
    } else {
        return 0;
    }
}




int KSSCalculator::calculateObjectDetectionKSS(double currentTimeSeconds) {
    int mobileKSS = 0;
    int eatDrinkKSS = 0;
    int smokeKSS = 0;

    // --- Mobile Phone Scoring ---
    int mobileCountL3_10m = countEventsInWindow(mobileEvents, WINDOW_10_MIN, currentTimeSeconds, 3); // ≥3s events in 10min
    int mobileCountL2_5m  = countEventsInWindow(mobileEvents, WINDOW_5_MIN, currentTimeSeconds, 2);  // ≥2s events in 5min
    int mobileCountL1_1m  = countEventsInWindow(mobileEvents, WINDOW_1_MIN, currentTimeSeconds, 1);   // ≥1s events in 1min


    // ADDED: Store counts
    // NEW UI
    this->mobile_event_count_L3_10m = mobileCountL3_10m;
    this->mobile_event_count_L2_5m = mobileCountL2_5m;
    this->mobile_event_count_L1_1m = mobileCountL1_1m;
    // END ADDED

    if (mobileCountL3_10m >= 5) mobileKSS = KSS_LEVEL_3; // Highest priority
    else if (mobileCountL2_5m >= 3) mobileKSS = KSS_LEVEL_2;
    else if (mobileCountL1_1m >= 2) mobileKSS = KSS_LEVEL_1;
    // printf("DEBUG Mobile KSS: L3_10m=%d, L2_5m=%d, L1_1m=%d -> KSS=%d\n", mobileCountL3_10m, mobileCountL2_5m, mobileCountL1_1m, mobileKSS);


    // --- Eating/Drinking Scoring ---
    int eatDrinkCountL3_10m = countEventsInWindow(eatDrinkEvents, WINDOW_10_MIN, currentTimeSeconds, 3); // ≥3s events in 10min
    int eatDrinkCountL2_10m = countEventsInWindow(eatDrinkEvents, WINDOW_10_MIN, currentTimeSeconds, 2); // ≥2s events in 10min
    int eatDrinkCountL1_5m  = countEventsInWindow(eatDrinkEvents, WINDOW_5_MIN, currentTimeSeconds, 1);  // ≥1s events in 5min

    // ADDED: Store counts
    this->eat_drink_event_count_L3_10m = eatDrinkCountL3_10m;
    // ... store others if needed ...

    if (eatDrinkCountL3_10m >= 3) eatDrinkKSS = KSS_LEVEL_3;
    else if (eatDrinkCountL2_10m >= 2) eatDrinkKSS = KSS_LEVEL_2;
    else if (eatDrinkCountL1_5m >= 1) eatDrinkKSS = KSS_LEVEL_1;
    // printf("DEBUG Eat/Drink KSS: L3_10m=%d, L2_10m=%d, L1_5m=%d -> KSS=%d\n", eatDrinkCountL3_10m, eatDrinkCountL2_10m, eatDrinkCountL1_5m, eatDrinkKSS);


    // --- Smoking Scoring ---
    int smokeCountL3_10m = countEventsInWindow(smokeEvents, WINDOW_10_MIN, currentTimeSeconds, 3); // ≥3s events in 10min
    int smokeCountL2_10m = countEventsInWindow(smokeEvents, WINDOW_10_MIN, currentTimeSeconds, 2); // ≥2s events in 10min
    int smokeCountL1_5m  = countEventsInWindow(smokeEvents, WINDOW_5_MIN, currentTimeSeconds, 1);  // ≥1s events in 5min

    if (smokeCountL3_10m >= 3) smokeKSS = KSS_LEVEL_3;
    else if (smokeCountL2_10m >= 2) smokeKSS = KSS_LEVEL_2;
    else if (smokeCountL1_5m >= 1) smokeKSS = KSS_LEVEL_1;
     // printf("DEBUG Smoke KSS: L3_10m=%d, L2_10m=%d, L1_5m=%d -> KSS=%d\n", smokeCountL3_10m, smokeCountL2_10m, smokeCountL1_5m, smokeKSS);


    // --- Combine and Cap ---
    // Take the MAXIMUM KSS score from any of the object categories
    int combinedObjectKSS = std::max({mobileKSS, eatDrinkKSS, smokeKSS});
    int finalObjectKSS = 0;
    if (combinedObjectKSS == KSS_LEVEL_3) finalObjectKSS = 6; // Level 3 maps to 6 KSS points (max)
    else if (combinedObjectKSS == KSS_LEVEL_2) finalObjectKSS = 4; // Level 2 maps to 4 KSS points
    else if (combinedObjectKSS == KSS_LEVEL_1) finalObjectKSS = 2; // Level 1 maps to 2 KSS points
    else finalObjectKSS = 0; // No object criteria met

    // Update the member variable before returning
    this->objectDetectionKSS = finalObjectKSS; 
    // Ensure member is updated if needed elsewhere

    return finalObjectKSS; // Return the calculated & potentially mapped score
}


// int KSSCalculator::calculateObjectDetectionKSS(double currentTimeSeconds) {
//     int mobileKSS = 0;
//     int eatDrinkKSS = 0;
//     int smokeKSS = 0;

//     // --- Mobile Phone Scoring ---
//     int mobileCountL3_10m = countEventsInWindow(mobileEvents, WINDOW_10_MIN, currentTimeSeconds, 3); // ≥3s events in 10min
//     int mobileCountL2_5m  = countEventsInWindow(mobileEvents, WINDOW_5_MIN, currentTimeSeconds, 2);  // ≥2s events in 5min
//     int mobileCountL1_1m  = countEventsInWindow(mobileEvents, WINDOW_1_MIN, currentTimeSeconds, 1);   // ≥1s events in 1min

//     if (mobileCountL3_10m >= 5) mobileKSS = KSS_LEVEL_3; // Highest priority
//     else if (mobileCountL2_5m >= 3) mobileKSS = KSS_LEVEL_2;
//     else if (mobileCountL1_1m >= 2) mobileKSS = KSS_LEVEL_1;
//     // printf("DEBUG Mobile KSS: L3_10m=%d, L2_5m=%d, L1_1m=%d -> KSS=%d\n", mobileCountL3_10m, mobileCountL2_5m, mobileCountL1_1m, mobileKSS);


//     // --- Eating/Drinking Scoring ---
//     int eatDrinkCountL3_10m = countEventsInWindow(eatDrinkEvents, WINDOW_10_MIN, currentTimeSeconds, 3); // ≥3s events in 10min
//     int eatDrinkCountL2_10m = countEventsInWindow(eatDrinkEvents, WINDOW_10_MIN, currentTimeSeconds, 2); // ≥2s events in 10min
//     int eatDrinkCountL1_5m  = countEventsInWindow(eatDrinkEvents, WINDOW_5_MIN, currentTimeSeconds, 1);  // ≥1s events in 5min

//     if (eatDrinkCountL3_10m >= 3) eatDrinkKSS = KSS_LEVEL_3;
//     else if (eatDrinkCountL2_10m >= 2) eatDrinkKSS = KSS_LEVEL_2;
//     else if (eatDrinkCountL1_5m >= 1) eatDrinkKSS = KSS_LEVEL_1;
//     // printf("DEBUG Eat/Drink KSS: L3_10m=%d, L2_10m=%d, L1_5m=%d -> KSS=%d\n", eatDrinkCountL3_10m, eatDrinkCountL2_10m, eatDrinkCountL1_5m, eatDrinkKSS);


//     // --- Smoking Scoring ---
//     int smokeCountL3_10m = countEventsInWindow(smokeEvents, WINDOW_10_MIN, currentTimeSeconds, 3); // ≥3s events in 10min
//     int smokeCountL2_10m = countEventsInWindow(smokeEvents, WINDOW_10_MIN, currentTimeSeconds, 2); // ≥2s events in 10min
//     int smokeCountL1_5m  = countEventsInWindow(smokeEvents, WINDOW_5_MIN, currentTimeSeconds, 1);  // ≥1s events in 5min

//     if (smokeCountL3_10m >= 3) smokeKSS = KSS_LEVEL_3;
//     else if (smokeCountL2_10m >= 2) smokeKSS = KSS_LEVEL_2;
//     else if (smokeCountL1_5m >= 1) smokeKSS = KSS_LEVEL_1;
//      // printf("DEBUG Smoke KSS: L3_10m=%d, L2_10m=%d, L1_5m=%d -> KSS=%d\n", smokeCountL3_10m, smokeCountL2_10m, smokeCountL1_5m, smokeKSS);


//     // --- Combine and Cap ---
//     // Take the MAXIMUM KSS score from any of the object categories
//     int combinedObjectKSS = std::max({mobileKSS, eatDrinkKSS, smokeKSS});
//     int finalObjectKSS = 0;
//     if (combinedObjectKSS == KSS_LEVEL_3) finalObjectKSS = 6; // Level 3 maps to 6 KSS points (max)
//     else if (combinedObjectKSS == KSS_LEVEL_2) finalObjectKSS = 4; // Level 2 maps to 4 KSS points
//     else if (combinedObjectKSS == KSS_LEVEL_1) finalObjectKSS = 2; // Level 1 maps to 2 KSS points
//     else finalObjectKSS = 0; // No object criteria met

//     return finalObjectKSS; // Return the calculated & potentially mapped score
// }


// getKSSAlertStatus remains the same
std::string KSSCalculator::getKSSAlertStatus(int kss) {
    if (kss <= 3) {
        return ""; // No alert text for low risk
    } else if (kss >= 4 && kss <= 7) {
        return "Moderate Alert";
    } else if (kss >= 8 && kss <= 9) {
        return "HIGH ALERT";
    } else { // kss >= 10
        return "EXTREME ALERT";
    }
}

# File: HeadPoseTracker.hpp
#ifndef HEADPOSETRACKER_H
#define HEADPOSETRACKER_H

#include <vector>
#include <deque>
#include <ctime>
#include <opencv2/core.hpp>
#include <Eigen/Dense>
#include <string>
#include <sstream>
#include <iomanip>

namespace my {
class HeadPoseTracker {
public:
    HeadPoseTracker();
    ~HeadPoseTracker() = default;

    enum LandmarkIndex {
        LEFT_EYE = 33,
        RIGHT_EYE = 263,
        NOSE_TIP = 1,
        MOUTH_CENTER = 13,
        FOREHEAD = 10,
        MOUTH_LEFT = 61,
        MOUTH_RIGHT = 291
    };

    enum class HeadDirection {
        NEUTRAL,
        LEFT,
        RIGHT,
        UP,
        DOWN,
        LEFT_UP,
        LEFT_DOWN,
        RIGHT_UP,
        RIGHT_DOWN
    };

    struct HeadPoseData {
        float yaw;
        float pitch;
        float roll;
        time_t timestamp;
    };

    struct HeadTurnEvent {
        float angle;
        time_t timestamp;
    };

    struct HeadPoseResults {
        std::vector<std::vector<std::string>> rows;
        bool reference_set;
    };

    HeadPoseResults run(const std::vector<cv::Point>& faceLandmarks);
    bool isCalibrated() const { return calibration_state.reference_set; }


    // FOR NEW UI
    int getHeadTurnCount15() const { return count_15_last_calc; }
    int getHeadTurnCount30() const { return count_30_last_calc; }
    int getHeadTurnCount45() const { return count_45_last_calc; }

private:
    bool isNearPerfectAxes(const Eigen::Matrix3f& computedAxes, float tolerance = 0.2);
    Eigen::Matrix3f calculateAxes(const std::vector<cv::Point>& faceLandmarks);
    void autoCalibrate(const std::vector<cv::Point>& faceLandmarks);
    std::tuple<float, float, float> rotationMatrixToEulerAngles(const Eigen::Matrix3f& R);
    int calculateHeadMovementKSS(int timeWindow = 300, int minDuration = 3); // Updated timeWindow to 300 seconds (5 minutes)
    HeadDirection calculateHeadDirection(float yaw, float pitch);

    float roll = 0.0f;
    float yaw = 0.0f;
    float pitch = 0.0f;
    float perclos = 0.0f;
    std::vector<std::vector<std::string>> rows;

    float SMOOTHING_FACTOR = 0.2f;
    float MIN_SCALE = 0.05f;
    float MAX_SCALE = 0.2f;

    const float YAW_THRESHOLD = 10.0f;
    const float PITCH_THRESHOLD = 5.0f;

    struct CalibrationState {
        bool reference_set = false;
        Eigen::Vector3f reference_origin = Eigen::Vector3f::Zero();
        Eigen::Matrix3f reference_rotation = Eigen::Matrix3f::Identity();
        Eigen::Matrix3f rotation = Eigen::Matrix3f::Identity();
        float scale = 0.1f;
    } calibration_state;

    int head_movement_kss = 0;
    std::deque<HeadPoseData> head_movement_history;
    std::deque<HeadTurnEvent> head_turn_events; // New: Store head turn events

    float clamp(float value, float min, float max) {
        return std::max(min, std::min(max, value));
    }

    // ADDED: Member variables to store last calculated counts
    // FOR NEW UI
    int count_15_last_calc = 0;
    int count_30_last_calc = 0;
    int count_45_last_calc = 0;
};

} // namespace my

#endif // HEADPOSETRACKER_H

# File: BlinkDetector.hpp
// #ifndef BLINKDETECTOR_HPP
// #define BLINKDETECTOR_HPP

// #include <opencv2/opencv.hpp>
// #include <vector>
// #include <chrono>
// #include <numeric>
// #include <iostream>
// #include <deque>

// namespace my {

// class BlinkDetector {
// public:
//     BlinkDetector() :
//         wereEyesClosedPreviously(false),
//         blink_counter(0),
//         blinks_in_window(0),
//         perclos_start_time(0),
//         LEFT_EYE_POINTS({ 33, 160, 158, 133, 153, 144 }),
//         RIGHT_EYE_POINTS({ 362, 385, 387, 263, 380, 373 }),
//         // ++ Define separate thresholds ++
//         left_ear_threshold(0.20),  // Default/observed threshold for left eye
//         right_ear_threshold(0.40), // Observed threshold for right eye (adjust based on testing!)
//         blink_rate(0.0),
//         last_blink_time(0.0)
//     {
//         perclos_start_time = std::chrono::duration_cast<std::chrono::seconds>(
//                                  std::chrono::system_clock::now().time_since_epoch()).count();
//     };

//     void run(const std::vector<cv::Point>& faceLandmarks, int frame_width, int frame_height);
//     float calculateEAR(const std::vector<cv::Point>& landmarks, const std::vector<int>& eye_points, int w, int h);
//     // isEyeClosed function can remain the same conceptually, but we'll call it with different thresholds
//     bool isEyeClosed(float ear, float threshold); // Keep signature general

//     // Accessor functions:
//     bool isLeftEyeClosed() const;
//     bool isRightEyeClosed() const;
//     float getLeftEARValue() const;
//     float getRightEARValue() const;
//     int getBlinkCount() const { return blink_counter; }
//     int getBlinksInWindow() const { return blinks_in_window; }
//     double getPerclos() const { return !perclos_durations.empty() ? perclos_durations.back() : 0.0; }
//     double getLastBlinkDuration() const { return !blink_durations.empty() ? blink_durations.back() : 0.0; }
//     double getBlinkRate() const { return blink_rate; }

//     // NEW UI
//     double getPerclosWindowStartTime() const { return perclos_start_time; }

// private:
//     double blink_rate;
//     double last_blink_time;

//     bool wereEyesClosedPreviously;
//     std::chrono::high_resolution_clock::time_point eyesClosedStartTime;

//     bool isLeftEyeClosedFlag;
//     bool isRightEyeClosedFlag;

//     float leftEAR;
//     float rightEAR;

//     int blink_counter;
//     int blinks_in_window;
//     double perclos_start_time;

//     // ++ Separate Threshold Constants ++
//     const float left_ear_threshold;
//     const float right_ear_threshold;

//     const std::vector<int> LEFT_EYE_POINTS;
//     const std::vector<int> RIGHT_EYE_POINTS;


//     std::vector<double> blink_durations;
//     std::vector<double> perclos_durations;
//     std::deque<float> leftEARHistory;
//     std::deque<float> rightEARHistory;
//     float smoothEAR(std::deque<float>& earHistory, float newEAR);
// };

// } // namespace my

// #endif



// File: behavior_analysis/BlinkDetector.hpp
#ifndef BLINKDETECTOR_HPP
#define BLINKDETECTOR_HPP

#include <opencv2/opencv.hpp>
#include <vector>
#include <chrono>
#include <numeric>
#include <iostream>
#include <deque>

namespace my {

class BlinkDetector {
public:
    BlinkDetector() : // Initialize with default values, will be overwritten
        wereEyesClosedPreviously(false),
        blink_counter(0),
        blinks_in_window(0),
        perclos_start_time(0),
        LEFT_EYE_POINTS({ 33, 160, 158, 133, 153, 144 }),
        RIGHT_EYE_POINTS({ 362, 385, 387, 263, 380, 373 }),
        // ++ Use non-const members for personalized thresholds ++
        personalized_left_ear_threshold(0.20),  // Default starting value
        personalized_right_ear_threshold(0.40), // Default starting value
        blink_rate(0.0),
        last_blink_time(0.0)
    {
        perclos_start_time = std::chrono::duration_cast<std::chrono::seconds>(
                                 std::chrono::system_clock::now().time_since_epoch()).count();
    };

    void run(const std::vector<cv::Point>& faceLandmarks, int frame_width, int frame_height);
    float calculateEAR(const std::vector<cv::Point>& landmarks, const std::vector<int>& eye_points, int w, int h);
    bool isEyeClosed(float ear, float threshold);

    // +++ ADDED: Setter for personalized thresholds +++
    void setPersonalizedEarThresholds(float left_thresh, float right_thresh);

    // Accessor functions:
    bool isLeftEyeClosed() const;
    bool isRightEyeClosed() const;
    float getLeftEARValue() const;
    float getRightEARValue() const;
    int getBlinkCount() const { return blink_counter; }
    int getBlinksInWindow() const { return blinks_in_window; }
    double getPerclos() const { return !perclos_durations.empty() ? perclos_durations.back() : 0.0; }
    double getLastBlinkDuration() const { return !blink_durations.empty() ? blink_durations.back() : 0.0; }
    double getBlinkRate() const { return blink_rate; }

    // NEW UI
    double getPerclosWindowStartTime() const { return perclos_start_time; }


private:
    double blink_rate;
    double last_blink_time;

    bool wereEyesClosedPreviously;
    std::chrono::high_resolution_clock::time_point eyesClosedStartTime;

    bool isLeftEyeClosedFlag;
    bool isRightEyeClosedFlag;

    float leftEAR;
    float rightEAR;

    int blink_counter;
    int blinks_in_window;
    double perclos_start_time;

    // ++ These are now member variables, not const ++
    float personalized_left_ear_threshold;
    float personalized_right_ear_threshold;

    const std::vector<int> LEFT_EYE_POINTS;
    const std::vector<int> RIGHT_EYE_POINTS;


    std::vector<double> blink_durations;
    std::vector<double> perclos_durations;
    std::deque<float> leftEARHistory;
    std::deque<float> rightEARHistory;
    float smoothEAR(std::deque<float>& earHistory, float newEAR);
};

} // namespace my

#endif

# File: YawnDetector.cpp
// #include "YawnDetector.hpp"
// #include <iostream>
// #include <opencv2/imgproc.hpp>
// #include <chrono>
// #include <cmath>
// #include <numeric>
// #include <algorithm> // For std::count_if

// YawnDetector::YawnDetector() :
//     MOUTH_TOP(13),
//     MOUTH_BOTTOM(14),
//     YAWN_THRESHOLD(50.0),
//     MIN_YAWN_DURATION(0.3),   // Minimum duration to be considered a yawn at all
//     YAWN_COOLDOWN(1.0),
//     KSS_TIME_WINDOW_SECONDS(300.0), // 5 minutes for KSS calculation
//     is_yawning_now(false),
//     yawn_start_time(0),
//     last_valid_yawn_end_time(0.0),
//     yawn_kss(0), // Initialize KSS to 0 (or 1 if preferred baseline)
//     last_yawn_duration(0.0),
//     processing_detected_yawn(false),
//     frames_below_threshold(0),
//     FRAMES_HYSTERESIS(3)
// {
//     // No reset time needed now, history deque manages the window
// }

// // ++ Rewritten KSS Calculation ++
// int YawnDetector::calculateYawnKSS(double current_time_seconds) {
//     // 1. Filter history for the last 5 minutes
//     double window_start_time = current_time_seconds - KSS_TIME_WINDOW_SECONDS;
//     // Remove yawns older than the window from the front
//     while (!completed_yawns_history.empty() && completed_yawns_history.front().timestamp < window_start_time) {
//         completed_yawns_history.pop_front();
//     }

//     // 2. Count yawns meeting different duration criteria within the window
//     int count_ge_2_sec = 0;
//     int count_ge_3_sec = 0;
//     int count_ge_4_sec = 0;

//     for (const auto& yawn : completed_yawns_history) {
//         // No need to check timestamp again, deque only contains yawns within the window now
//         if (yawn.duration >= 4.0) {
//             count_ge_4_sec++;
//             count_ge_3_sec++;
//             count_ge_2_sec++;
//         } else if (yawn.duration >= 3.0) {
//             count_ge_3_sec++;
//             count_ge_2_sec++;
//         } else if (yawn.duration >= 2.0) {
//             count_ge_2_sec++;
//         }
//         // Yawns < 2 seconds (but >= MIN_YAWN_DURATION) are ignored for KSS calculation based on rules
//     }

//     // 3. Apply KSS rules (highest matching rule takes precedence)
//     int calculated_kss = 0; // Default to 0 KSS points for yawn component


//     if (count_ge_4_sec >= 5) {
//         calculated_kss = 7; // KSS 7-9 range
//     } else if (count_ge_3_sec >= 3) {
//         calculated_kss = 4; // KSS 4-6 range
//     } else if (count_ge_2_sec >= 3) {
//         calculated_kss = 1; // KSS 1-3 range
//     }


//     // ADDED: Store counts for getters
//     // NEW UI
//     this->count_ge_2_sec_last_calc = count_ge_2_sec;
//     this->count_ge_3_sec_last_calc = count_ge_3_sec;
//     this->count_ge_4_sec_last_calc = count_ge_4_sec;
//     // END ADDED

//     this->yawn_kss = calculated_kss; // Store the calculated KSS
//     return this->yawn_kss;
// }


// double YawnDetector::calculatePixelDistance(cv::Point landmark1, cv::Point landmark2, int frame_width, int frame_height) {
//     // Frame width/height might not be needed if distance is absolute pixels
//     return std::hypot((double)landmark1.x - landmark2.x, (double)landmark1.y - landmark2.y);
// }

// // Removed calculateYawnFrequency function


// YawnDetector::YawnMetrics YawnDetector::run(const std::vector<cv::Point>& faceLandmarks, int frame_width, int frame_height) {
//     YawnMetrics metrics = {};
//     auto current_time_point = std::chrono::system_clock::now();
//     double current_time_seconds = std::chrono::duration<double>(current_time_point.time_since_epoch()).count();
//     auto yawn_start_time_point = std::chrono::system_clock::from_time_t(yawn_start_time); // For duration calc

//     // Reset logic removed - handled by KSS window

//     if (faceLandmarks.size() > MOUTH_BOTTOM) {
//         cv::Point mouth_top = faceLandmarks[MOUTH_TOP];
//         cv::Point mouth_bottom = faceLandmarks[MOUTH_BOTTOM];
//         double mouth_distance = calculatePixelDistance(mouth_top, mouth_bottom, frame_width, frame_height);

//         // --- Yawn Detection State Machine (logic remains similar) ---
//         if (mouth_distance > YAWN_THRESHOLD) {
//             frames_below_threshold = 0;
//             if (!is_yawning_now) {
//                 is_yawning_now = true;
//                 yawn_start_time = std::chrono::system_clock::to_time_t(current_time_point);
//                 processing_detected_yawn = false; // Reset flag for potential new yawn
//                 last_yawn_duration = 0.0; // Reset duration display
//             }
//         } else { // Mouth distance is below threshold
//             if (is_yawning_now) { // Was yawning in the previous frame(s)
//                 frames_below_threshold++;
//                 if (frames_below_threshold >= FRAMES_HYSTERESIS) { // Mouth closed long enough
//                     std::chrono::duration<double> yawn_duration_chrono = current_time_point - yawn_start_time_point;
//                     double current_yawn_duration = yawn_duration_chrono.count();
//                     last_yawn_duration = current_yawn_duration; // Store for display metric

//                     double time_since_last_valid_yawn = current_time_seconds - last_valid_yawn_end_time;

//                     // Check if this completed yawn is valid to be *recorded*
//                     if (current_yawn_duration >= MIN_YAWN_DURATION &&
//                         !processing_detected_yawn &&
//                         time_since_last_valid_yawn > YAWN_COOLDOWN)
//                     {
//                         // *** Record the valid yawn event ***
//                         completed_yawns_history.push_back({current_time_seconds, current_yawn_duration});
//                         last_valid_yawn_end_time = current_time_seconds;  // Update time of last recorded valid yawn
//                         processing_detected_yawn = true; // Mark this specific potential yawn as processed
//                     } else if (current_yawn_duration < MIN_YAWN_DURATION) {
//                         last_yawn_duration = 0.0; // Don't display duration if too short
//                     }

//                     // End the yawn state regardless of validity for recording
//                     is_yawning_now = false;
//                     yawn_start_time = 0;
//                     // processing_detected_yawn remains true until a new yawn starts
//                 }
//             } else {
//                  // Mouth is closed, and wasn't yawning previously. Reset hysteresis counter.
//                  frames_below_threshold = 0;
//                  // processing_detected_yawn flag remains unchanged here.
//             }
//         }

//         // Update KSS based on history every frame
//         metrics.yawnKSS = calculateYawnKSS(current_time_seconds);

//     } else { // Not enough landmarks
//         if (is_yawning_now) { // If a yawn was in progress, end it abruptly
//             is_yawning_now = false;
//             yawn_start_time = 0;
//             last_yawn_duration = 0.0;
//             processing_detected_yawn = false;
//             frames_below_threshold = 0;
//         }
//         // Calculate KSS based on existing history even if face is lost temporarily
//         metrics.yawnKSS = calculateYawnKSS(current_time_seconds);
//     }

//     // Fill metrics structure
//     metrics.isYawning = is_yawning_now;             // Current mouth open state
//     metrics.yawnDuration = last_yawn_duration;      // Duration of the last completed yawn (valid or not)
//     // Calculate the frequency metric based on the same logic as KSS (count >=2s in 5min)
//     metrics.yawnFrequency_5min = 0; // Calculate this based on history if needed for display
//     for(const auto& yawn : completed_yawns_history) {
//         if (yawn.duration >= 2.0) {
//              metrics.yawnFrequency_5min++;
//         }
//     }
//     // metrics.yawnCount remains removed unless you re-add the resettable counter

//     return metrics;
// }




// DYNAMIC CALIBRATION
// File: behavior_analysis/YawnDetector.cpp
#include "YawnDetector.hpp"
#include <iostream>
#include <opencv2/imgproc.hpp>
#include <chrono>
#include <cmath>
#include <numeric>     // For std::accumulate
#include <algorithm>   // For std::sort, std::count_if
#include <vector>      // Needed for sorting in calculate_median

// +++ ADDED: Helper function definition HERE +++
// Helper to calculate median from a deque
double calculate_median(std::deque<double>& data) { // Takes non-const ref as it needs sorting copy
    if (data.empty()) {
        return 0.0; // Return 0 or some default for empty data
    }
    // Copy deque to vector for sorting
    std::vector<double> vec(data.begin(), data.end());
    std::sort(vec.begin(), vec.end());
    size_t n = vec.size();
    if (n % 2 == 0) {
        // Even number of elements: average the middle two
        return (vec[n / 2 - 1] + vec[n / 2]) / 2.0;
    } else {
        // Odd number of elements: return the middle element
        return vec[n / 2];
    }
}
// ++++++++++++++++++++++++++++++++++++++++++++++++

YawnDetector::YawnDetector() :
    MOUTH_TOP(13),
    MOUTH_BOTTOM(14),
    // YAWN_THRESHOLD(50.0), // Removed const initialization here
    MIN_YAWN_DURATION(0.3),
    YAWN_COOLDOWN(1.0),
    KSS_TIME_WINDOW_SECONDS(300.0),
    is_yawning_now(false),
    yawn_start_time(0),
    last_valid_yawn_end_time(0.0),
    yawn_kss(0),
    last_yawn_duration(0.0),
    processing_detected_yawn(false),
    frames_below_threshold(0),
    FRAMES_HYSTERESIS(3),
    personalized_yawn_threshold(50.0) // Initialize with default
{
    // completed_yawns_history initialized implicitly
}

void YawnDetector::setPersonalizedYawnThreshold(double threshold) {
    this->personalized_yawn_threshold = threshold;
    printf("INFO: Personalized Yawn threshold set: %.2f\n", this->personalized_yawn_threshold);
}

int YawnDetector::calculateYawnKSS(double current_time_seconds) {
    // 1. Filter history for the last 5 minutes
    double window_start_time = current_time_seconds - KSS_TIME_WINDOW_SECONDS;
    while (!completed_yawns_history.empty() && completed_yawns_history.front().timestamp < window_start_time) {
        completed_yawns_history.pop_front();
    }

    // 2. Count yawns meeting different duration criteria
    int count_ge_2_sec = 0;
    int count_ge_3_sec = 0;
    int count_ge_4_sec = 0;
    for (const auto& yawn : completed_yawns_history) {
        if (yawn.duration >= 4.0) { count_ge_4_sec++; count_ge_3_sec++; count_ge_2_sec++; }
        else if (yawn.duration >= 3.0) { count_ge_3_sec++; count_ge_2_sec++; }
        else if (yawn.duration >= 2.0) { count_ge_2_sec++; }
    }

    // 3. Apply KSS rules
    int calculated_kss = 0;
    if (count_ge_4_sec >= 5) { calculated_kss = 7; }
    else if (count_ge_3_sec >= 3) { calculated_kss = 4; }
    else if (count_ge_2_sec >= 3) { calculated_kss = 1; }

    // Store counts for getters
    this->count_ge_2_sec_last_calc = count_ge_2_sec;
    this->count_ge_3_sec_last_calc = count_ge_3_sec;
    this->count_ge_4_sec_last_calc = count_ge_4_sec;

    this->yawn_kss = calculated_kss;
    return this->yawn_kss;
}

double YawnDetector::calculatePixelDistance(cv::Point landmark1, cv::Point landmark2, int frame_width, int frame_height) {
    return std::hypot((double)landmark1.x - landmark2.x, (double)landmark1.y - landmark2.y);
}


YawnDetector::YawnMetrics YawnDetector::run(const std::vector<cv::Point>& faceLandmarks, int frame_width, int frame_height) {
    YawnMetrics metrics = {};
    auto current_time_point = std::chrono::system_clock::now();
    double current_time_seconds = std::chrono::duration<double>(current_time_point.time_since_epoch()).count();
    auto yawn_start_time_point = std::chrono::system_clock::from_time_t(yawn_start_time);

    if (faceLandmarks.size() > MOUTH_BOTTOM) {
        cv::Point mouth_top = faceLandmarks[MOUTH_TOP];
        cv::Point mouth_bottom = faceLandmarks[MOUTH_BOTTOM];
        double mouth_distance = calculatePixelDistance(mouth_top, mouth_bottom, frame_width, frame_height);

        if (mouth_distance > this->personalized_yawn_threshold) { // Use personalized threshold
            frames_below_threshold = 0;
            if (!is_yawning_now) {
                is_yawning_now = true;
                yawn_start_time = std::chrono::system_clock::to_time_t(current_time_point);
                processing_detected_yawn = false;
                last_yawn_duration = 0.0;
            }
        } else { // Mouth distance is below threshold
            if (is_yawning_now) {
                frames_below_threshold++;
                if (frames_below_threshold >= FRAMES_HYSTERESIS) {
                    std::chrono::duration<double> yawn_duration_chrono = current_time_point - yawn_start_time_point;
                    double current_yawn_duration = yawn_duration_chrono.count();
                    last_yawn_duration = current_yawn_duration;

                    double time_since_last_valid_yawn = current_time_seconds - last_valid_yawn_end_time;

                    if (current_yawn_duration >= MIN_YAWN_DURATION &&
                        !processing_detected_yawn &&
                        time_since_last_valid_yawn > YAWN_COOLDOWN)
                    {
                        completed_yawns_history.push_back({current_time_seconds, current_yawn_duration});
                        last_valid_yawn_end_time = current_time_seconds;
                        processing_detected_yawn = true;
                    } else if (current_yawn_duration < MIN_YAWN_DURATION) {
                        last_yawn_duration = 0.0;
                    }
                    is_yawning_now = false;
                    yawn_start_time = 0;
                }
            } else {
                 frames_below_threshold = 0;
            }
        }
        metrics.yawnKSS = calculateYawnKSS(current_time_seconds);
    } else { // Not enough landmarks
        if (is_yawning_now) { /* Reset state */ is_yawning_now = false; yawn_start_time = 0; last_yawn_duration = 0.0; processing_detected_yawn = false; frames_below_threshold = 0; }
        metrics.yawnKSS = calculateYawnKSS(current_time_seconds); // Calculate based on history
    }

    // Fill metrics structure
    metrics.isYawning = is_yawning_now;
    metrics.yawnDuration = last_yawn_duration;
    metrics.yawnFrequency_5min = 0; // Recalculate based on updated history
    for(const auto& yawn : completed_yawns_history) { if (yawn.duration >= 2.0) { metrics.yawnFrequency_5min++; } }

    return metrics;
}

# File: HeadPoseTracker.cpp
#include "HeadPoseTracker.hpp"
#include <iostream>
#include <iomanip>
#include <cmath>

namespace my {

HeadPoseTracker::HeadPoseTracker() {
}

bool HeadPoseTracker::isNearPerfectAxes(const Eigen::Matrix3f& computedAxes, float tolerance) {
    Eigen::Vector3f x_axis = computedAxes.col(0);
    Eigen::Vector3f y_axis = computedAxes.col(1);
    Eigen::Vector3f z_axis = computedAxes.col(2);

    float dot_xy = x_axis.dot(y_axis);
    float dot_yz = y_axis.dot(z_axis);
    float dot_zx = z_axis.dot(x_axis);

    float norm_x = x_axis.norm();
    float norm_y = y_axis.norm();
    float norm_z = z_axis.norm();

    bool isOrthogonal = std::abs(dot_xy) < tolerance &&
                        std::abs(dot_yz) < tolerance &&
                        std::abs(dot_zx) < tolerance;

    bool isUnitLength = std::abs(norm_x - 1.0f) < tolerance &&
                        std::abs(norm_y - 1.0f) < tolerance &&
                        std::abs(norm_z - 1.0f) < tolerance;


    return isOrthogonal && isUnitLength;
}

Eigen::Matrix3f HeadPoseTracker::calculateAxes(const std::vector<cv::Point>& faceLandmarks) {
    Eigen::Vector3f leftEye(faceLandmarks[LandmarkIndex::LEFT_EYE].x, faceLandmarks[LandmarkIndex::LEFT_EYE].y, 0.0f);
    Eigen::Vector3f rightEye(faceLandmarks[LandmarkIndex::RIGHT_EYE].x, faceLandmarks[LandmarkIndex::RIGHT_EYE].y, 0.0f);
    Eigen::Vector3f forehead(faceLandmarks[LandmarkIndex::FOREHEAD].x, faceLandmarks[LandmarkIndex::FOREHEAD].y, 0.0f);
    Eigen::Vector3f mouth(faceLandmarks[LandmarkIndex::MOUTH_CENTER].x, faceLandmarks[LandmarkIndex::MOUTH_CENTER].y, -20.0f);
    Eigen::Vector3f noseTip(faceLandmarks[LandmarkIndex::NOSE_TIP].x, faceLandmarks[LandmarkIndex::NOSE_TIP].y, 20.0f);
    Eigen::Vector3f mouthLeft(faceLandmarks[LandmarkIndex::MOUTH_LEFT].x, faceLandmarks[LandmarkIndex::MOUTH_LEFT].y, -15.0f);
    Eigen::Vector3f mouthRight(faceLandmarks[LandmarkIndex::MOUTH_RIGHT].x, faceLandmarks[LandmarkIndex::MOUTH_RIGHT].y, -15.0f);

    Eigen::Vector3f z_axis = noseTip - (leftEye + rightEye) / 2.0f;
    if (z_axis.norm() < 1e-6) {
        std::cerr << "Warning: z_axis is too small, using default z_axis\n";
        z_axis = Eigen::Vector3f(0, 0, 1);
    }
    z_axis.normalize();

    Eigen::Vector3f x_axis = rightEye - leftEye;
    if (x_axis.norm() < 1e-6) {
        std::cerr << "Warning: x_axis is too small, using default x_axis\n";
        x_axis = Eigen::Vector3f(1, 0, 0);
    }
    x_axis.normalize();

    Eigen::Vector3f y_axis = z_axis.cross(x_axis);
    if (y_axis.norm() < 1e-6) {
        std::cerr << "Warning: y_axis is too small, using default y_axis\n";
        y_axis = Eigen::Vector3f(0, 1, 0);
    }
    y_axis.normalize();

    x_axis = y_axis.cross(z_axis);
    x_axis.normalize();
    z_axis = x_axis.cross(y_axis);
    z_axis.normalize();

    Eigen::Matrix3f axes;
    axes.col(0) = x_axis;
    axes.col(1) = y_axis;
    axes.col(2) = z_axis;

    float det = axes.determinant();
    if (std::abs(det - 1.0f) > 1e-4) {
        std::cerr << "Warning: Computed axes determinant is not 1: " << det << "\n";
        return Eigen::Matrix3f::Identity();
    }

    return axes;
}

void HeadPoseTracker::autoCalibrate(const std::vector<cv::Point>& faceLandmarks) {
    if (!calibration_state.reference_set) {
        calibration_state.reference_origin = Eigen::Vector3f(faceLandmarks[LandmarkIndex::NOSE_TIP].x, faceLandmarks[LandmarkIndex::NOSE_TIP].y, 20.0f);
        calibration_state.reference_rotation = calculateAxes(faceLandmarks);
        if (isNearPerfectAxes(calibration_state.reference_rotation, 0.5)) {
            calibration_state.reference_set = true;
            // std::cout << "Reference pose set successfully\n";
        } else {
            std::cout << "Failed to set reference pose: axes not orthogonal\n";
        }
        return;
    }

    Eigen::Matrix3f currentRot = calculateAxes(faceLandmarks);
    Eigen::Matrix3f R_rel = currentRot * calibration_state.reference_rotation.transpose();
    Eigen::Matrix3f R_temp = (1 - SMOOTHING_FACTOR) * calibration_state.rotation + SMOOTHING_FACTOR * R_rel;

    for (int i = 0; i < 3; i++) {
        for (int j = 0; j < 3; j++) {
            R_temp(i, j) = std::max(-1e6f, std::min(1e6f, R_temp(i, j)));
        }
    }

    Eigen::JacobiSVD<Eigen::Matrix3f> svd(R_temp, Eigen::ComputeFullU | Eigen::ComputeFullV);
    calibration_state.rotation = svd.matrixU() * svd.matrixV().transpose();

    float det = calibration_state.rotation.determinant();
    if (std::abs(det - 1.0f) > 1e-4) {
        std::cerr << "Warning: Corrected rotation matrix determinant is not 1: " << det << "\n";
        calibration_state.rotation = Eigen::Matrix3f::Identity();
    }

    const cv::Point& right_eye = faceLandmarks[LandmarkIndex::RIGHT_EYE];
    const cv::Point& left_eye  = faceLandmarks[LandmarkIndex::LEFT_EYE];
    float eye_dist = std::hypot((float)(right_eye.x - left_eye.x), (float)(right_eye.y - left_eye.y));
    calibration_state.scale = clamp(eye_dist * 1.5f, MIN_SCALE, MAX_SCALE);
}

std::tuple<float, float, float> HeadPoseTracker::rotationMatrixToEulerAngles(const Eigen::Matrix3f& R) {
    float det = R.determinant();
    if (std::abs(det - 1.0f) > 1e-4) {
        std::cerr << "Warning: Rotation matrix determinant is not 1: " << det << std::endl;
        return std::make_tuple(0.0f, 0.0f, 0.0f);
    }

    float pitch = std::asin(-R(2, 0));
    float yaw, roll;

    if (std::abs(R(2, 0)) < 0.99999f) {
        yaw = std::atan2(R(1, 0), R(0, 0));
        roll = std::atan2(R(2, 1), R(2, 2));
    } else {
        yaw = std::atan2(-R(0, 1), R(1, 1));
        roll = 0.0f;
    }

    if (std::isnan(yaw) || std::isnan(pitch) || std::isnan(roll)) {
        std::cerr << "Warning: Euler angles contain nan\n";
        return std::make_tuple(0.0f, 0.0f, 0.0f);
    }

    yaw = yaw * 180.0f / M_PI;
    pitch = pitch * 180.0f / M_PI;
    roll = roll * 180.0f / M_PI;

    if (yaw > 90.0f) yaw -= 180.0f;
    if (yaw < -90.0f) yaw += 180.0f;
    if (pitch > 90.0f) pitch -= 180.0f;
    if (pitch < -90.0f) pitch += 180.0f;
    if (roll > 90.0f) roll -= 180.0f;
    if (roll < -90.0f) roll += 180.0f;

    return std::make_tuple(yaw, pitch, roll);
}

HeadPoseTracker::HeadDirection HeadPoseTracker::calculateHeadDirection(float yaw, float pitch) {
    bool isLeft = yaw < -YAW_THRESHOLD;
    bool isRight = yaw > YAW_THRESHOLD;
    bool isUp = pitch > PITCH_THRESHOLD;
    bool isDown = pitch < -PITCH_THRESHOLD;

    if (isLeft && isUp) return HeadDirection::LEFT_UP;
    if (isLeft && isDown) return HeadDirection::LEFT_DOWN;
    if (isRight && isUp) return HeadDirection::RIGHT_UP;
    if (isRight && isDown) return HeadDirection::RIGHT_DOWN;
    if (isLeft) return HeadDirection::LEFT;
    if (isRight) return HeadDirection::RIGHT;
    if (isUp) return HeadDirection::UP;
    if (isDown) return HeadDirection::DOWN;

    return HeadDirection::NEUTRAL;
}

int HeadPoseTracker::calculateHeadMovementKSS(int timeWindow, int minDuration) {
    // Define thresholds for head turns
    const float threshold_15 = 15.0f; // Head turn ≥ 15°
    const float threshold_30 = 20.0f; // Head turn ≥ 30°
    const float threshold_45 = 35.0f; // Head turn ≥ 45°

    // Count head turns within the time window
    time_t current_time = time(0);
    std::deque<HeadTurnEvent> recent_events;

    // Filter head turn events to keep only those within the 5-minute window
    for (const auto& event : head_turn_events) {
        if (current_time - event.timestamp <= timeWindow) {
            recent_events.push_back(event);
        }
    }

    // Remove old events from the history
    while (!head_turn_events.empty() && (current_time - head_turn_events.front().timestamp > timeWindow + 10)) {
        head_turn_events.pop_front();
    }

    if (recent_events.empty()) {
        head_movement_kss = 1;
        return head_movement_kss;
    }

    // Count head turns at each threshold
    int count_15 = 0; // Head turns ≥ 15°
    int count_30 = 0; // Head turns ≥ 30°
    int count_45 = 0; // Head turns ≥ 45°

    for (const auto& event : recent_events) {
        if (event.angle >= threshold_45) {
            count_45++;
            count_30++;
            count_15++;
        } else if (event.angle >= threshold_30) {
            count_30++;
            count_15++;
        } else if (event.angle >= threshold_15) {
            count_15++;
        }
    }

    // Determine KSS score based on the criteria
    if (count_45 >= 5) {
        head_movement_kss = 7; // KSS 7-9 (we'll use 7 as the base, can adjust to 9 if needed)
    } else if (count_30 >= 3) {
        head_movement_kss = 4; // KSS 4-6 (we'll use 4 as the base, can adjust to 6 if needed)
    } else if (count_15 >= 3) {
        head_movement_kss = 1; // KSS 1-3 (we'll use 1 as the base, can adjust to 3 if needed)
    } else {
        head_movement_kss = 1; // Default KSS if no criteria are met
    }


    // ADDED: Store counts for getters
    // NEW UI
    this->count_15_last_calc = count_15;
    this->count_30_last_calc = count_30;
    this->count_45_last_calc = count_45;
    // END ADDED

    return head_movement_kss;
}

HeadPoseTracker::HeadPoseResults HeadPoseTracker::run(const std::vector<cv::Point>& faceLandmarks) {
    HeadPoseResults results;
    results.reference_set = calibration_state.reference_set;
    if (faceLandmarks.empty()) {
        std::vector<std::vector<std::string>> rows = {
            {"Yaw", "N/A"}, {"Pitch", "N/A"}, {"Roll", "N/A"},
            {"Head KSS", "N/A"}, {"Direction", "N/A"}
        };
        results.rows = rows;
        return results;
    }

    autoCalibrate(faceLandmarks);

    std::tie(yaw, pitch, roll) = rotationMatrixToEulerAngles(calibration_state.rotation.transpose());

    // Smoothing angles
    static float smoothed_yaw = 0.0f, smoothed_pitch = 0.0f, smoothed_roll = 0.0f;
    const float alpha = 0.2f; // Smoothing factor
    smoothed_yaw = (1.0f - alpha) * smoothed_yaw + alpha * yaw;
    smoothed_pitch = (1.0f - alpha) * smoothed_pitch + alpha * pitch;
    smoothed_roll = (1.0f - alpha) * smoothed_roll + alpha * roll;
    yaw = smoothed_yaw;
    pitch = smoothed_pitch;
    roll = smoothed_roll;

    // Record current pose data
    HeadPoseData headPoseData;
    headPoseData.yaw = yaw;
    headPoseData.pitch = pitch;
    headPoseData.roll = roll;
    headPoseData.timestamp = time(0);
    head_movement_history.push_back(headPoseData);

    // Detect head turn events
    static float prev_yaw = 0.0f, prev_pitch = 0.0f;
    static bool was_below_15 = true; // Track if the head was below the 15° threshold
    static time_t last_event_time = 0;
    const float min_time_between_events = 1.0f; // Minimum 1 second between head turn events to avoid overcounting

    float max_angle = std::max(std::abs(yaw), std::abs(pitch));
    time_t current_time = time(0);

    // Check if the head has crossed a threshold (e.g., from < 15° to ≥ 15°)
    if (max_angle >= 15.0f && was_below_15 && (current_time - last_event_time) >= min_time_between_events) {
        HeadTurnEvent event;
        event.angle = max_angle;
        event.timestamp = current_time;
        head_turn_events.push_back(event);
        last_event_time = current_time;
        was_below_15 = false;
    } else if (max_angle < 15.0f) {
        was_below_15 = true; // Reset when head returns to neutral
    }

    // Calculate head direction
    HeadDirection direction = calculateHeadDirection(yaw, pitch);
    std::string direction_str;
    switch (direction) {
        case HeadDirection::NEUTRAL: direction_str = "Neutral"; break;
        case HeadDirection::LEFT: direction_str = "Left"; break;
        case HeadDirection::RIGHT: direction_str = "Right"; break;
        case HeadDirection::UP: direction_str = "Up"; break;
        case HeadDirection::DOWN: direction_str = "Down"; break;
        case HeadDirection::LEFT_UP: direction_str = "Left-Up"; break;
        case HeadDirection::LEFT_DOWN: direction_str = "Left-Down"; break;
        case HeadDirection::RIGHT_UP: direction_str = "Right-Up"; break;
        case HeadDirection::RIGHT_DOWN: direction_str = "Right-Down"; break;
    }

    // Calculate KSS Score after updating history
    int head_movement_kss_score = calculateHeadMovementKSS();

    // Format output strings
    std::stringstream stream;
    stream << std::fixed << std::setprecision(1);
    stream.str(std::string()); stream << yaw; std::string yaw_str = stream.str();
    stream.str(std::string()); stream << pitch; std::string pitch_str = stream.str();
    stream.str(std::string()); stream << roll; std::string roll_str = stream.str();

    std::vector<std::vector<std::string>> result_rows = {
        {"Yaw", yaw_str + " deg"},
        {"Pitch", pitch_str + " deg"},
        {"Roll", roll_str + " deg"},
        {"Head KSS", std::to_string(head_movement_kss_score)},
        {"Direction", direction_str}
    };
    results.rows = result_rows;

    return results;
}

} // namespace my

# File: BlinkDetector.cpp
// #include "BlinkDetector.hpp"
// #include <iostream>
// #include <opencv2/imgproc.hpp>
// #include <numeric>
// #include <chrono>

// float my::BlinkDetector::smoothEAR(std::deque<float>& earHistory, float newEAR) {
//     // (Implementation is the same)
//     earHistory.push_back(newEAR);
//     if (earHistory.size() > 5) { earHistory.pop_front(); }
//     if (earHistory.empty()) { return newEAR; }
//     float sum = std::accumulate(earHistory.begin(), earHistory.end(), 0.0f);
//     return sum / earHistory.size();
// }

// void my::BlinkDetector::run(const std::vector<cv::Point>& faceLandmarks, int frame_width, int frame_height){
//     int h = frame_height;
//     int w = frame_width;

//     if (faceLandmarks.empty()) {
//         isLeftEyeClosedFlag = false;
//         isRightEyeClosedFlag = false;
//         wereEyesClosedPreviously = false;
//         return;
//     }

//     leftEAR = calculateEAR(faceLandmarks, LEFT_EYE_POINTS, w, h);
//     rightEAR = calculateEAR(faceLandmarks, RIGHT_EYE_POINTS, w, h);

//     leftEAR = smoothEAR(leftEARHistory, leftEAR);
//     rightEAR = smoothEAR(rightEARHistory, rightEAR);

//     // *** MODIFIED: Use separate thresholds for determining closure state ***
//     isLeftEyeClosedFlag = isEyeClosed(leftEAR, left_ear_threshold);
//     isRightEyeClosedFlag = isEyeClosed(rightEAR, right_ear_threshold);

//     // Determine combined eye closure state using OR logic
//     // bool areEyesClosedNow = isLeftEyeClosedFlag || isRightEyeClosedFlag;
//     bool areEyesClosedNow = isLeftEyeClosedFlag && isRightEyeClosedFlag;

//     // --- Blink Detection Logic (using combined state - remains the same) ---
//     if (areEyesClosedNow) {
//         if (!wereEyesClosedPreviously) {
//             eyesClosedStartTime = std::chrono::high_resolution_clock::now();
//             wereEyesClosedPreviously = true;
//         }
//     } else {
//         if (wereEyesClosedPreviously) {
//             auto now = std::chrono::high_resolution_clock::now();
//             double closureDuration = std::chrono::duration_cast<std::chrono::milliseconds>(now - eyesClosedStartTime).count() / 1000.0;
//             blink_counter++;
//             blinks_in_window++;
//             blink_durations.push_back(closureDuration);
//             // Calculate Blink Rate (remains the same logic)
//             double current_time_sec = std::chrono::duration_cast<std::chrono::seconds>(std::chrono::system_clock::now().time_since_epoch()).count();
//             if (last_blink_time != 0) {
//                 double time_since_last_blink = current_time_sec - last_blink_time;
//                 if (time_since_last_blink > 0) blink_rate = 60.0 / time_since_last_blink; else blink_rate = 0.0;
//             }
//             last_blink_time = current_time_sec;
//             wereEyesClosedPreviously = false;
//         }
//     }

//     // --- PERCLOS Calculation Window (remains the same logic) ---
//     double current_time_sec = std::chrono::duration_cast<std::chrono::seconds>(std::chrono::system_clock::now().time_since_epoch()).count();
//     if (current_time_sec - perclos_start_time >= 60.0) {
//         double total_closure_duration_in_window = std::accumulate(blink_durations.begin(), blink_durations.end(), 0.0);
//         double perclos = (total_closure_duration_in_window / 60.0) * 100.0;
//         perclos_durations.push_back(perclos);
//         perclos_start_time = current_time_sec;
//         blink_durations.clear();
//         blinks_in_window = 0;
//     }
//     // isLeftEyeClosedFlag and isRightEyeClosedFlag were updated earlier
// }

// // calculateEAR remains the same
// float my::BlinkDetector::calculateEAR(const std::vector<cv::Point>& landmarks, const std::vector<int>& eye_points, int w, int h) {
//     // (Implementation is the same)
//     if (landmarks.size() <= *std::max_element(eye_points.begin(), eye_points.end())) { std::cerr << "Error in calculateEAR: Not enough landmarks (" << landmarks.size() << ") for max index " << *std::max_element(eye_points.begin(), eye_points.end()) << std::endl; return 1.0f; } try { cv::Point p1=landmarks[eye_points[0]], p2=landmarks[eye_points[1]], p3=landmarks[eye_points[2]], p4=landmarks[eye_points[3]], p5=landmarks[eye_points[4]], p6=landmarks[eye_points[5]]; double v1=cv::norm(p2-p6), v2=cv::norm(p3-p5), horz=cv::norm(p1-p4); if(horz<1e-6) return 1.0f; return static_cast<float>((v1+v2)/(2.0*horz)); } catch (const std::exception& e) { std::cerr << "Exception in calculateEAR: " << e.what() << std::endl; return 1.0f; }
// }

// // isEyeClosed implementation remains the same (takes threshold as argument)
// bool my::BlinkDetector::isEyeClosed(float ear, float threshold) {
//     return ear < threshold;
// }

// // Accessor functions remain the same
// bool my::BlinkDetector::isLeftEyeClosed() const{ return isLeftEyeClosedFlag; }
// bool my::BlinkDetector::isRightEyeClosed() const { return isRightEyeClosedFlag; }
// float my::BlinkDetector::getLeftEARValue() const{ return leftEAR; }
// float my::BlinkDetector::getRightEARValue() const{ return rightEAR; }












// DYNAMIC CALIBRATION

// File: behavior_analysis/BlinkDetector.cpp
#include "BlinkDetector.hpp"
#include <iostream>
#include <opencv2/imgproc.hpp>
#include <numeric>
#include <chrono>

// --- smoothEAR implementation remains the same ---
float my::BlinkDetector::smoothEAR(std::deque<float>& earHistory, float newEAR) {
    earHistory.push_back(newEAR);
    if (earHistory.size() > 5) { earHistory.pop_front(); }
    if (earHistory.empty()) { return newEAR; }
    float sum = std::accumulate(earHistory.begin(), earHistory.end(), 0.0f);
    return sum / earHistory.size();
}

// +++ ADDED: Implementation of the setter +++
void my::BlinkDetector::setPersonalizedEarThresholds(float left_thresh, float right_thresh) {
    this->personalized_left_ear_threshold = left_thresh;
    this->personalized_right_ear_threshold = right_thresh;
    printf("INFO: Personalized EAR thresholds set: L=%.3f, R=%.3f\n",
           this->personalized_left_ear_threshold, this->personalized_right_ear_threshold);
}
// +++++++++++++++++++++++++++++++++++++++++++++

void my::BlinkDetector::run(const std::vector<cv::Point>& faceLandmarks, int frame_width, int frame_height){
    int h = frame_height;
    int w = frame_width;

    if (faceLandmarks.empty()) {
        isLeftEyeClosedFlag = false;
        isRightEyeClosedFlag = false;
        wereEyesClosedPreviously = false;
        return;
    }

    leftEAR = calculateEAR(faceLandmarks, LEFT_EYE_POINTS, w, h);
    rightEAR = calculateEAR(faceLandmarks, RIGHT_EYE_POINTS, w, h);

    leftEAR = smoothEAR(leftEARHistory, leftEAR);
    rightEAR = smoothEAR(rightEARHistory, rightEAR);

    // *** MODIFIED: Use personalized member thresholds ***
    isLeftEyeClosedFlag = isEyeClosed(leftEAR, this->personalized_left_ear_threshold);
    isRightEyeClosedFlag = isEyeClosed(rightEAR, this->personalized_right_ear_threshold);

    // Determine combined eye closure state using AND logic (both must be closed for PERCLOS/blink count)
    bool areEyesClosedNow = isLeftEyeClosedFlag && isRightEyeClosedFlag; // Changed from OR

    // --- Blink Detection Logic (using combined state - remains the same) ---
    if (areEyesClosedNow) {
        if (!wereEyesClosedPreviously) {
            eyesClosedStartTime = std::chrono::high_resolution_clock::now();
            wereEyesClosedPreviously = true;
        }
    } else {
        if (wereEyesClosedPreviously) {
            auto now = std::chrono::high_resolution_clock::now();
            double closureDuration = std::chrono::duration_cast<std::chrono::milliseconds>(now - eyesClosedStartTime).count() / 1000.0;
            // Only count as a blink if duration is reasonable (e.g., > 0.05s and < 1.0s) - TUNE THIS
            if (closureDuration > 0.05 && closureDuration < 1.0) {
                blink_counter++;
                blinks_in_window++;
                blink_durations.push_back(closureDuration); // Store duration for PERCLOS

                // Calculate Blink Rate (remains the same logic)
                double current_time_sec = std::chrono::duration_cast<std::chrono::seconds>(std::chrono::system_clock::now().time_since_epoch()).count();
                if (last_blink_time != 0) {
                    double time_since_last_blink = current_time_sec - last_blink_time;
                    if (time_since_last_blink > 0) blink_rate = 60.0 / time_since_last_blink; else blink_rate = 0.0;
                }
                last_blink_time = current_time_sec;
            }
            // Reset regardless of whether it was counted as a valid blink duration
            wereEyesClosedPreviously = false;
        }
    }

    // --- PERCLOS Calculation Window (remains the same logic) ---
    double current_time_sec = std::chrono::duration_cast<std::chrono::seconds>(std::chrono::system_clock::now().time_since_epoch()).count();
    if (current_time_sec - perclos_start_time >= 60.0) {
        double total_closure_duration_in_window = 0;
        // If eyes are currently closed, add the time elapsed since they closed
        if (wereEyesClosedPreviously) {
            total_closure_duration_in_window += std::chrono::duration_cast<std::chrono::milliseconds>(std::chrono::high_resolution_clock::now() - eyesClosedStartTime).count() / 1000.0;
        }
        // Add durations of completed blinks within the window
        total_closure_duration_in_window += std::accumulate(blink_durations.begin(), blink_durations.end(), 0.0);

        double perclos = (total_closure_duration_in_window / 60.0) * 100.0;
        perclos = std::min(perclos, 100.0); // Cap PERCLOS at 100%
        perclos_durations.push_back(perclos);

        // Reset for next window
        perclos_start_time = current_time_sec;
        blink_durations.clear();
        blinks_in_window = 0;
        // If eyes were closed when window reset, restart the timer for the current closure
        if (wereEyesClosedPreviously) {
            eyesClosedStartTime = std::chrono::high_resolution_clock::now();
        }
    }
}

// --- calculateEAR remains the same ---
float my::BlinkDetector::calculateEAR(const std::vector<cv::Point>& landmarks, const std::vector<int>& eye_points, int w, int h) {
    if (landmarks.empty()) return 1.0f; int max_idx = 0; for(int idx : eye_points) { if (idx > max_idx) max_idx = idx; } if (max_idx >= landmarks.size()) { return 1.0f; } try { cv::Point p1=landmarks.at(eye_points.at(0)); cv::Point p2=landmarks.at(eye_points.at(1)); cv::Point p3=landmarks.at(eye_points.at(2)); cv::Point p4=landmarks.at(eye_points.at(3)); cv::Point p5=landmarks.at(eye_points.at(4)); cv::Point p6=landmarks.at(eye_points.at(5)); double v1=cv::norm(p2-p6); double v2=cv::norm(p3-p5); double h=cv::norm(p1-p4); if(h<1e-6) return 1.0f; return static_cast<float>((v1+v2)/(2.0*h)); } catch(const std::out_of_range& oor) { std::cerr << "Out of Range error in calculateEAR: " << oor.what() << std::endl; return 1.0f; } catch(...) { std::cerr << "Unknown exception in calculateEAR" << std::endl; return 1.0f; }
}

// --- isEyeClosed implementation remains the same ---
bool my::BlinkDetector::isEyeClosed(float ear, float threshold) {
    return ear < threshold;
}

// --- Accessor functions remain the same ---
bool my::BlinkDetector::isLeftEyeClosed() const{ return isLeftEyeClosedFlag; }
bool my::BlinkDetector::isRightEyeClosed() const { return isRightEyeClosedFlag; }
float my::BlinkDetector::getLeftEARValue() const{ return leftEAR; }
float my::BlinkDetector::getRightEARValue() const{ return rightEAR; }

# File: KSSCalculator.hpp
// KSSCalculator.hpp
#ifndef KSSCALCULATOR_HPP
#define KSSCALCULATOR_HPP

#include <string>
#include <vector>
#include <deque>  // <-- Ensure deque is included if needed by ObjectEvent
#include <chrono> // <-- Ensure chrono is included if needed by ObjectEvent

class KSSCalculator {
public:
    KSSCalculator();


    // FOR UI
    // ADDED: Getters for consecutive frame counts
    int getConsecutiveMobileFrames() const { return consecutiveMobileFrames; }
    int getConsecutiveEatDrinkFrames() const { return consecutiveEatDrinkFrames; }
    int getConsecutiveSmokeFrames() const { return consecutiveSmokeFrames; }

    // ADDED: Getters for event counts (optional, might be complex to show all)
    // Example: Get count of level 3 mobile events in last 10 mins
    int getMobileEventsL3_10m(double currentTimeSeconds) {
        // Note: Need to call countEventsInWindow here or store result from calculateObjectDetectionKSS
        // For simplicity, let's just return the stored value if calculateObjectDetectionKSS updates it.
        // Alternatively, KSSCalculator could store the last calculated counts.
        // Let's assume calculateObjectDetectionKSS stores these internally for now.
        // We'll add placeholder members and getters.
         return mobile_event_count_L3_10m; // Need to compute and store this in calculateObjectDetectionKSS
    }

    // UNTIL THIS


    // Setters for input factors
    void setPerclos(double perclos);
    void setHeadPose(int headPoseKSS); // Setter remains the same
    void setYawnMetrics(bool isYawning, double yawnFrequency, double yawnDuration);
    void setDetectedObjects(const std::vector<std::string>& currentFrameObjects, double currentTimeSeconds);
    void setBlinksLastMinute(int count);

    // **** MODIFIED RETURN TYPE ****
    // Calculate the composite KSS score and return detailed breakdown
    std::vector<std::vector<std::string>> calculateCompositeKSS();

    // Get KSS alert status string (remains the same)
    std::string getKSSAlertStatus(int compositeKSS);

private:
    // --- Constants ---
    // Duration thresholds (in frames, assuming ~30 FPS)
    const int DURATION_THRESHOLD_1S = 30;
    const int DURATION_THRESHOLD_2S = 60;
    const int DURATION_THRESHOLD_3S = 90;
    // Time windows (in seconds)
    const double WINDOW_1_MIN = 60.0;
    const double WINDOW_5_MIN = 300.0;
    const double WINDOW_10_MIN = 600.0;
    // KSS Scores mapping (example, adjust if needed)
    const int KSS_LEVEL_1 = 1; // Corresponds to base KSS 1-3
    const int KSS_LEVEL_2 = 4; // Corresponds to base KSS 4-6
    const int KSS_LEVEL_3 = 7; // Corresponds to base KSS 7-9

    // Helper functions to calculate individual KSS scores
    int calculateBlinkKSS();
    int calculateBlinkCountKSS();
    int calculateYawnKSS();
    int calculateObjectDetectionKSS(double currentTimeSeconds); // *** MODIFIED: Needs current time

    // --- Event Tracking ---
    struct ObjectEvent {
        double timestamp; // Time the duration threshold was met
        int durationLevel; // 1, 2, or 3 (corresponding to 1s, 2s, 3s)
    };
    std::deque<ObjectEvent> mobileEvents;
    std::deque<ObjectEvent> eatDrinkEvents;
    std::deque<ObjectEvent> smokeEvents;

    // --- Consecutive Detection Tracking ---
    int consecutiveMobileFrames = 0;
    int consecutiveEatDrinkFrames = 0;
    int consecutiveSmokeFrames = 0;
    // Flags to prevent multiple event triggers for the same continuous detection
    bool triggeredMobileLevel1 = false;
    bool triggeredMobileLevel2 = false;
    bool triggeredMobileLevel3 = false;
    bool triggeredEatDrinkLevel1 = false;
    bool triggeredEatDrinkLevel2 = false;
    bool triggeredEatDrinkLevel3 = false;
    bool triggeredSmokeLevel1 = false;
    bool triggeredSmokeLevel2 = false;
    bool triggeredSmokeLevel3 = false;

    // --- Helper to count events in a window ---
    int countEventsInWindow(const std::deque<ObjectEvent>& eventQueue, double windowSeconds, double currentTimeSeconds, int minDurationLevel = 1);

    // Individual KSS scores (keep as member variables if needed internally)
    int blinkKSS;
    int blinkCountKSS;
    int headPoseKSS; // Still set externally via setter
    int yawnKSS;
    int objectDetectionKSS;

    // Input factors
    double perclos;
    int blinksLastMinute;
    bool isYawning;
    double yawnFrequency;
    double yawnDuration;
    // No longer need to store detectedObjects list here




    // FOR UI
    int mobile_event_count_L3_10m = 0;
    int mobile_event_count_L2_5m = 0;
    int mobile_event_count_L1_1m = 0;
    int eat_drink_event_count_L3_10m = 0;
};

#endif // KSSCALCULATOR_HPP

# File: yolo11.h
#ifndef _RKNN_DEMO_YOLO11_H_
#define _RKNN_DEMO_YOLO11_H_

#include "rknn_api.h"
#include "common.h"

#if defined(RV1106_1103)
    typedef struct { /* ... dma buf ... */ } rknn_dma_buf;
#endif

// *** RENAME the context struct ***
typedef struct yolo11_app_context_t { 
    rknn_context rknn_ctx;
    rknn_input_output_num io_num;
    rknn_tensor_attr* input_attrs;
    rknn_tensor_attr* output_attrs;
#if defined(RV1106_1103)
    rknn_tensor_mem* input_mems[1];
    rknn_tensor_mem* output_mems[9]; // Assuming max 9 outputs for YOLO? Adjust if needed.
    rknn_dma_buf img_dma_buf;
#endif
#if defined(ZERO_COPY)
    rknn_tensor_mem* input_mems[1];
    rknn_tensor_mem* output_mems[9]; // Assuming max 9 outputs
    rknn_tensor_attr* input_native_attrs;
    rknn_tensor_attr* output_native_attrs;
#endif
    int model_channel;
    int model_width;
    int model_height;
    bool is_quant;
} yolo11_app_context_t; 


#include "postprocess.h" // Include YOLO postprocess header


// *** RENAME functions for clarity ***
int init_yolo11(const char* model_path, yolo11_app_context_t* app_ctx); 

int release_yolo11(yolo11_app_context_t* app_ctx); 

int inference_yolo11(yolo11_app_context_t* app_ctx,
                     image_buffer_t* img,
                     object_detect_result_list* od_results);

#endif //_RKNN_DEMO_YOLO11_H_

# File: postprocess.cc
#include "yolo11.h"

#include <math.h>
#include <stdint.h>
#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#include <sys/time.h>

#include <set>
#include <vector>
#define LABEL_NALE_TXT_PATH "../../model/custom_class.txt"

static char *labels[OBJ_CLASS_NUM];

inline static int clamp(float val, int min, int max) { return val > min ? (val < max ? val : max) : min; }

static char *readLine(FILE *fp, char *buffer, int *len)
{
    int ch;
    int i = 0;
    size_t buff_len = 0;

    buffer = (char *)malloc(buff_len + 1);
    if (!buffer)
        return NULL; // Out of memory

    while ((ch = fgetc(fp)) != '\n' && ch != EOF)
    {
        buff_len++;
        void *tmp = realloc(buffer, buff_len + 1);
        if (tmp == NULL)
        {
            free(buffer);
            return NULL; // Out of memory
        }
        buffer = (char *)tmp;

        buffer[i] = (char)ch;
        i++;
    }
    buffer[i] = '\0';

    *len = buff_len;

    // Detect end
    if (ch == EOF && (i == 0 || ferror(fp)))
    {
        free(buffer);
        return NULL;
    }
    return buffer;
}

static int readLines(const char *fileName, char *lines[], int max_line)
{
    FILE *file = fopen(fileName, "r");
    char *s;
    int i = 0;
    int n = 0;

    if (file == NULL)
    {
        printf("Open %s fail!\n", fileName);
        return -1;
    }

    while ((s = readLine(file, s, &n)) != NULL)
    {
        lines[i++] = s;
        if (i >= max_line)
            break;
    }
    fclose(file);
    return i;
}

static int loadLabelName(const char *locationFilename, char *label[])
{
    printf("load label %s\n", locationFilename);
    readLines(locationFilename, label, OBJ_CLASS_NUM);
    return 0;
}

static float CalculateOverlap(float xmin0, float ymin0, float xmax0, float ymax0, float xmin1, float ymin1, float xmax1,
                              float ymax1)
{
    float w = fmax(0.f, fmin(xmax0, xmax1) - fmax(xmin0, xmin1) + 1.0);
    float h = fmax(0.f, fmin(ymax0, ymax1) - fmax(ymin0, ymin1) + 1.0);
    float i = w * h;
    float u = (xmax0 - xmin0 + 1.0) * (ymax0 - ymin0 + 1.0) + (xmax1 - xmin1 + 1.0) * (ymax1 - ymin1 + 1.0) - i;
    return u <= 0.f ? 0.f : (i / u);
}

static int nms(int validCount, std::vector<float> &outputLocations, std::vector<int> classIds, std::vector<int> &order,
               int filterId, float threshold)
{
    for (int i = 0; i < validCount; ++i)
    {
        int n = order[i];
        if (n == -1 || classIds[n] != filterId)
        {
            continue;
        }
        for (int j = i + 1; j < validCount; ++j)
        {
            int m = order[j];
            if (m == -1 || classIds[m] != filterId)
            {
                continue;
            }
            float xmin0 = outputLocations[n * 4 + 0];
            float ymin0 = outputLocations[n * 4 + 1];
            float xmax0 = outputLocations[n * 4 + 0] + outputLocations[n * 4 + 2];
            float ymax0 = outputLocations[n * 4 + 1] + outputLocations[n * 4 + 3];

            float xmin1 = outputLocations[m * 4 + 0];
            float ymin1 = outputLocations[m * 4 + 1];
            float xmax1 = outputLocations[m * 4 + 0] + outputLocations[m * 4 + 2];
            float ymax1 = outputLocations[m * 4 + 1] + outputLocations[m * 4 + 3];

            float iou = CalculateOverlap(xmin0, ymin0, xmax0, ymax0, xmin1, ymin1, xmax1, ymax1);

            if (iou > threshold)
            {
                order[j] = -1;
            }
        }
    }
    return 0;
}

static int quick_sort_indice_inverse(std::vector<float> &input, int left, int right, std::vector<int> &indices)
{
    float key;
    int key_index;
    int low = left;
    int high = right;
    if (left < right)
    {
        key_index = indices[left];
        key = input[left];
        while (low < high)
        {
            while (low < high && input[high] <= key)
            {
                high--;
            }
            input[low] = input[high];
            indices[low] = indices[high];
            while (low < high && input[low] >= key)
            {
                low++;
            }
            input[high] = input[low];
            indices[high] = indices[low];
        }
        input[low] = key;
        indices[low] = key_index;
        quick_sort_indice_inverse(input, left, low - 1, indices);
        quick_sort_indice_inverse(input, low + 1, right, indices);
    }
    return low;
}

static float sigmoid(float x) { return 1.0 / (1.0 + expf(-x)); }

static float unsigmoid(float y) { return -1.0 * logf((1.0 / y) - 1.0); }

inline static int32_t __clip(float val, float min, float max)
{
    float f = val <= min ? min : (val >= max ? max : val);
    return f;
}

static int8_t qnt_f32_to_affine(float f32, int32_t zp, float scale)
{
    float dst_val = (f32 / scale) + zp;
    int8_t res = (int8_t)__clip(dst_val, -128, 127);
    return res;
}

static uint8_t qnt_f32_to_affine_u8(float f32, int32_t zp, float scale)
{
    float dst_val = (f32 / scale) + zp;
    uint8_t res = (uint8_t)__clip(dst_val, 0, 255);
    return res;
}

static float deqnt_affine_to_f32(int8_t qnt, int32_t zp, float scale) { return ((float)qnt - (float)zp) * scale; }

static float deqnt_affine_u8_to_f32(uint8_t qnt, int32_t zp, float scale) { return ((float)qnt - (float)zp) * scale; }

static void compute_dfl(float* tensor, int dfl_len, float* box){
    for (int b=0; b<4; b++){
        float exp_t[dfl_len];
        float exp_sum=0;
        float acc_sum=0;
        for (int i=0; i< dfl_len; i++){
            exp_t[i] = exp(tensor[i+b*dfl_len]);
            exp_sum += exp_t[i];
        }
        
        for (int i=0; i< dfl_len; i++){
            acc_sum += exp_t[i]/exp_sum *i;
        }
        box[b] = acc_sum;
    }
}

static int process_u8(uint8_t *box_tensor, int32_t box_zp, float box_scale,
                      uint8_t *score_tensor, int32_t score_zp, float score_scale,
                      uint8_t *score_sum_tensor, int32_t score_sum_zp, float score_sum_scale,
                      int grid_h, int grid_w, int stride, int dfl_len,
                      std::vector<float> &boxes,
                      std::vector<float> &objProbs,
                      std::vector<int> &classId,
                      float threshold)
{
    int validCount = 0;
    int grid_len = grid_h * grid_w;
    uint8_t score_thres_u8 = qnt_f32_to_affine_u8(threshold, score_zp, score_scale);
    uint8_t score_sum_thres_u8 = qnt_f32_to_affine_u8(threshold, score_sum_zp, score_sum_scale);

    for (int i = 0; i < grid_h; i++)
    {
        for (int j = 0; j < grid_w; j++)
        {
            int offset = i * grid_w + j;
            int max_class_id = -1;

            // Use score sum to quickly filter
            if (score_sum_tensor != nullptr)
            {
                if (score_sum_tensor[offset] < score_sum_thres_u8)
                {
                    continue;
                }
            }

            uint8_t max_score = -score_zp;
            for (int c = 0; c < OBJ_CLASS_NUM; c++)
            {
                if ((score_tensor[offset] > score_thres_u8) && (score_tensor[offset] > max_score))
                {
                    max_score = score_tensor[offset];
                    max_class_id = c;
                }
                offset += grid_len;
            }

            // compute box
            if (max_score > score_thres_u8)
            {
                offset = i * grid_w + j;
                float box[4];
                float before_dfl[dfl_len * 4];
                for (int k = 0; k < dfl_len * 4; k++)
                {
                    before_dfl[k] = deqnt_affine_u8_to_f32(box_tensor[offset], box_zp, box_scale);
                    offset += grid_len;
                }
                compute_dfl(before_dfl, dfl_len, box);

                float x1, y1, x2, y2, w, h;
                x1 = (-box[0] + j + 0.5) * stride;
                y1 = (-box[1] + i + 0.5) * stride;
                x2 = (box[2] + j + 0.5) * stride;
                y2 = (box[3] + i + 0.5) * stride;
                w = x2 - x1;
                h = y2 - y1;
                boxes.push_back(x1);
                boxes.push_back(y1);
                boxes.push_back(w);
                boxes.push_back(h);

                objProbs.push_back(deqnt_affine_u8_to_f32(max_score, score_zp, score_scale));
                classId.push_back(max_class_id);
                validCount++;
            }
        }
    }
    return validCount;
}

static int process_i8(int8_t *box_tensor, int32_t box_zp, float box_scale,
                      int8_t *score_tensor, int32_t score_zp, float score_scale,
                      int8_t *score_sum_tensor, int32_t score_sum_zp, float score_sum_scale,
                      int grid_h, int grid_w, int stride, int dfl_len,
                      std::vector<float> &boxes, 
                      std::vector<float> &objProbs, 
                      std::vector<int> &classId, 
                      float threshold)
{
    int validCount = 0;
    int grid_len = grid_h * grid_w;
    int8_t score_thres_i8 = qnt_f32_to_affine(threshold, score_zp, score_scale);

    for (int i = 0; i < grid_h; i++)
    {
        for (int j = 0; j < grid_w; j++)
        {
            int offset = i * grid_w + j;
            int max_class_id = -1;

            // Compute score sum dynamically since score_sum_tensor is not provided
            float score_sum = 0.0f;
            int score_offset = offset;
            for (int c = 0; c < OBJ_CLASS_NUM; c++)
            {
                float score_f32 = deqnt_affine_to_f32(score_tensor[score_offset], score_zp, score_scale);
                score_f32 = sigmoid(score_f32); // Apply sigmoid to transform logits to probabilities
                score_sum += score_f32;
                score_offset += grid_len;
            }
            score_sum = std::max(0.0f, std::min(1.0f, score_sum));
            if (score_sum < threshold)
            {
                continue;
            }

            int8_t max_score = -score_zp;
            score_offset = offset;
            for (int c = 0; c < OBJ_CLASS_NUM; c++)
            {
                if ((score_tensor[score_offset] > score_thres_i8) && (score_tensor[score_offset] > max_score))
                {
                    max_score = score_tensor[score_offset];
                    max_class_id = c;
                }
                score_offset += grid_len;
            }

            // Compute box
            float max_score_f32 = deqnt_affine_to_f32(max_score, score_zp, score_scale);
            max_score_f32 = sigmoid(max_score_f32); // Apply sigmoid to transform logits to probabilities
            if (max_score_f32 > threshold)
            {
                offset = i * grid_w + j;
                float box[4];
                float before_dfl[dfl_len * 4];
                for (int k = 0; k < dfl_len * 4; k++)
                {
                    before_dfl[k] = deqnt_affine_to_f32(box_tensor[offset], box_zp, box_scale);
                    offset += grid_len;
                }
                compute_dfl(before_dfl, dfl_len, box);

                float x1, y1, x2, y2, w, h;
                x1 = (-box[0] + j + 0.5) * stride;
                y1 = (-box[1] + i + 0.5) * stride;
                x2 = (box[2] + j + 0.5) * stride;
                y2 = (box[3] + i + 0.5) * stride;
                w = x2 - x1;
                h = y2 - y1;
                boxes.push_back(x1);
                boxes.push_back(y1);
                boxes.push_back(w);
                boxes.push_back(h);

                objProbs.push_back(max_score_f32);
                classId.push_back(max_class_id);
                validCount++;
            }
        }
    }
    return validCount;
}

static int process_fp32(float *box_tensor, float *score_tensor, float *score_sum_tensor, 
                        int grid_h, int grid_w, int stride, int dfl_len,
                        std::vector<float> &boxes, 
                        std::vector<float> &objProbs, 
                        std::vector<int> &classId, 
                        float threshold)
{
    int validCount = 0;
    int grid_len = grid_h * grid_w;
    for (int i = 0; i < grid_h; i++)
    {
        for (int j = 0; j < grid_w; j++)
        {
            int offset = i * grid_w + j;
            int max_class_id = -1;

            // Compute score sum dynamically
            float score_sum = 0.0f;
            int score_offset = offset;
            for (int c = 0; c < OBJ_CLASS_NUM; c++)
            {
                float score_f32 = score_tensor[score_offset];
                score_f32 = sigmoid(score_f32); // Apply sigmoid
                score_sum += score_f32;
                score_offset += grid_len;
            }
            score_sum = std::max(0.0f, std::min(1.0f, score_sum));
            if (score_sum < threshold)
            {
                continue;
            }

            float max_score = 0;
            score_offset = offset;
            for (int c = 0; c < OBJ_CLASS_NUM; c++)
            {
                float score_f32 = score_tensor[score_offset];
                score_f32 = sigmoid(score_f32); // Apply sigmoid
                if ((score_f32 > threshold) && (score_f32 > max_score))
                {
                    max_score = score_f32;
                    max_class_id = c;
                }
                score_offset += grid_len;
            }

            // Compute box
            if (max_score > threshold)
            {
                offset = i * grid_w + j;
                float box[4];
                float before_dfl[dfl_len * 4];
                for (int k = 0; k < dfl_len * 4; k++)
                {
                    before_dfl[k] = box_tensor[offset];
                    offset += grid_len;
                }
                compute_dfl(before_dfl, dfl_len, box);

                float x1, y1, x2, y2, w, h;
                x1 = (-box[0] + j + 0.5) * stride;
                y1 = (-box[1] + i + 0.5) * stride;
                x2 = (box[2] + j + 0.5) * stride;
                y2 = (box[3] + i + 0.5) * stride;
                w = x2 - x1;
                h = y2 - y1;
                boxes.push_back(x1);
                boxes.push_back(y1);
                boxes.push_back(w);
                boxes.push_back(h);

                objProbs.push_back(max_score);
                classId.push_back(max_class_id);
                validCount++;
            }
        }
    }
    return validCount;
}

#if defined(RV1106_1103)
static int process_i8_rv1106(int8_t *box_tensor, int32_t box_zp, float box_scale,
                             int8_t *score_tensor, int32_t score_zp, float score_scale,
                             int8_t *score_sum_tensor, int32_t score_sum_zp, float score_sum_scale,
                             int grid_h, int grid_w, int stride, int dfl_len,
                             std::vector<float> &boxes,
                             std::vector<float> &objProbs,
                             std::vector<int> &classId,
                             float threshold) {
    int validCount = 0;
    int grid_len = grid_h * grid_w;
    int8_t score_thres_i8 = qnt_f32_to_affine(threshold, score_zp, score_scale);

    for (int i = 0; i < grid_h; i++) {
        for (int j = 0; j < grid_w; j++) {
            int offset = i * grid_w + j;
            int max_class_id = -1;

            // Compute score sum dynamically
            float score_sum = 0.0f;
            int score_offset = offset * OBJ_CLASS_NUM;
            for (int c = 0; c < OBJ_CLASS_NUM; c++) {
                float score_f32 = deqnt_affine_to_f32(score_tensor[score_offset + c], score_zp, score_scale);
                score_f32 = sigmoid(score_f32); // Apply sigmoid
                score_sum += score_f32;
            }
            score_sum = std::max(0.0f, std::min(1.0f, score_sum));
            if (score_sum < threshold) {
                continue;
            }

            int8_t max_score = -score_zp;
            score_offset = offset * OBJ_CLASS_NUM;
            for (int c = 0; c < OBJ_CLASS_NUM; c++) {
                if ((score_tensor[score_offset + c] > score_thres_i8) && (score_tensor[score_offset + c] > max_score)) {
                    max_score = score_tensor[score_offset + c];
                    max_class_id = c;
                }
            }

            // Compute box
            float max_score_f32 = deqnt_affine_to_f32(max_score, score_zp, score_scale);
            max_score_f32 = sigmoid(max_score_f32); // Apply sigmoid
            if (max_score_f32 > threshold) {
                offset = (i * grid_w + j) * 4 * dfl_len;
                float box[4];
                float before_dfl[dfl_len * 4];
                for (int k = 0; k < dfl_len * 4; k++) {
                    before_dfl[k] = deqnt_affine_to_f32(box_tensor[offset + k], box_zp, box_scale);
                }
                compute_dfl(before_dfl, dfl_len, box);

                float x1, y1, x2, y2, w, h;
                x1 = (-box[0] + j + 0.5) * stride;
                y1 = (-box[1] + i + 0.5) * stride;
                x2 = (box[2] + j + 0.5) * stride;
                y2 = (box[3] + i + 0.5) * stride;
                w = x2 - x1;
                h = y2 - y1;
                boxes.push_back(x1);
                boxes.push_back(y1);
                boxes.push_back(w);
                boxes.push_back(h);

                objProbs.push_back(max_score_f32);
                classId.push_back(max_class_id);
                validCount++;
            }
        }
    }
    // printf("validCount=%d\n", validCount);
    // printf("grid h-%d, w-%d, stride %d\n", grid_h, grid_w, stride);
    return validCount;
}
#endif

int post_process(yolo11_app_context_t *app_ctx, void *outputs, letterbox_t *letter_box, float conf_threshold, float nms_threshold, object_detect_result_list *od_results) // <-- Use renamed struct
{
#if defined(RV1106_1103) 
    rknn_tensor_mem **_outputs = (rknn_tensor_mem **)outputs;
#else
    rknn_output *_outputs = (rknn_output *)outputs;
#endif
    std::vector<float> filterBoxes;
    std::vector<float> objProbs;
    std::vector<int> classId;
    int validCount = 0;
    int stride = 0;
    int grid_h = 0;
    int grid_w = 0;
    int model_in_w = app_ctx->model_width;
    int model_in_h = app_ctx->model_height;

    memset(od_results, 0, sizeof(object_detect_result_list));

    // default 3 branch
#ifdef RKNPU1
    int dfl_len = app_ctx->output_attrs[0].dims[2] / 4;
#else
    int dfl_len = app_ctx->output_attrs[0].dims[1] / 4;
#endif
    int output_per_branch = app_ctx->io_num.n_output / 3;
    for (int i = 0; i < 3; i++)
    {
#if defined(RV1106_1103)
        dfl_len = app_ctx->output_attrs[0].dims[3] / 4;
        void *score_sum = nullptr;
        int32_t score_sum_zp = 0;
        float score_sum_scale = 1.0;
        if (output_per_branch == 3) {
            score_sum = _outputs[i * output_per_branch + 2]->virt_addr;
            score_sum_zp = app_ctx->output_attrs[i * output_per_branch + 2].zp;
            score_sum_scale = app_ctx->output_attrs[i * output_per_branch + 2].scale;
        }
        int box_idx = i * output_per_branch;
        int score_idx = i * output_per_branch + 1;
        grid_h = app_ctx->output_attrs[box_idx].dims[1];
        grid_w = app_ctx->output_attrs[box_idx].dims[2];
        stride = model_in_h / grid_h;
        
        if (app_ctx->is_quant) {
            validCount += process_i8_rv1106((int8_t *)_outputs[box_idx]->virt_addr, app_ctx->output_attrs[box_idx].zp, app_ctx->output_attrs[box_idx].scale,
                                (int8_t *)_outputs[score_idx]->virt_addr, app_ctx->output_attrs[score_idx].zp,
                                app_ctx->output_attrs[score_idx].scale, (int8_t *)score_sum, score_sum_zp, score_sum_scale,
                                grid_h, grid_w, stride, dfl_len, filterBoxes, objProbs, classId, conf_threshold);
        }
        else
        {
            printf("RV1106/1103 only support quantization mode\n", LABEL_NALE_TXT_PATH);
            return -1;
        }

#else
        void *score_sum = nullptr;
        int32_t score_sum_zp = 0;
        float score_sum_scale = 1.0;
        if (output_per_branch == 3){
            score_sum = _outputs[i * output_per_branch + 2].buf;
            score_sum_zp = app_ctx->output_attrs[i * output_per_branch + 2].zp;
            score_sum_scale = app_ctx->output_attrs[i * output_per_branch + 2].scale;
        }
        int box_idx = i * output_per_branch;
        int score_idx = i * output_per_branch + 1;

#ifdef RKNPU1
        grid_h = app_ctx->output_attrs[box_idx].dims[1];
        grid_w = app_ctx->output_attrs[box_idx].dims[0];
#else
        grid_h = app_ctx->output_attrs[box_idx].dims[2];
        grid_w = app_ctx->output_attrs[box_idx].dims[3];
#endif
        stride = model_in_h / grid_h;

        if (app_ctx->is_quant)
        {
#ifdef RKNPU1
            validCount += process_u8((uint8_t *)_outputs[box_idx].buf, app_ctx->output_attrs[box_idx].zp, app_ctx->output_attrs[box_idx].scale,
                                     (uint8_t *)_outputs[score_idx].buf, app_ctx->output_attrs[score_idx].zp, app_ctx->output_attrs[score_idx].scale,
                                     (uint8_t *)score_sum, score_sum_zp, score_sum_scale,
                                     grid_h, grid_w, stride, dfl_len,
                                     filterBoxes, objProbs, classId, conf_threshold);
#else
            validCount += process_i8((int8_t *)_outputs[box_idx].buf, app_ctx->output_attrs[box_idx].zp, app_ctx->output_attrs[box_idx].scale,
                                     (int8_t *)_outputs[score_idx].buf, app_ctx->output_attrs[score_idx].zp, app_ctx->output_attrs[score_idx].scale,
                                     (int8_t *)score_sum, score_sum_zp, score_sum_scale,
                                     grid_h, grid_w, stride, dfl_len, 
                                     filterBoxes, objProbs, classId, conf_threshold);
#endif
        }
        else
        {
            validCount += process_fp32((float *)_outputs[box_idx].buf, (float *)_outputs[score_idx].buf, (float *)score_sum,
                                       grid_h, grid_w, stride, dfl_len, 
                                       filterBoxes, objProbs, classId, conf_threshold);
        }
#endif
    }

    // no object detect
    if (validCount <= 0)
    {
        return 0;
    }
    std::vector<int> indexArray;
    for (int i = 0; i < validCount; ++i)
    {
        indexArray.push_back(i);
    }
    quick_sort_indice_inverse(objProbs, 0, validCount - 1, indexArray);

    std::set<int> class_set(std::begin(classId), std::end(classId));

    for (auto c : class_set)
    {
        nms(validCount, filterBoxes, classId, indexArray, c, nms_threshold);
    }

    int last_count = 0;
    od_results->count = 0;

    /* box valid detect target */
    for (int i = 0; i < validCount; ++i)
    {
        if (indexArray[i] == -1 || last_count >= OBJ_NUMB_MAX_SIZE)
        {
            continue;
        }
        int n = indexArray[i];

        float x1 = filterBoxes[n * 4 + 0] - letter_box->x_pad;
        float y1 = filterBoxes[n * 4 + 1] - letter_box->y_pad;
        float x2 = x1 + filterBoxes[n * 4 + 2];
        float y2 = y1 + filterBoxes[n * 4 + 3];
        int id = classId[n];
        float obj_conf = objProbs[i];

        od_results->results[last_count].box.left = (int)(clamp(x1, 0, model_in_w) / letter_box->scale);
        od_results->results[last_count].box.top = (int)(clamp(y1, 0, model_in_h) / letter_box->scale);
        od_results->results[last_count].box.right = (int)(clamp(x2, 0, model_in_w) / letter_box->scale);
        od_results->results[last_count].box.bottom = (int)(clamp(y2, 0, model_in_h) / letter_box->scale);
        od_results->results[last_count].prop = obj_conf;
        od_results->results[last_count].cls_id = id;
        last_count++;
    }
    od_results->count = last_count;
    return 0;
}

int init_post_process()
{
    int ret = 0;
    ret = loadLabelName(LABEL_NALE_TXT_PATH, labels);
    if (ret < 0)
    {
        printf("Load %s failed!\n", LABEL_NALE_TXT_PATH);
        return -1;
    }
    return 0;
}

char *coco_cls_to_name(int cls_id)
{
    if (cls_id >= OBJ_CLASS_NUM)
    {
        return "null";
    }

    if (labels[cls_id])
    {
        return labels[cls_id];
    }

    return "null";
}

void deinit_post_process()
{
    for (int i = 0; i < OBJ_CLASS_NUM; i++)
    {
        if (labels[i] != nullptr)
        {
            free(labels[i]);
            labels[i] = nullptr;
        }
    }
}

# File: postprocess.h
#ifndef _RKNN_YOLO11_DEMO_POSTPROCESS_H_
#define _RKNN_YOLO11_DEMO_POSTPROCESS_H_

#include <stdint.h>
#include <vector>
#include "rknn_api.h"
#include "common.h"
#include "image_utils.h"

#define OBJ_NAME_MAX_SIZE 64
#define OBJ_NUMB_MAX_SIZE 128
#define OBJ_CLASS_NUM 4
#define NMS_THRESH 0.5
#define BOX_THRESH 0.6  // Lowered from 0.25 to 0.1 to be less strict

// class rknn_app_context_t;

typedef struct {
    image_rect_t box;
    float prop;
    int cls_id;
} object_detect_result;

typedef struct {
    int id;
    int count;
    object_detect_result results[OBJ_NUMB_MAX_SIZE];
} object_detect_result_list;

int init_post_process();
void deinit_post_process();
char *coco_cls_to_name(int cls_id);
// int post_process(rknn_app_context_t *app_ctx, void *outputs, letterbox_t *letter_box, float conf_threshold, float nms_threshold, object_detect_result_list *od_results);
int post_process(yolo11_app_context_t *app_ctx, void *outputs, letterbox_t *letter_box, float conf_threshold, float nms_threshold, object_detect_result_list *od_results); 

void deinitPostProcess();
#endif //_RKNN_YOLO11_DEMO_POSTPROCESS_H_



# File: yolo11.cc
#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#include <math.h>

#include "yolo11.h"
#include "common.h"
#include "file_utils.h"
#include "image_utils.h"

static void dump_tensor_attr(rknn_tensor_attr *attr)
{
    printf("");
    // printf("  index=%d, name=%s, n_dims=%d, dims=[%d, %d, %d, %d], n_elems=%d, size=%d, fmt=%s, type=%s, qnt_type=%s, "
    //        "zp=%d, scale=%f\n",
    //        attr->index, attr->name, attr->n_dims, attr->dims[0], attr->dims[1], attr->dims[2], attr->dims[3],
    //        attr->n_elems, attr->size, get_format_string(attr->fmt), get_type_string(attr->type),
    //        get_qnt_type_string(attr->qnt_type), attr->zp, attr->scale);
}


int init_yolo11(const char *model_path, yolo11_app_context_t *app_ctx)
{
    int ret;
    int model_len = 0;
    char *model_buf = NULL; // Use char* for model buffer
    rknn_context ctx = 0;

    // Ensure app_ctx is clean before starting
    memset(app_ctx, 0, sizeof(yolo11_app_context_t));

    // Load RKNN Model
    printf("Loading YOLO model: %s\n", model_path);
    model_len = read_data_from_file(model_path, &model_buf);
    if (model_buf == NULL) {
        printf("ERROR: load_model fail for YOLO!\n");
        return -1;
    }

    // Initialize RKNN context
    ret = rknn_init(&ctx, model_buf, model_len, 0, NULL);
    free(model_buf); // Free the buffer right after init
    model_buf = NULL;
    if (ret < 0) {
        printf("ERROR: rknn_init for YOLO model failed! ret=%d\n", ret);
        return -1; // Cannot proceed
    }

    // +++ Set Core Mask for YOLO Model +++
    ret = rknn_set_core_mask(ctx, RKNN_NPU_CORE_1); // Assign to Core 1
    if (ret < 0) {
        printf("ERROR: rknn_set_core_mask(yolo, CORE_1) failed! ret=%d\n", ret);
        rknn_destroy(ctx); // Clean up the initialized context before returning
        return -1; // Indicate failure
    }
    printf("INFO: YOLO Model assigned to NPU Core 1.\n");
    // +++++++++++++++++++++++++++++++++++++

    // --- Continue with querying model info ---
    app_ctx->rknn_ctx = ctx; // Store the context handle in our struct

    // Query I/O Number
    rknn_input_output_num io_num;
    ret = rknn_query(ctx, RKNN_QUERY_IN_OUT_NUM, &io_num, sizeof(io_num));
    if (ret != RKNN_SUCC) {
        printf("ERROR: rknn_query(Num) for YOLO failed! ret=%d\n", ret);
        rknn_destroy(ctx); // Cleanup
        return -1;
    }
    // printf("YOLO model input num: %d, output num: %d\n", io_num.n_input, io_num.n_output);
    app_ctx->io_num = io_num;

    // Query Input Attributes
    // printf("YOLO input tensors:\n");
    app_ctx->input_attrs = (rknn_tensor_attr *)malloc(io_num.n_input * sizeof(rknn_tensor_attr));
    if (!app_ctx->input_attrs) { printf("ERROR: malloc fail for yolo input attrs\n"); rknn_destroy(ctx); return -1; }
    memset(app_ctx->input_attrs, 0, io_num.n_input * sizeof(rknn_tensor_attr));
    for (uint32_t i = 0; i < io_num.n_input; i++) {
        app_ctx->input_attrs[i].index = i;
        ret = rknn_query(ctx, RKNN_QUERY_INPUT_ATTR, &(app_ctx->input_attrs[i]), sizeof(rknn_tensor_attr));
        if (ret != RKNN_SUCC) {
            printf("ERROR: rknn_query(Input %d) for YOLO failed! ret=%d\n", i, ret);
            free(app_ctx->input_attrs); // Free allocated memory
            rknn_destroy(ctx);
            return -1;
        }
        dump_tensor_attr(&(app_ctx->input_attrs[i]));
    }

    // Query Output Attributes
    // printf("YOLO output tensors:\n");
    app_ctx->output_attrs = (rknn_tensor_attr *)malloc(io_num.n_output * sizeof(rknn_tensor_attr));
    if (!app_ctx->output_attrs) { printf("ERROR: malloc fail for yolo output attrs\n"); free(app_ctx->input_attrs); rknn_destroy(ctx); return -1; }
    memset(app_ctx->output_attrs, 0, io_num.n_output * sizeof(rknn_tensor_attr));
    for (uint32_t i = 0; i < io_num.n_output; i++) {
        app_ctx->output_attrs[i].index = i;
        ret = rknn_query(ctx, RKNN_QUERY_OUTPUT_ATTR, &(app_ctx->output_attrs[i]), sizeof(rknn_tensor_attr));
        if (ret != RKNN_SUCC) {
            printf("ERROR: rknn_query(Output %d) for YOLO failed! ret=%d\n", i, ret);
            free(app_ctx->output_attrs); // Free allocated memory
            free(app_ctx->input_attrs);
            rknn_destroy(ctx);
            return -1;
        }
        dump_tensor_attr(&(app_ctx->output_attrs[i]));
    }

    // Determine quantization status (check output type and qnt_type)
    // Adjust this logic based on your specific YOLO model's quantization details
    if (app_ctx->output_attrs[0].qnt_type != RKNN_TENSOR_QNT_NONE &&
        (app_ctx->output_attrs[0].type == RKNN_TENSOR_INT8 || app_ctx->output_attrs[0].type == RKNN_TENSOR_UINT8)) {
        app_ctx->is_quant = true;
        printf("INFO: YOLO model appears to be quantized (output 0 type: %s, qnt: %s)\n",
               get_type_string(app_ctx->output_attrs[0].type),
               get_qnt_type_string(app_ctx->output_attrs[0].qnt_type));
    } else {
        app_ctx->is_quant = false;
         printf("INFO: YOLO model appears to be FP32/FP16 (output 0 type: %s, qnt: %s)\n",
               get_type_string(app_ctx->output_attrs[0].type),
               get_qnt_type_string(app_ctx->output_attrs[0].qnt_type));
    }

    // Determine input dimensions based on format
    if (app_ctx->input_attrs[0].fmt == RKNN_TENSOR_NCHW) {
        // printf("YOLO model is NCHW input fmt\n");
        app_ctx->model_channel = app_ctx->input_attrs[0].dims[1];
        app_ctx->model_height = app_ctx->input_attrs[0].dims[2];
        app_ctx->model_width = app_ctx->input_attrs[0].dims[3];
    } else { // Assume NHWC
        // printf("YOLO model is NHWC input fmt\n");
        app_ctx->model_height = app_ctx->input_attrs[0].dims[1];
        app_ctx->model_width = app_ctx->input_attrs[0].dims[2];
        app_ctx->model_channel = app_ctx->input_attrs[0].dims[3];
    }
    // printf("YOLO model input height=%d, width=%d, channel=%d\n",
    //        app_ctx->model_height, app_ctx->model_width, app_ctx->model_channel);

    return 0; // Success
}

int release_yolo11(yolo11_app_context_t *app_ctx) // <-- Renamed function and struct
{
    if (app_ctx->input_attrs != NULL)
    {
        free(app_ctx->input_attrs);
        app_ctx->input_attrs = NULL;
    }
    if (app_ctx->output_attrs != NULL)
    {
        free(app_ctx->output_attrs);
        app_ctx->output_attrs = NULL;
    }
    if (app_ctx->rknn_ctx != 0)
    {
        rknn_destroy(app_ctx->rknn_ctx);
        app_ctx->rknn_ctx = 0;
    }
    return 0;
}

int inference_yolo11(yolo11_app_context_t *app_ctx, image_buffer_t *img, object_detect_result_list *od_results) // <-- Renamed function and struct
{
    int ret;
    image_buffer_t dst_img;
    letterbox_t letter_box;
    rknn_input inputs[app_ctx->io_num.n_input];
    rknn_output outputs[app_ctx->io_num.n_output];
    const float nms_threshold = NMS_THRESH;      // 默认的NMS阈值
    const float box_conf_threshold = BOX_THRESH; // 默认的置信度阈值
    int bg_color = 114;

    if ((!app_ctx) || !(img) || (!od_results))
    {
        return -1;
    }

    memset(od_results, 0x00, sizeof(*od_results));
    memset(&letter_box, 0, sizeof(letterbox_t));
    memset(&dst_img, 0, sizeof(image_buffer_t));
    memset(inputs, 0, sizeof(inputs));
    memset(outputs, 0, sizeof(outputs));

    // Pre Process
    dst_img.width = app_ctx->model_width;
    dst_img.height = app_ctx->model_height;
    dst_img.format = IMAGE_FORMAT_RGB888;
    dst_img.size = get_image_size(&dst_img);
    dst_img.virt_addr = (unsigned char *)malloc(dst_img.size);
    if (dst_img.virt_addr == NULL)
    {
        // printf("malloc buffer size:%d fail!\n", dst_img.size);
        return -1;
    }

    // letterbox
    ret = convert_image_with_letterbox(img, &dst_img, &letter_box, bg_color);
    if (ret < 0)
    {
        // printf("convert_image_with_letterbox fail! ret=%d\n", ret);
        return -1;
    }

    // Set Input Data
    inputs[0].index = 0;
    inputs[0].type = RKNN_TENSOR_UINT8;
    inputs[0].fmt = RKNN_TENSOR_NHWC;
    inputs[0].size = app_ctx->model_width * app_ctx->model_height * app_ctx->model_channel;
    inputs[0].buf = dst_img.virt_addr;

    ret = rknn_inputs_set(app_ctx->rknn_ctx, app_ctx->io_num.n_input, inputs);
    if (ret < 0)
    {
        // printf("rknn_input_set fail! ret=%d\n", ret);
        return -1;
    }

    // Run
    printf("rknn_run\n");
    ret = rknn_run(app_ctx->rknn_ctx, nullptr);
    if (ret < 0)
    {
        // printf("rknn_run fail! ret=%d\n", ret);
        return -1;
    }

    // Get Output
    memset(outputs, 0, sizeof(outputs));
    for (int i = 0; i < app_ctx->io_num.n_output; i++)
    {
        outputs[i].index = i;
        outputs[i].want_float = (!app_ctx->is_quant);
    }
    ret = rknn_outputs_get(app_ctx->rknn_ctx, app_ctx->io_num.n_output, outputs, NULL);
    if (ret < 0)
    {
        // printf("rknn_outputs_get fail! ret=%d\n", ret);
        goto out;
    }

    // Post Process
    post_process(app_ctx, outputs, &letter_box, box_conf_threshold, nms_threshold, od_results);

    // Remeber to release rknn output
    rknn_outputs_release(app_ctx->rknn_ctx, app_ctx->io_num.n_output, outputs);

out:
    if (dst_img.virt_addr != NULL)
    {
        free(dst_img.virt_addr);
    }

    return ret;
}

# File: yolo11.cc
#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#include <math.h>

#include "yolo11.h"
#include "common.h"
#include "file_utils.h"
#include "image_utils.h"

static void dump_tensor_attr(rknn_tensor_attr *attr)
{
    printf("");
    // printf("  index=%d, name=%s, n_dims=%d, dims=[%d, %d, %d, %d], n_elems=%d, size=%d, fmt=%s, type=%s, qnt_type=%s, "
    //        "zp=%d, scale=%f\n",
    //        attr->index, attr->name, attr->n_dims, attr->dims[3], attr->dims[2], attr->dims[1], attr->dims[0],
    //        attr->n_elems, attr->size, get_format_string(attr->fmt), get_type_string(attr->type),
    //        get_qnt_type_string(attr->qnt_type), attr->zp, attr->scale);
}

int init_yolo11_model(const char *model_path, rknn_app_context_t *app_ctx)
{
    int ret;
    int model_len = 0;
    char *model;
    rknn_context ctx = 0;

    // Load RKNN Model
    model_len = read_data_from_file(model_path, &model);
    if (model == NULL)
    {
        printf("load_model fail!\n");
        return -1;
    }

    ret = rknn_init(&ctx, model, model_len, 0);
    free(model);
    if (ret < 0)
    {
        // printf("rknn_init fail! ret=%d\n", ret);
        return -1;
    }

    // Get Model Input Output Number
    rknn_input_output_num io_num;
    ret = rknn_query(ctx, RKNN_QUERY_IN_OUT_NUM, &io_num, sizeof(io_num));
    if (ret != RKNN_SUCC)
    {
        // printf("rknn_query fail! ret=%d\n", ret);
        return -1;
    }
    // printf("model input num: %d, output num: %d\n", io_num.n_input, io_num.n_output);

    // Get Model Input Info
    printf("input tensors:\n");
    rknn_tensor_attr input_attrs[io_num.n_input];
    memset(input_attrs, 0, sizeof(input_attrs));
    for (int i = 0; i < io_num.n_input; i++)
    {
        input_attrs[i].index = i;
        ret = rknn_query(ctx, RKNN_QUERY_INPUT_ATTR, &(input_attrs[i]), sizeof(rknn_tensor_attr));
        if (ret != RKNN_SUCC)
        {
            printf("rknn_query fail! ret=%d\n", ret);
            return -1;
        }
        dump_tensor_attr(&(input_attrs[i]));
    }

    // Get Model Output Info
    // printf("output tensors:\n");
    rknn_tensor_attr output_attrs[io_num.n_output];
    memset(output_attrs, 0, sizeof(output_attrs));
    for (int i = 0; i < io_num.n_output; i++)
    {
        output_attrs[i].index = i;
        ret = rknn_query(ctx, RKNN_QUERY_OUTPUT_ATTR, &(output_attrs[i]), sizeof(rknn_tensor_attr));
        if (ret != RKNN_SUCC)
        {
            // printf("rknn_query fail! ret=%d\n", ret);
            return -1;
        }
        dump_tensor_attr(&(output_attrs[i]));
    }

    // Set to context
    app_ctx->rknn_ctx = ctx;

    // TODO
    if (output_attrs[0].qnt_type == RKNN_TENSOR_QNT_AFFINE_ASYMMETRIC && output_attrs[0].type == RKNN_TENSOR_UINT8)
    {
        app_ctx->is_quant = true;
    }
    else
    {
        app_ctx->is_quant = false;
    }

    app_ctx->io_num = io_num;
    app_ctx->input_attrs = (rknn_tensor_attr *)malloc(io_num.n_input * sizeof(rknn_tensor_attr));
    memcpy(app_ctx->input_attrs, input_attrs, io_num.n_input * sizeof(rknn_tensor_attr));
    app_ctx->output_attrs = (rknn_tensor_attr *)malloc(io_num.n_output * sizeof(rknn_tensor_attr));
    memcpy(app_ctx->output_attrs, output_attrs, io_num.n_output * sizeof(rknn_tensor_attr));

    if (input_attrs[0].fmt == RKNN_TENSOR_NCHW)
    {
        printf("model is NCHW input fmt\n");
        app_ctx->model_channel = input_attrs[0].dims[2];
        app_ctx->model_height = input_attrs[0].dims[1];
        app_ctx->model_width = input_attrs[0].dims[0];
    }
    else
    {
        printf("model is NHWC input fmt\n");
        app_ctx->model_height = input_attrs[0].dims[2];
        app_ctx->model_width = input_attrs[0].dims[1];
        app_ctx->model_channel = input_attrs[0].dims[0];
    }
    // printf("model input height=%d, width=%d, channel=%d\n",
    //        app_ctx->model_height, app_ctx->model_width, app_ctx->model_channel);

    return 0;
}

int release_yolo11_model(rknn_app_context_t *app_ctx)
{
    if (app_ctx->input_attrs != NULL)
    {
        free(app_ctx->input_attrs);
        app_ctx->input_attrs = NULL;
    }
    if (app_ctx->output_attrs != NULL)
    {
        free(app_ctx->output_attrs);
        app_ctx->output_attrs = NULL;
    }
    if (app_ctx->rknn_ctx != 0)
    {
        rknn_destroy(app_ctx->rknn_ctx);
        app_ctx->rknn_ctx = 0;
    }
    return 0;
}

int inference_yolo11_model(rknn_app_context_t *app_ctx, image_buffer_t *img, object_detect_result_list *od_results)
{
    int ret;
    image_buffer_t dst_img;
    letterbox_t letter_box;
    rknn_input inputs[app_ctx->io_num.n_input];
    rknn_output outputs[app_ctx->io_num.n_output];
    const float nms_threshold = NMS_THRESH;      // Default NMS threshold
    const float box_conf_threshold = BOX_THRESH; // Default box threshold
    int bg_color = 114;

    if ((!app_ctx) || !(img) || (!od_results))
    {
        return -1;
    }

    memset(od_results, 0x00, sizeof(*od_results));
    memset(&letter_box, 0, sizeof(letterbox_t));
    memset(&dst_img, 0, sizeof(image_buffer_t));
    memset(inputs, 0, sizeof(inputs));
    memset(outputs, 0, sizeof(outputs));

    // Pre Process
    dst_img.width = app_ctx->model_width;
    dst_img.height = app_ctx->model_height;
    dst_img.format = IMAGE_FORMAT_RGB888;
    dst_img.size = get_image_size(&dst_img);
    dst_img.virt_addr = (unsigned char *)malloc(dst_img.size);
    if (dst_img.virt_addr == NULL)
    {
        printf("malloc buffer size:%d fail!\n", dst_img.size);
        return -1;
    }

    // letterbox
    ret = convert_image_with_letterbox(img, &dst_img, &letter_box, bg_color);
    if (ret < 0)
    {
        printf("convert_image_with_letterbox fail! ret=%d\n", ret);
        return -1;
    }

    // Set Input Data
    inputs[0].index = 0;
    inputs[0].type = RKNN_TENSOR_UINT8;
    inputs[0].fmt = RKNN_TENSOR_NHWC;
    inputs[0].size = app_ctx->model_width * app_ctx->model_height * app_ctx->model_channel;
    inputs[0].buf = dst_img.virt_addr;

    ret = rknn_inputs_set(app_ctx->rknn_ctx, app_ctx->io_num.n_input, inputs);
    if (ret < 0)
    {
        printf("rknn_input_set fail! ret=%d\n", ret);
        return -1;
    }

    // Run
    printf("rknn_run\n");
    ret = rknn_run(app_ctx->rknn_ctx, nullptr);
    if (ret < 0)
    {
        // printf("rknn_run fail! ret=%d\n", ret);
        return -1;
    }

    // Get Output
    memset(outputs, 0, sizeof(outputs));
    for (int i = 0; i < app_ctx->io_num.n_output; i++)
    {
        outputs[i].index = i;
        outputs[i].want_float = (!app_ctx->is_quant);
    }
    ret = rknn_outputs_get(app_ctx->rknn_ctx, app_ctx->io_num.n_output, outputs, NULL);
    if (ret < 0)
    {
        // printf("rknn_outputs_get fail! ret=%d\n", ret);
        goto out;
    }

    // Post Process
    post_process(app_ctx, outputs, &letter_box, box_conf_threshold, nms_threshold, od_results);

    // Remeber to release rknn output
    rknn_outputs_release(app_ctx->rknn_ctx, app_ctx->io_num.n_output, outputs);

out:
    if (dst_img.virt_addr != NULL)
    {
        free(dst_img.virt_addr);
    }

    return ret;
}

# File: face_analyzer.h
// // SSD mobilenet
// #ifndef RKNN_FACE_ANALYZER_H
// #define RKNN_FACE_ANALYZER_H

// #include "rknn_api.h"
// #include "common.h"
// #include <vector>

// // Constants remain the same
// #define MAX_FACE_RESULTS 64
// #define NUM_FACE_LANDMARKS 468
// #define NUM_EYE_CONTOUR_LANDMARKS 71
// #define NUM_IRIS_LANDMARKS 5

// // Structs point_t, box_rect_t, face_object_t, face_analyzer_result_t remain the same
// typedef struct point_t { int x; int y; } point_t;
// typedef struct box_rect_t { int left; int top; int right; int bottom; } box_rect_t;
// typedef struct face_object_t {
//     box_rect_t box;
//     float score;
//     point_t face_landmarks[NUM_FACE_LANDMARKS];
//     bool face_landmarks_valid;
//     point_t eye_landmarks_left[NUM_EYE_CONTOUR_LANDMARKS];
//     point_t iris_landmarks_left[NUM_IRIS_LANDMARKS];
//     bool eye_landmarks_left_valid;
//     bool iris_landmarks_left_valid;
//     point_t eye_landmarks_right[NUM_EYE_CONTOUR_LANDMARKS];
//     point_t iris_landmarks_right[NUM_IRIS_LANDMARKS];
//     bool eye_landmarks_right_valid;
//     bool iris_landmarks_right_valid;
// } face_object_t;
// typedef struct face_analyzer_result_t { int count; face_object_t faces[MAX_FACE_RESULTS]; } face_analyzer_result_t;


// // Context structure updated for Zero-Copy
// typedef struct face_analyzer_app_context_t {
//     // Model 1: Face Detection
//     rknn_context rknn_ctx_detect;
//     rknn_input_output_num io_num_detect;
//     rknn_tensor_attr *input_attrs_detect;
//     rknn_tensor_attr *output_attrs_detect;
//     rknn_tensor_mem **input_mems_detect;  // Array of input memory handles
//     rknn_tensor_mem **output_mems_detect; // Array of output memory handles
//     int model1_channel; int model1_width; int model1_height; rknn_tensor_format input_fmt_detect;

//     // Model 2: Face Landmarks
//     rknn_context rknn_ctx_landmark;
//     rknn_input_output_num io_num_landmark;
//     rknn_tensor_attr *input_attrs_landmark;
//     rknn_tensor_attr *output_attrs_landmark;
//     rknn_tensor_mem **input_mems_landmark;  // Array of input memory handles
//     rknn_tensor_mem **output_mems_landmark; // Array of output memory handles
//     int model2_channel; int model2_width; int model2_height; rknn_tensor_format input_fmt_landmark;

//     // Model 3: Iris Landmarks
//     rknn_context rknn_ctx_iris;
//     rknn_input_output_num io_num_iris;
//     rknn_tensor_attr *input_attrs_iris;
//     rknn_tensor_attr *output_attrs_iris;
//     rknn_tensor_mem **input_mems_iris;      // Array of input memory handles
//     rknn_tensor_mem **output_mems_iris;     // Array of output memory handles
//     int model3_channel; int model3_width; int model3_height; rknn_tensor_format input_fmt_iris;

// } face_analyzer_app_context_t;


// // Function prototypes remain the same
// int init_face_analyzer(const char *detection_model_path,
//                        const char* landmark_model_path,
//                        const char* iris_model_path,
//                        face_analyzer_app_context_t *app_ctx);

// int release_face_analyzer(face_analyzer_app_context_t *app_ctx);

// int inference_face_analyzer(face_analyzer_app_context_t *app_ctx,
//                             image_buffer_t *img,
//                             face_analyzer_result_t *out_result);

// #endif // RKNN_FACE_ANALYZER_H













// For retinaface
#ifndef RKNN_FACE_ANALYZER_H
#define RKNN_FACE_ANALYZER_H

#include "rknn_api.h"
#include "common.h"
#include <vector>

// Constants remain the same
#define MAX_FACE_RESULTS 64
#define NUM_FACE_LANDMARKS 468
#define NUM_EYE_CONTOUR_LANDMARKS 71
#define NUM_IRIS_LANDMARKS 5

// Structs point_t, box_rect_t, face_object_t, face_analyzer_result_t remain the same
typedef struct point_t { int x; int y; } point_t;
typedef struct box_rect_t { int left; int top; int right; int bottom; } box_rect_t;
typedef struct face_object_t {
    box_rect_t box;
    float score;
    point_t face_landmarks[NUM_FACE_LANDMARKS];
    bool face_landmarks_valid;
    point_t eye_landmarks_left[NUM_EYE_CONTOUR_LANDMARKS];
    point_t iris_landmarks_left[NUM_IRIS_LANDMARKS];
    bool eye_landmarks_left_valid;
    bool iris_landmarks_left_valid;
    point_t eye_landmarks_right[NUM_EYE_CONTOUR_LANDMARKS];
    point_t iris_landmarks_right[NUM_IRIS_LANDMARKS];
    bool eye_landmarks_right_valid;
    bool iris_landmarks_right_valid;
} face_object_t;
typedef struct face_analyzer_result_t { int count; face_object_t faces[MAX_FACE_RESULTS]; } face_analyzer_result_t;


// Context structure updated for Zero-Copy AND RetinaFace outputs
typedef struct face_analyzer_app_context_t {
    // Model 1: Face Detection (RetinaFace)
    rknn_context rknn_ctx_detect;
    rknn_input_output_num io_num_detect;
    rknn_tensor_attr *input_attrs_detect;
    rknn_tensor_attr *output_attrs_detect; // Will point to attributes for 3 tensors
    rknn_tensor_mem **input_mems_detect;   // Array of input memory handles (size 1)
    rknn_tensor_mem **output_mems_detect;  // Array of output memory handles (size 3) // MODIFIED SIZE
    int model1_channel; int model1_width; int model1_height; rknn_tensor_format input_fmt_detect;
    bool is_quant_detect; // Flag to know if RetinaFace model is quantized

    // Model 2: Face Landmarks
    rknn_context rknn_ctx_landmark;
    rknn_input_output_num io_num_landmark;
    rknn_tensor_attr *input_attrs_landmark;
    rknn_tensor_attr *output_attrs_landmark;
    rknn_tensor_mem **input_mems_landmark;
    rknn_tensor_mem **output_mems_landmark;
    int model2_channel; int model2_width; int model2_height; rknn_tensor_format input_fmt_landmark;

    // Model 3: Iris Landmarks
    rknn_context rknn_ctx_iris;
    rknn_input_output_num io_num_iris;
    rknn_tensor_attr *input_attrs_iris;
    rknn_tensor_attr *output_attrs_iris;
    rknn_tensor_mem **input_mems_iris;
    rknn_tensor_mem **output_mems_iris;
    int model3_channel; int model3_width; int model3_height; rknn_tensor_format input_fmt_iris;

} face_analyzer_app_context_t;


// Function prototypes remain the same
int init_face_analyzer(const char *detection_model_path, // Path will now be to RetinaFace.rknn
                       const char* landmark_model_path,
                       const char* iris_model_path,
                       face_analyzer_app_context_t *app_ctx);

int release_face_analyzer(face_analyzer_app_context_t *app_ctx);

int inference_face_analyzer(face_analyzer_app_context_t *app_ctx,
                            image_buffer_t *img,
                            face_analyzer_result_t *out_result);

#endif // RKNN_FACE_ANALYZER_H

# File: face_analyzer.cc
// // Mobilenet

// #include <stdio.h>
// #include <stdlib.h>
// #include <string.h>
// #include <math.h>
// #include <vector>
// #include <algorithm> // For std::min, std::max, std::sort
// #include <cmath>     // For expf, roundf

// // Include RKNN API first
// #include "rknn_api.h"

// // Include common utilities
// #include "common.h"      // For get_qnt_type_string etc. & letterbox_t
// #include "file_utils.h"  // For read_data_from_file
// #include "image_utils.h" // For image operations (crop, letterbox) & IMAGE_FORMAT_*

// // Include the header for this specific implementation last
// #include "face_analyzer.h"

// // --- Constants (Copied from previous version, adjust if needed) ---
// #define NMS_THRESHOLD 0.55
// #define FACE_CONF_THRESHOLD 0.5
// #define BOX_SCALE_X 1.5 // Scaling for face crop for face landmark model
// #define BOX_SCALE_Y 1.7 // Scaling for face crop for face landmark model
// #define EYE_CROP_SCALE 1.8 // Scaling factor for eye crop relative to eye landmarks
// #define IRIS_CROP_SCALE 2.8 // Scaling factor for iris crop relative to iris center (if needed, currently uses EYE_CROP_SCALE)

// // --- Anchor Generation Structs and Function (Copied from previous version) ---
// struct AnchorOptions {
//     int input_size_width = 128; int input_size_height = 128;
//     std::vector<int> strides = {8, 16}; int num_layers = 2;
//     std::vector<int> anchors_per_location = {2, 6};
//     float anchor_offset_x = 0.5f; float anchor_offset_y = 0.5f;
//     float fixed_anchor_size = 1.0f;
// };
// struct Anchor { float x_center, y_center, h, w; };

// static std::vector<Anchor> generate_anchors(const AnchorOptions& options) {
//     std::vector<Anchor> anchors;
//     int layer_id = 0;
//     while (layer_id < options.num_layers) {
//         int feature_map_height = ceil((float)options.input_size_height / options.strides[layer_id]);
//         int feature_map_width = ceil((float)options.input_size_width / options.strides[layer_id]);
//         int anchors_per_loc = options.anchors_per_location[layer_id];

//         for (int y = 0; y < feature_map_height; ++y) {
//             for (int x = 0; x < feature_map_width; ++x) {
//                 for (int anchor_id = 0; anchor_id < anchors_per_loc; ++anchor_id) {
//                     float x_center = (x + options.anchor_offset_x) * (1.0f / feature_map_width);
//                     float y_center = (y + options.anchor_offset_y) * (1.0f / feature_map_height);
//                     float w = options.fixed_anchor_size;
//                     float h = options.fixed_anchor_size;
//                     anchors.push_back({x_center, y_center, h, w});
//                 }
//             }
//         }
//         layer_id++;
//     }
//     // Ensure the number matches the model output (e.g., 896)
//     if (anchors.size() != 896) {
//          printf("WARN: Generated anchor count (%zu) != expected (896)!\n", anchors.size());
//          // Depending on the model, this might be an error or expected.
//          // If it's an error, return an empty vector or handle appropriately.
//     }
//     return anchors;
// }
// // --- End Anchor Generation ---


// // --- Utility Functions (Copied/Adapted) ---
// static inline float dequantize_int8_to_float(int8_t val, int32_t zp, float scale) {
//     return ((float)val - (float)zp) * scale;
// }

// static int clamp(int x, int min_val, int max_val) {
//     return std::max(min_val, std::min(x, max_val));
// }

// static float clampf(float x, float min_val, float max_val) {
//      return std::max(min_val, std::min(x, max_val));
// }

// static void dump_tensor_attr(rknn_tensor_attr *attr, const char* model_name) {
//     // Simple version - uncomment more details if needed
//     // printf("%s Idx:%d Name:%s Dims:[%d,%d,%d,%d] Fmt:%s Type:%s Qnt:%s ZP:%d Scale:%.3f\n",
//     //     model_name, attr->index, attr->name,
//     //     attr->n_dims > 0 ? attr->dims[0] : -1,
//     //     attr->n_dims > 1 ? attr->dims[1] : -1,
//     //     attr->n_dims > 2 ? attr->dims[2] : -1,
//     //     attr->n_dims > 3 ? attr->dims[3] : -1,
//     //     get_format_string(attr->fmt), get_type_string(attr->type),
//     //     get_qnt_type_string(attr->qnt_type), attr->zp, attr->scale);
// }

// // Simple crop (assumes continuous buffer, only works for specific formats)
// // Consider using a more robust version from image_utils if available/needed
// static int crop_image_simple(image_buffer_t *src_img, image_buffer_t *dst_img, box_rect_t crop_box) {
//     if (!src_img || !src_img->virt_addr || !dst_img) return -1;

//     int channels = 0;
//     if (src_img->format == IMAGE_FORMAT_RGB888) channels = 3;
// #ifdef IMAGE_FORMAT_BGR888
//     else if (src_img->format == IMAGE_FORMAT_BGR888) channels = 3;
// #endif
//     else return -1; // Only support RGB/BGR for this simple version

//     int src_w = src_img->width;
//     int src_h = src_img->height;
//     int crop_x = crop_box.left;
//     int crop_y = crop_box.top;
//     int crop_w = crop_box.right - crop_box.left;
//     int crop_h = crop_box.bottom - crop_box.top;

//     // Basic validation
//     if (crop_w <= 0 || crop_h <= 0 || crop_x >= src_w || crop_y >= src_h) {
//         printf("ERROR: Invalid crop ROI [%d, %d, %d, %d] for source image %dx%d\n",
//                crop_box.left, crop_box.top, crop_box.right, crop_box.bottom, src_w, src_h);
//         return -1;
//     }

//     // Adjust ROI to fit within source bounds
//     int x_start = std::max(0, crop_x);
//     int y_start = std::max(0, crop_y);
//     int x_end = std::min(src_w, crop_x + crop_w);
//     int y_end = std::min(src_h, crop_y + crop_h);

//     int valid_crop_w = x_end - x_start;
//     int valid_crop_h = y_end - y_start;

//     if (valid_crop_w <= 0 || valid_crop_h <= 0) {
//          printf("ERROR: Adjusted crop ROI has zero size.\n");
//          return -1;
//     }

//     dst_img->width = crop_w; // Destination retains original requested crop size
//     dst_img->height = crop_h;
//     dst_img->format = src_img->format;
//     dst_img->size = crop_w * crop_h * channels;

//     // Allocate destination buffer if not already allocated (or reallocate if size differs?)
//     // For simplicity, assume caller manages allocation or it's allocated once.
//     // If virt_addr is NULL, allocate it.
//     if (dst_img->virt_addr == NULL) {
//         dst_img->virt_addr = (unsigned char*)malloc(dst_img->size);
//         if (!dst_img->virt_addr) {
//             printf("ERROR: Failed to allocate memory for cropped image (%d bytes)\n", dst_img->size);
//             return -1;
//         }
//     } else if (dst_img->size < (size_t)(crop_w * crop_h * channels)) {
//          printf("ERROR: Provided destination buffer too small for crop.\n");
//          // Or realloc? For now, treat as error.
//          return -1;
//     }


//     // Clear destination buffer (e.g., black background)
//     memset(dst_img->virt_addr, 0, dst_img->size);

//     unsigned char* src_data = src_img->virt_addr;
//     unsigned char* dst_data = dst_img->virt_addr;
//     size_t src_stride = src_img->width_stride ? src_img->width_stride : (size_t)src_w * channels; // Use width_stride if available
//     size_t dst_stride = (size_t)crop_w * channels;

//     // Calculate where the valid source data should be placed in the destination
//     int dst_x_offset = x_start - crop_x; // Offset within the destination buffer
//     int dst_y_offset = y_start - crop_y;

//     // Copy valid region row by row
//     for (int y = 0; y < valid_crop_h; ++y) {
//         unsigned char* src_row_ptr = src_data + (size_t)(y_start + y) * src_stride + (size_t)x_start * channels;
//         unsigned char* dst_row_ptr = dst_data + (size_t)(dst_y_offset + y) * dst_stride + (size_t)dst_x_offset * channels;
//         memcpy(dst_row_ptr, src_row_ptr, (size_t)valid_crop_w * channels);
//     }

//     return 0;
// }


// static float CalculateOverlap(float xmin0, float ymin0, float xmax0, float ymax0, float xmin1, float ymin1, float xmax1, float ymax1) {
//     float w = fmax(0.f, fmin(xmax0, xmax1) - fmax(xmin0, xmin1)); // Width requires no +1 for IoU
//     float h = fmax(0.f, fmin(ymax0, ymax1) - fmax(ymin0, ymin1)); // Height requires no +1 for IoU
//     float intersection = w * h;
//     float area0 = (xmax0 - xmin0) * (ymax0 - ymin0);
//     float area1 = (xmax1 - xmin1) * (ymax1 - ymin1);
//     float union_area = area0 + area1 - intersection;
//     return union_area <= 0.f ? 0.f : (intersection / union_area);
// }

// struct DetectionCandidate {
//     int index;
//     float score;
//     box_rect_t box;
//     // Removed operator< as we sort using lambda now
// };

// static void nms(std::vector<DetectionCandidate>& candidates, float threshold) {
//     if (candidates.empty()) return;
//     // Sort candidates by score in descending order
//     std::sort(candidates.begin(), candidates.end(), [](const DetectionCandidate& a, const DetectionCandidate& b) {
//         return a.score > b.score;
//     });

//     std::vector<bool> removed(candidates.size(), false);
//     std::vector<DetectionCandidate> result_candidates;
//     result_candidates.reserve(candidates.size()); // Reserve space

//     for (size_t i = 0; i < candidates.size(); ++i) {
//         if (removed[i]) continue;
//         result_candidates.push_back(candidates[i]); // Keep this candidate

//         // Suppress overlapping boxes with lower scores
//         for (size_t j = i + 1; j < candidates.size(); ++j) {
//             if (removed[j]) continue;
//             float iou = CalculateOverlap(
//                 (float)candidates[i].box.left, (float)candidates[i].box.top,
//                 (float)candidates[i].box.right, (float)candidates[i].box.bottom,
//                 (float)candidates[j].box.left, (float)candidates[j].box.top,
//                 (float)candidates[j].box.right, (float)candidates[j].box.bottom
//             );
//             if (iou > threshold) {
//                 removed[j] = true;
//             }
//         }
//     }
//     candidates = std::move(result_candidates); // Efficiently replace with results
// }
// // --- End Utility Functions ---


// // --- Post Processing Functions (Modified for Zero-Copy) ---
// static int post_process_face_detection(
//     rknn_tensor_mem* output_mems[],
//     face_analyzer_app_context_t *app_ctx,
//     const letterbox_t *det_letter_box,
//     int src_img_width, int src_img_height,
//     std::vector<DetectionCandidate>& out_faces)
// {
//     AnchorOptions anchor_opts;
//     anchor_opts.input_size_width = app_ctx->model1_width;
//     anchor_opts.input_size_height = app_ctx->model1_height;
//     std::vector<Anchor> anchors = generate_anchors(anchor_opts);

//     if (app_ctx->io_num_detect.n_output < 2) { /* error */ return -1; }

//     rknn_tensor_attr* box_attr   = &app_ctx->output_attrs_detect[0];
//     rknn_tensor_attr* score_attr = &app_ctx->output_attrs_detect[1];

//     // Basic validation
//     if (box_attr->type != RKNN_TENSOR_INT8 || score_attr->type != RKNN_TENSOR_INT8 ||
//         box_attr->dims[1] != score_attr->dims[1]) // Check if anchor count matches
//     {
//         printf("ERROR: Detection output type/dims mismatch.\n");
//         return -1;
//     }

//     int num_anchors = box_attr->dims[1];
//     int box_regressors_per_anchor = box_attr->dims[box_attr->n_dims - 1]; // Usually 16

//     if (anchors.size() != num_anchors) { /* error */ return -1; }

//     int8_t* box_data   = (int8_t*)output_mems[0]->virt_addr;
//     int8_t* score_data = (int8_t*)output_mems[1]->virt_addr;
//     if (!box_data || !score_data) {
//         printf("ERROR: Output memory virtual address is NULL in post_process_face_detection.\n");
//         return -1; // Critical error
//     }

//     float box_scale   = box_attr->scale;
//     int32_t box_zp    = box_attr->zp;
//     float score_scale = score_attr->scale;
//     int32_t score_zp  = score_attr->zp;

//     std::vector<DetectionCandidate> candidates;
//     candidates.reserve(num_anchors / 4);

//     const float detection_size_w = (float)app_ctx->model1_width;
//     const float detection_size_h = (float)app_ctx->model1_height;

//     for (int i = 0; i < num_anchors; ++i) {
//         float logit = dequantize_int8_to_float(score_data[i], score_zp, score_scale);
//         float score = 1.0f / (1.0f + expf(-logit));

//         if (score >= FACE_CONF_THRESHOLD) {
//             int box_offset = i * box_regressors_per_anchor;
//             float x_offset = dequantize_int8_to_float(box_data[box_offset + 0], box_zp, box_scale);
//             float y_offset = dequantize_int8_to_float(box_data[box_offset + 1], box_zp, box_scale);
//             float w_scale  = dequantize_int8_to_float(box_data[box_offset + 2], box_zp, box_scale);
//             float h_scale  = dequantize_int8_to_float(box_data[box_offset + 3], box_zp, box_scale);

//             const Anchor& anchor = anchors[i];
//             float pred_x_center = (x_offset / detection_size_w * anchor.w) + anchor.x_center;
//             float pred_y_center = (y_offset / detection_size_h * anchor.h) + anchor.y_center;
//             float pred_w = (w_scale / detection_size_w) * anchor.w;
//             float pred_h = (h_scale / detection_size_h) * anchor.h;

//             float ymin_norm = pred_y_center - pred_h / 2.0f;
//             float xmin_norm = pred_x_center - pred_w / 2.0f;
//             float ymax_norm = pred_y_center + pred_h / 2.0f;
//             float xmax_norm = pred_x_center + pred_w / 2.0f;

//             ymin_norm = clampf(ymin_norm, 0.0f, 1.0f); xmin_norm = clampf(xmin_norm, 0.0f, 1.0f);
//             ymax_norm = clampf(ymax_norm, 0.0f, 1.0f); xmax_norm = clampf(xmax_norm, 0.0f, 1.0f);

//             float xmin_unscaled = (xmin_norm * detection_size_w - det_letter_box->x_pad) / det_letter_box->scale;
//             float ymin_unscaled = (ymin_norm * detection_size_h - det_letter_box->y_pad) / det_letter_box->scale;
//             float xmax_unscaled = (xmax_norm * detection_size_w - det_letter_box->x_pad) / det_letter_box->scale;
//             float ymax_unscaled = (ymax_norm * detection_size_h - det_letter_box->y_pad) / det_letter_box->scale;

//             int xmin = clamp((int)roundf(xmin_unscaled), 0, src_img_width - 1);
//             int ymin = clamp((int)roundf(ymin_unscaled), 0, src_img_height - 1);
//             int xmax = clamp((int)roundf(xmax_unscaled), 0, src_img_width - 1);
//             int ymax = clamp((int)roundf(ymax_unscaled), 0, src_img_height - 1);

//             if (xmax > xmin && ymax > ymin) {
//                 DetectionCandidate cand; cand.index = i; cand.score = score;
//                 cand.box.left = xmin; cand.box.top = ymin; cand.box.right = xmax; cand.box.bottom = ymax;
//                 candidates.push_back(cand);
//             }
//         }
//     }

//     nms(candidates, NMS_THRESHOLD);
//     out_faces = std::move(candidates);
//     return 0;
// }

// // Updated to read from rknn_tensor_mem* landmark_mem
// static int post_process_face_landmarks(
//     rknn_tensor_mem* landmark_mem,
//     face_analyzer_app_context_t *app_ctx,
//     const box_rect_t& crop_roi,
//     const letterbox_t *lmk_letter_box,
//     point_t out_landmarks[NUM_FACE_LANDMARKS])
// {
//     rknn_tensor_attr* landmark_attr = &app_ctx->output_attrs_landmark[0];
//     if (landmark_attr->type != RKNN_TENSOR_INT8) { /* error */ return -1; }

//     bool valid_dims = false; int num_elements = 0;
//     if (landmark_attr->n_dims == 4 && landmark_attr->dims[0] == 1) { num_elements = landmark_attr->dims[1] * landmark_attr->dims[2] * landmark_attr->dims[3]; valid_dims = (num_elements == NUM_FACE_LANDMARKS * 3); }
//     else if (landmark_attr->n_dims == 2 && landmark_attr->dims[0] == 1) { num_elements = landmark_attr->dims[1]; valid_dims = (num_elements == NUM_FACE_LANDMARKS * 3); }
//     if (!valid_dims) { /* error */ return -1; }

//     int8_t* landmark_data = (int8_t*)landmark_mem->virt_addr;
//     if (!landmark_data) { printf("ERROR: Landmark output virt_addr is NULL.\n"); return -1;}

//     float landmark_scale = landmark_attr->scale;
//     int32_t landmark_zp  = landmark_attr->zp;

//     float crop_roi_w = (float)(crop_roi.right - crop_roi.left);
//     float crop_roi_h = (float)(crop_roi.bottom - crop_roi.top);
//     if (crop_roi_w <= 0 || crop_roi_h <= 0) { /* error */ return -1; }

//     for (int i = 0; i < NUM_FACE_LANDMARKS; ++i) {
//         int offset = i * 3;
//         float x_lmk_model = dequantize_int8_to_float(landmark_data[offset + 0], landmark_zp, landmark_scale);
//         float y_lmk_model = dequantize_int8_to_float(landmark_data[offset + 1], landmark_zp, landmark_scale);

//         float x_unpadded = (x_lmk_model - lmk_letter_box->x_pad);
//         float y_unpadded = (y_lmk_model - lmk_letter_box->y_pad);
//         float x_orig_crop_rel = x_unpadded / lmk_letter_box->scale;
//         float y_orig_crop_rel = y_unpadded / lmk_letter_box->scale;
//         float final_x = x_orig_crop_rel + crop_roi.left;
//         float final_y = y_orig_crop_rel + crop_roi.top;

//         out_landmarks[i].x = (int)roundf(final_x);
//         out_landmarks[i].y = (int)roundf(final_y);
//     }
//     return 0;
// }

// // Updated to read from rknn_tensor_mem* iris_output_mems[]
// static int post_process_iris_landmarks(
//     rknn_tensor_mem* iris_output_mems[],
//     face_analyzer_app_context_t *app_ctx,
//     const box_rect_t& eye_crop_roi,
//     const letterbox_t *iris_letter_box,
//     point_t out_eye_contour[NUM_EYE_CONTOUR_LANDMARKS],
//     point_t out_iris[NUM_IRIS_LANDMARKS])
// {
//      if (app_ctx->io_num_iris.n_output < 2) { /* error */ return -1; }
//      rknn_tensor_attr* eye_contour_attr = &app_ctx->output_attrs_iris[0];
//      rknn_tensor_attr* iris_pts_attr    = &app_ctx->output_attrs_iris[1];

//      bool eye_dims_ok = (eye_contour_attr->n_dims == 4 && eye_contour_attr->n_elems == NUM_EYE_CONTOUR_LANDMARKS * 3) || (eye_contour_attr->n_dims == 2 && eye_contour_attr->n_elems == NUM_EYE_CONTOUR_LANDMARKS * 3);
//      bool iris_dims_ok = (iris_pts_attr->n_dims == 4 && iris_pts_attr->n_elems == NUM_IRIS_LANDMARKS * 3) || (iris_pts_attr->n_dims == 2 && iris_pts_attr->n_elems == NUM_IRIS_LANDMARKS * 3);
//      if (!eye_dims_ok || !iris_dims_ok || eye_contour_attr->type != RKNN_TENSOR_INT8 || iris_pts_attr->type != RKNN_TENSOR_INT8) { /* error */ return -1; }

//      int8_t* eye_data = (int8_t*)iris_output_mems[0]->virt_addr;
//      if (!eye_data) { printf("ERROR: Eye contour output virt_addr is NULL.\n"); return -1; }
//      float eye_scale  = eye_contour_attr->scale;
//      int32_t eye_zp   = eye_contour_attr->zp;

//      int8_t* iris_data = (int8_t*)iris_output_mems[1]->virt_addr;
//       if (!iris_data) { printf("ERROR: Iris points output virt_addr is NULL.\n"); return -1; }
//      float iris_scale = iris_pts_attr->scale;
//      int32_t iris_zp  = iris_pts_attr->zp;

//      float eye_crop_roi_w = (float)(eye_crop_roi.right - eye_crop_roi.left);
//      float eye_crop_roi_h = (float)(eye_crop_roi.bottom - eye_crop_roi.top);
//      if (eye_crop_roi_w <= 0 || eye_crop_roi_h <= 0) { /* error */ return -1; }

//      // Process Eye Contour Landmarks
//      for (int i = 0; i < NUM_EYE_CONTOUR_LANDMARKS; ++i) {
//          int offset = i * 3;
//          float x_eye_model = dequantize_int8_to_float(eye_data[offset + 0], eye_zp, eye_scale);
//          float y_eye_model = dequantize_int8_to_float(eye_data[offset + 1], eye_zp, eye_scale);

//          float x_unpadded = (x_eye_model - iris_letter_box->x_pad);
//          float y_unpadded = (y_eye_model - iris_letter_box->y_pad);
//          float x_orig_eye_rel = x_unpadded / iris_letter_box->scale;
//          float y_orig_eye_rel = y_unpadded / iris_letter_box->scale;
//          float final_x = x_orig_eye_rel + eye_crop_roi.left;
//          float final_y = y_orig_eye_rel + eye_crop_roi.top;
//          out_eye_contour[i].x = (int)roundf(final_x);
//          out_eye_contour[i].y = (int)roundf(final_y);
//      }

//     // Process Iris Center Landmarks
//      for (int i = 0; i < NUM_IRIS_LANDMARKS; ++i) {
//          int offset = i * 3;
//          float x_iris_model = dequantize_int8_to_float(iris_data[offset + 0], iris_zp, iris_scale);
//          float y_iris_model = dequantize_int8_to_float(iris_data[offset + 1], iris_zp, iris_scale);

//          float x_unpadded = (x_iris_model - iris_letter_box->x_pad);
//          float y_unpadded = (y_iris_model - iris_letter_box->y_pad);
//          float x_orig_eye_rel = x_unpadded / iris_letter_box->scale;
//          float y_orig_eye_rel = y_unpadded / iris_letter_box->scale;
//          float final_x = x_orig_eye_rel + eye_crop_roi.left;
//          float final_y = y_orig_eye_rel + eye_crop_roi.top;
//          out_iris[i].x = (int)roundf(final_x);
//          out_iris[i].y = (int)roundf(final_y);
//      }

//     return 0;
// }
// // --- End Post Processing Functions ---

// // // --- Model Initialization and Release ---
// int init_face_analyzer(const char *detection_model_path,
//                        const char* landmark_model_path,
//                        const char* iris_model_path,
//                        face_analyzer_app_context_t *app_ctx) {
//     memset(app_ctx, 0, sizeof(face_analyzer_app_context_t));
//     int ret;
//     int model_len = 0;
//     unsigned char *model_buf = NULL;

//     // --- Init Detect Model ---
//     printf("Init Face Detect Model: %s\n", detection_model_path);
//     model_len = read_data_from_file(detection_model_path, reinterpret_cast<char**>(&model_buf));
//     if (!model_buf) { printf("ERROR: read_data_from_file failed for detect model\n"); return -1; }
//     ret = rknn_init(&app_ctx->rknn_ctx_detect, model_buf, model_len, 0, NULL);
//     free(model_buf); model_buf = NULL;
//     if (ret < 0) { printf("ERROR: rknn_init(detect) failed! ret=%d\n", ret); return -1; }
//     ret = rknn_set_core_mask(app_ctx->rknn_ctx_detect, RKNN_NPU_CORE_0);
//     if(ret < 0) { printf("WARN: rknn_set_core_mask(detect, CORE_0) failed! ret=%d\n", ret); }
//     else { printf("INFO: Face Detection Model assigned to NPU Core 0.\n"); }
//     ret = rknn_query(app_ctx->rknn_ctx_detect, RKNN_QUERY_IN_OUT_NUM, &app_ctx->io_num_detect, sizeof(app_ctx->io_num_detect));
//     if (ret != RKNN_SUCC) { goto cleanup_detect; }
//     app_ctx->input_attrs_detect = (rknn_tensor_attr*)malloc(app_ctx->io_num_detect.n_input * sizeof(rknn_tensor_attr));
//     if (!app_ctx->input_attrs_detect) { ret = RKNN_ERR_MALLOC_FAIL; goto cleanup_detect; }
//     memset(app_ctx->input_attrs_detect, 0, app_ctx->io_num_detect.n_input * sizeof(rknn_tensor_attr));
//     for (uint32_t i = 0; i < app_ctx->io_num_detect.n_input; i++) { app_ctx->input_attrs_detect[i].index = i; ret = rknn_query(app_ctx->rknn_ctx_detect, RKNN_QUERY_NATIVE_INPUT_ATTR, &app_ctx->input_attrs_detect[i], sizeof(rknn_tensor_attr)); if (ret != RKNN_SUCC) goto cleanup_detect; dump_tensor_attr(&app_ctx->input_attrs_detect[i], "DetectIn(Native)"); }
//     app_ctx->output_attrs_detect = (rknn_tensor_attr*)malloc(app_ctx->io_num_detect.n_output * sizeof(rknn_tensor_attr));
//     if (!app_ctx->output_attrs_detect) { ret = RKNN_ERR_MALLOC_FAIL; goto cleanup_detect; }
//     memset(app_ctx->output_attrs_detect, 0, app_ctx->io_num_detect.n_output * sizeof(rknn_tensor_attr));
//     for (uint32_t i = 0; i < app_ctx->io_num_detect.n_output; i++) { app_ctx->output_attrs_detect[i].index = i; ret = rknn_query(app_ctx->rknn_ctx_detect, RKNN_QUERY_NATIVE_NHWC_OUTPUT_ATTR, &app_ctx->output_attrs_detect[i], sizeof(rknn_tensor_attr)); if (ret != RKNN_SUCC) goto cleanup_detect; dump_tensor_attr(&app_ctx->output_attrs_detect[i], "DetectOut(Native)"); }
//     if (app_ctx->io_num_detect.n_input > 0) { app_ctx->input_attrs_detect[0].type = RKNN_TENSOR_UINT8; app_ctx->input_attrs_detect[0].fmt = RKNN_TENSOR_NHWC; app_ctx->input_fmt_detect = RKNN_TENSOR_NHWC; }
//     app_ctx->input_mems_detect = (rknn_tensor_mem**)malloc(sizeof(rknn_tensor_mem*) * app_ctx->io_num_detect.n_input);
//     if(!app_ctx->input_mems_detect){ ret = RKNN_ERR_MALLOC_FAIL; goto cleanup_detect; } memset(app_ctx->input_mems_detect, 0, sizeof(rknn_tensor_mem*) * app_ctx->io_num_detect.n_input);
//     for(uint32_t i = 0; i < app_ctx->io_num_detect.n_input; ++i) { app_ctx->input_mems_detect[i] = rknn_create_mem(app_ctx->rknn_ctx_detect, app_ctx->input_attrs_detect[i].size_with_stride); if(!app_ctx->input_mems_detect[i]) { ret = RKNN_ERR_MALLOC_FAIL; goto cleanup_detect; } ret = rknn_set_io_mem(app_ctx->rknn_ctx_detect, app_ctx->input_mems_detect[i], &app_ctx->input_attrs_detect[i]); if (ret < 0) goto cleanup_detect; }
//     app_ctx->output_mems_detect = (rknn_tensor_mem**)malloc(sizeof(rknn_tensor_mem*) * app_ctx->io_num_detect.n_output);
//     if(!app_ctx->output_mems_detect){ ret = RKNN_ERR_MALLOC_FAIL; goto cleanup_detect; } memset(app_ctx->output_mems_detect, 0, sizeof(rknn_tensor_mem*) * app_ctx->io_num_detect.n_output);
//     for(uint32_t i = 0; i < app_ctx->io_num_detect.n_output; ++i) { app_ctx->output_mems_detect[i] = rknn_create_mem(app_ctx->rknn_ctx_detect, app_ctx->output_attrs_detect[i].size_with_stride); if(!app_ctx->output_mems_detect[i]) { ret = RKNN_ERR_MALLOC_FAIL; goto cleanup_detect; } ret = rknn_set_io_mem(app_ctx->rknn_ctx_detect, app_ctx->output_mems_detect[i], &app_ctx->output_attrs_detect[i]); if (ret < 0) goto cleanup_detect; }
//     app_ctx->model1_height = app_ctx->input_attrs_detect[0].dims[1]; app_ctx->model1_width = app_ctx->input_attrs_detect[0].dims[2]; app_ctx->model1_channel = app_ctx->input_attrs_detect[0].dims[3];

//     // --- Init Landmark Model ---
//     printf("Init Face Landmark Model: %s\n", landmark_model_path);
//     model_len = read_data_from_file(landmark_model_path, reinterpret_cast<char**>(&model_buf));
//     if (!model_buf) { ret = RKNN_ERR_MODEL_INVALID; goto cleanup_detect; }
//     ret = rknn_init(&app_ctx->rknn_ctx_landmark, model_buf, model_len, 0, NULL);
//     free(model_buf); model_buf = NULL;
//     if (ret < 0) { goto cleanup_detect; }
//     ret = rknn_set_core_mask(app_ctx->rknn_ctx_landmark, RKNN_NPU_CORE_0);
//     if(ret < 0) { printf("WARN: rknn_set_core_mask(landmark, CORE_0) failed! ret=%d\n", ret); } else { printf("INFO: Face Landmark Model assigned to NPU Core 0.\n"); }
//     ret = rknn_query(app_ctx->rknn_ctx_landmark, RKNN_QUERY_IN_OUT_NUM, &app_ctx->io_num_landmark, sizeof(app_ctx->io_num_landmark));
//     if (ret != RKNN_SUCC) { goto cleanup_landmark; }
//     app_ctx->input_attrs_landmark = (rknn_tensor_attr*)malloc(app_ctx->io_num_landmark.n_input * sizeof(rknn_tensor_attr));
//     if (!app_ctx->input_attrs_landmark) { ret = RKNN_ERR_MALLOC_FAIL; goto cleanup_landmark; }
//     memset(app_ctx->input_attrs_landmark, 0, app_ctx->io_num_landmark.n_input * sizeof(rknn_tensor_attr));
//     for (uint32_t i = 0; i < app_ctx->io_num_landmark.n_input; i++) { app_ctx->input_attrs_landmark[i].index = i; ret = rknn_query(app_ctx->rknn_ctx_landmark, RKNN_QUERY_NATIVE_INPUT_ATTR, &app_ctx->input_attrs_landmark[i], sizeof(rknn_tensor_attr)); if (ret != RKNN_SUCC) goto cleanup_landmark; dump_tensor_attr(&app_ctx->input_attrs_landmark[i], "LmkIn(Native)"); }
//     app_ctx->output_attrs_landmark = (rknn_tensor_attr*)malloc(app_ctx->io_num_landmark.n_output * sizeof(rknn_tensor_attr));
//     if (!app_ctx->output_attrs_landmark) { ret = RKNN_ERR_MALLOC_FAIL; goto cleanup_landmark; }
//     memset(app_ctx->output_attrs_landmark, 0, app_ctx->io_num_landmark.n_output * sizeof(rknn_tensor_attr));
//     for (uint32_t i = 0; i < app_ctx->io_num_landmark.n_output; i++) { app_ctx->output_attrs_landmark[i].index = i; ret = rknn_query(app_ctx->rknn_ctx_landmark, RKNN_QUERY_NATIVE_NHWC_OUTPUT_ATTR, &app_ctx->output_attrs_landmark[i], sizeof(rknn_tensor_attr)); if (ret != RKNN_SUCC) goto cleanup_landmark; dump_tensor_attr(&app_ctx->output_attrs_landmark[i], "LmkOut(Native)"); }
//     if (app_ctx->io_num_landmark.n_input > 0) { app_ctx->input_attrs_landmark[0].type = RKNN_TENSOR_UINT8; app_ctx->input_attrs_landmark[0].fmt = RKNN_TENSOR_NHWC; app_ctx->input_fmt_landmark = RKNN_TENSOR_NHWC; }
//     app_ctx->input_mems_landmark = (rknn_tensor_mem**)malloc(sizeof(rknn_tensor_mem*) * app_ctx->io_num_landmark.n_input);
//     if(!app_ctx->input_mems_landmark){ ret = RKNN_ERR_MALLOC_FAIL; goto cleanup_landmark; } memset(app_ctx->input_mems_landmark, 0, sizeof(rknn_tensor_mem*) * app_ctx->io_num_landmark.n_input);
//     for(uint32_t i = 0; i < app_ctx->io_num_landmark.n_input; ++i) { app_ctx->input_mems_landmark[i] = rknn_create_mem(app_ctx->rknn_ctx_landmark, app_ctx->input_attrs_landmark[i].size_with_stride); if(!app_ctx->input_mems_landmark[i]) { ret = RKNN_ERR_MALLOC_FAIL; goto cleanup_landmark; } ret = rknn_set_io_mem(app_ctx->rknn_ctx_landmark, app_ctx->input_mems_landmark[i], &app_ctx->input_attrs_landmark[i]); if (ret < 0) goto cleanup_landmark; }
//     app_ctx->output_mems_landmark = (rknn_tensor_mem**)malloc(sizeof(rknn_tensor_mem*) * app_ctx->io_num_landmark.n_output);
//     if(!app_ctx->output_mems_landmark){ ret = RKNN_ERR_MALLOC_FAIL; goto cleanup_landmark; } memset(app_ctx->output_mems_landmark, 0, sizeof(rknn_tensor_mem*) * app_ctx->io_num_landmark.n_output);
//     for(uint32_t i = 0; i < app_ctx->io_num_landmark.n_output; ++i) { app_ctx->output_mems_landmark[i] = rknn_create_mem(app_ctx->rknn_ctx_landmark, app_ctx->output_attrs_landmark[i].size_with_stride); if(!app_ctx->output_mems_landmark[i]) { ret = RKNN_ERR_MALLOC_FAIL; goto cleanup_landmark; } ret = rknn_set_io_mem(app_ctx->rknn_ctx_landmark, app_ctx->output_mems_landmark[i], &app_ctx->output_attrs_landmark[i]); if (ret < 0) goto cleanup_landmark; }
//     app_ctx->model2_height = app_ctx->input_attrs_landmark[0].dims[1]; app_ctx->model2_width = app_ctx->input_attrs_landmark[0].dims[2]; app_ctx->model2_channel = app_ctx->input_attrs_landmark[0].dims[3];

//     // --- Init Iris Model ---
//     printf("Init Iris Landmark Model: %s\n", iris_model_path);
//     model_len = read_data_from_file(iris_model_path, reinterpret_cast<char**>(&model_buf));
//     if (!model_buf) { ret = RKNN_ERR_MODEL_INVALID; goto cleanup_landmark; }
//     ret = rknn_init(&app_ctx->rknn_ctx_iris, model_buf, model_len, 0, NULL);
//     free(model_buf); model_buf = NULL;
//     if (ret < 0) { goto cleanup_landmark; }
//     ret = rknn_set_core_mask(app_ctx->rknn_ctx_iris, RKNN_NPU_CORE_0);
//     if(ret < 0) { printf("WARN: rknn_set_core_mask(iris, CORE_0) failed! ret=%d\n", ret); } else { printf("INFO: Iris Landmark Model assigned to NPU Core 0.\n"); }
//     ret = rknn_query(app_ctx->rknn_ctx_iris, RKNN_QUERY_IN_OUT_NUM, &app_ctx->io_num_iris, sizeof(app_ctx->io_num_iris));
//     if (ret != RKNN_SUCC || app_ctx->io_num_iris.n_output < 2) { goto cleanup_iris; }
//     app_ctx->input_attrs_iris = (rknn_tensor_attr*)malloc(app_ctx->io_num_iris.n_input * sizeof(rknn_tensor_attr));
//     if (!app_ctx->input_attrs_iris) { ret = RKNN_ERR_MALLOC_FAIL; goto cleanup_iris; }
//     memset(app_ctx->input_attrs_iris, 0, app_ctx->io_num_iris.n_input * sizeof(rknn_tensor_attr));
//     for (uint32_t i = 0; i < app_ctx->io_num_iris.n_input; i++) { app_ctx->input_attrs_iris[i].index = i; ret = rknn_query(app_ctx->rknn_ctx_iris, RKNN_QUERY_NATIVE_INPUT_ATTR, &app_ctx->input_attrs_iris[i], sizeof(rknn_tensor_attr)); if (ret != RKNN_SUCC) goto cleanup_iris; dump_tensor_attr(&app_ctx->input_attrs_iris[i], "IrisIn(Native)"); }
//     app_ctx->output_attrs_iris = (rknn_tensor_attr*)malloc(app_ctx->io_num_iris.n_output * sizeof(rknn_tensor_attr));
//     if (!app_ctx->output_attrs_iris) { ret = RKNN_ERR_MALLOC_FAIL; goto cleanup_iris; }
//     memset(app_ctx->output_attrs_iris, 0, app_ctx->io_num_iris.n_output * sizeof(rknn_tensor_attr));
//     for (uint32_t i = 0; i < app_ctx->io_num_iris.n_output; i++) { app_ctx->output_attrs_iris[i].index = i; ret = rknn_query(app_ctx->rknn_ctx_iris, RKNN_QUERY_NATIVE_NHWC_OUTPUT_ATTR, &app_ctx->output_attrs_iris[i], sizeof(rknn_tensor_attr)); if (ret != RKNN_SUCC) goto cleanup_iris; dump_tensor_attr(&app_ctx->output_attrs_iris[i], "IrisOut(Native)"); }
//     if (app_ctx->io_num_iris.n_input > 0) { app_ctx->input_attrs_iris[0].type = RKNN_TENSOR_UINT8; app_ctx->input_attrs_iris[0].fmt = RKNN_TENSOR_NHWC; app_ctx->input_fmt_iris = RKNN_TENSOR_NHWC; }
//     app_ctx->input_mems_iris = (rknn_tensor_mem**)malloc(sizeof(rknn_tensor_mem*) * app_ctx->io_num_iris.n_input);
//     if(!app_ctx->input_mems_iris){ ret = RKNN_ERR_MALLOC_FAIL; goto cleanup_iris; } memset(app_ctx->input_mems_iris, 0, sizeof(rknn_tensor_mem*) * app_ctx->io_num_iris.n_input);
//     for(uint32_t i = 0; i < app_ctx->io_num_iris.n_input; ++i) { app_ctx->input_mems_iris[i] = rknn_create_mem(app_ctx->rknn_ctx_iris, app_ctx->input_attrs_iris[i].size_with_stride); if(!app_ctx->input_mems_iris[i]) { ret = RKNN_ERR_MALLOC_FAIL; goto cleanup_iris; } ret = rknn_set_io_mem(app_ctx->rknn_ctx_iris, app_ctx->input_mems_iris[i], &app_ctx->input_attrs_iris[i]); if (ret < 0) goto cleanup_iris; }
//     app_ctx->output_mems_iris = (rknn_tensor_mem**)malloc(sizeof(rknn_tensor_mem*) * app_ctx->io_num_iris.n_output);
//     if(!app_ctx->output_mems_iris){ ret = RKNN_ERR_MALLOC_FAIL; goto cleanup_iris; } memset(app_ctx->output_mems_iris, 0, sizeof(rknn_tensor_mem*) * app_ctx->io_num_iris.n_output);
//     for(uint32_t i = 0; i < app_ctx->io_num_iris.n_output; ++i) { app_ctx->output_mems_iris[i] = rknn_create_mem(app_ctx->rknn_ctx_iris, app_ctx->output_attrs_iris[i].size_with_stride); if(!app_ctx->output_mems_iris[i]) { ret = RKNN_ERR_MALLOC_FAIL; goto cleanup_iris; } ret = rknn_set_io_mem(app_ctx->rknn_ctx_iris, app_ctx->output_mems_iris[i], &app_ctx->output_attrs_iris[i]); if (ret < 0) goto cleanup_iris; }
//     app_ctx->model3_height = app_ctx->input_attrs_iris[0].dims[1]; app_ctx->model3_width = app_ctx->input_attrs_iris[0].dims[2]; app_ctx->model3_channel = app_ctx->input_attrs_iris[0].dims[3];

//     printf("INFO: Face Analyzer init successful.\n");
//     return 0; // Success

// // --- Cleanup Labels ---
// cleanup_iris:
//     if (app_ctx->rknn_ctx_iris != 0) rknn_destroy(app_ctx->rknn_ctx_iris);
//     if (app_ctx->input_attrs_iris) free(app_ctx->input_attrs_iris);
//     if (app_ctx->output_attrs_iris) free(app_ctx->output_attrs_iris);
//     if(app_ctx->input_mems_iris) { for(uint32_t i=0; i<app_ctx->io_num_iris.n_input; ++i) if(app_ctx->input_mems_iris[i]) rknn_destroy_mem(app_ctx->rknn_ctx_iris, app_ctx->input_mems_iris[i]); free(app_ctx->input_mems_iris); }
//     if(app_ctx->output_mems_iris) { for(uint32_t i=0; i<app_ctx->io_num_iris.n_output; ++i) if(app_ctx->output_mems_iris[i]) rknn_destroy_mem(app_ctx->rknn_ctx_iris, app_ctx->output_mems_iris[i]); free(app_ctx->output_mems_iris); }
//     // Fall through
// cleanup_landmark:
//     if (app_ctx->rknn_ctx_landmark != 0) rknn_destroy(app_ctx->rknn_ctx_landmark);
//     if (app_ctx->input_attrs_landmark) free(app_ctx->input_attrs_landmark);
//     if (app_ctx->output_attrs_landmark) free(app_ctx->output_attrs_landmark);
//     if(app_ctx->input_mems_landmark) { for(uint32_t i=0; i<app_ctx->io_num_landmark.n_input; ++i) if(app_ctx->input_mems_landmark[i]) rknn_destroy_mem(app_ctx->rknn_ctx_landmark, app_ctx->input_mems_landmark[i]); free(app_ctx->input_mems_landmark); }
//     if(app_ctx->output_mems_landmark) { for(uint32_t i=0; i<app_ctx->io_num_landmark.n_output; ++i) if(app_ctx->output_mems_landmark[i]) rknn_destroy_mem(app_ctx->rknn_ctx_landmark, app_ctx->output_mems_landmark[i]); free(app_ctx->output_mems_landmark); }
//     // Fall through
// cleanup_detect:
//     if (app_ctx->rknn_ctx_detect != 0) rknn_destroy(app_ctx->rknn_ctx_detect);
//     if (app_ctx->input_attrs_detect) free(app_ctx->input_attrs_detect);
//     if (app_ctx->output_attrs_detect) free(app_ctx->output_attrs_detect);
//     if(app_ctx->input_mems_detect) { for(uint32_t i=0; i<app_ctx->io_num_detect.n_input; ++i) if(app_ctx->input_mems_detect[i]) rknn_destroy_mem(app_ctx->rknn_ctx_detect, app_ctx->input_mems_detect[i]); free(app_ctx->input_mems_detect); }
//     if(app_ctx->output_mems_detect) { for(uint32_t i=0; i<app_ctx->io_num_detect.n_output; ++i) if(app_ctx->output_mems_detect[i]) rknn_destroy_mem(app_ctx->rknn_ctx_detect, app_ctx->output_mems_detect[i]); free(app_ctx->output_mems_detect); }
//     // Final cleanup
//     memset(app_ctx, 0, sizeof(face_analyzer_app_context_t));
//     printf("ERROR: init_face_analyzer failed during cleanup stage with ret=%d\n", ret);
//     return ret > 0 ? -ret : ret;
// }

// // --- Release Function (Zero-Copy Version) ---
// int release_face_analyzer(face_analyzer_app_context_t *app_ctx) {
//     // Release Iris resources
//     if(app_ctx->input_mems_iris) {
//         for(uint32_t i=0; i<app_ctx->io_num_iris.n_input; ++i) { if(app_ctx->input_mems_iris[i]) { rknn_destroy_mem(app_ctx->rknn_ctx_iris, app_ctx->input_mems_iris[i]); app_ctx->input_mems_iris[i] = NULL; } }
//         free(app_ctx->input_mems_iris); app_ctx->input_mems_iris = NULL;
//     }
//     if(app_ctx->output_mems_iris) {
//         for(uint32_t i=0; i<app_ctx->io_num_iris.n_output; ++i) { if(app_ctx->output_mems_iris[i]) { rknn_destroy_mem(app_ctx->rknn_ctx_iris, app_ctx->output_mems_iris[i]); app_ctx->output_mems_iris[i] = NULL; } }
//         free(app_ctx->output_mems_iris); app_ctx->output_mems_iris = NULL;
//     }
//     if (app_ctx->input_attrs_iris) { free(app_ctx->input_attrs_iris); app_ctx->input_attrs_iris = NULL; }
//     if (app_ctx->output_attrs_iris) { free(app_ctx->output_attrs_iris); app_ctx->output_attrs_iris = NULL; }
//     if (app_ctx->rknn_ctx_iris != 0) { rknn_destroy(app_ctx->rknn_ctx_iris); app_ctx->rknn_ctx_iris = 0; }

//     // Release Landmark resources
//      if(app_ctx->input_mems_landmark) {
//         for(uint32_t i=0; i<app_ctx->io_num_landmark.n_input; ++i) { if(app_ctx->input_mems_landmark[i]) { rknn_destroy_mem(app_ctx->rknn_ctx_landmark, app_ctx->input_mems_landmark[i]); app_ctx->input_mems_landmark[i] = NULL; } }
//         free(app_ctx->input_mems_landmark); app_ctx->input_mems_landmark = NULL;
//     }
//     if(app_ctx->output_mems_landmark) {
//         for(uint32_t i=0; i<app_ctx->io_num_landmark.n_output; ++i) { if(app_ctx->output_mems_landmark[i]) { rknn_destroy_mem(app_ctx->rknn_ctx_landmark, app_ctx->output_mems_landmark[i]); app_ctx->output_mems_landmark[i] = NULL; } }
//         free(app_ctx->output_mems_landmark); app_ctx->output_mems_landmark = NULL;
//     }
//     if (app_ctx->input_attrs_landmark) { free(app_ctx->input_attrs_landmark); app_ctx->input_attrs_landmark = NULL; }
//     if (app_ctx->output_attrs_landmark) { free(app_ctx->output_attrs_landmark); app_ctx->output_attrs_landmark = NULL; }
//     if (app_ctx->rknn_ctx_landmark != 0) { rknn_destroy(app_ctx->rknn_ctx_landmark); app_ctx->rknn_ctx_landmark = 0; }

//     // Release Detection resources
//      if(app_ctx->input_mems_detect) {
//         for(uint32_t i=0; i<app_ctx->io_num_detect.n_input; ++i) { if(app_ctx->input_mems_detect[i]) { rknn_destroy_mem(app_ctx->rknn_ctx_detect, app_ctx->input_mems_detect[i]); app_ctx->input_mems_detect[i] = NULL; } }
//         free(app_ctx->input_mems_detect); app_ctx->input_mems_detect = NULL;
//     }
//     if(app_ctx->output_mems_detect) {
//         for(uint32_t i=0; i<app_ctx->io_num_detect.n_output; ++i) { if(app_ctx->output_mems_detect[i]) { rknn_destroy_mem(app_ctx->rknn_ctx_detect, app_ctx->output_mems_detect[i]); app_ctx->output_mems_detect[i] = NULL; } }
//         free(app_ctx->output_mems_detect); app_ctx->output_mems_detect = NULL;
//     }
//     if (app_ctx->input_attrs_detect) { free(app_ctx->input_attrs_detect); app_ctx->input_attrs_detect = NULL; }
//     if (app_ctx->output_attrs_detect) { free(app_ctx->output_attrs_detect); app_ctx->output_attrs_detect = NULL; }
//     if (app_ctx->rknn_ctx_detect != 0) { rknn_destroy(app_ctx->rknn_ctx_detect); app_ctx->rknn_ctx_detect = 0; }

//     printf("INFO: Face Analyzer released successfully.\n");
//     return 0;
// }


// // --- Main Inference Function (Modified for Zero-Copy) ---
// int inference_face_analyzer(face_analyzer_app_context_t *app_ctx,
//                             image_buffer_t *src_img, // Source image (e.g., from camera)
//                             face_analyzer_result_t *out_result) {
//     int ret = 0;
//     memset(out_result, 0, sizeof(face_analyzer_result_t));
//     letterbox_t det_letter_box;
//     memset(&det_letter_box, 0, sizeof(letterbox_t));
//     int bg_color = 114;

//     // --- Stage 1: Face Detection ---
//     if (!app_ctx->input_mems_detect || !app_ctx->input_mems_detect[0] || !app_ctx->input_mems_detect[0]->virt_addr) {
//         printf("ERROR: Detection input memory not initialized.\n"); return -1;
//     }
//     image_buffer_t detect_input_buf_wrapper;
//     detect_input_buf_wrapper.width = app_ctx->model1_width;
//     detect_input_buf_wrapper.height = app_ctx->model1_height;
//     detect_input_buf_wrapper.format = IMAGE_FORMAT_RGB888; // Must match what letterbox outputs
//     detect_input_buf_wrapper.virt_addr = (unsigned char*)app_ctx->input_mems_detect[0]->virt_addr;
//     detect_input_buf_wrapper.size = app_ctx->input_mems_detect[0]->size;
//     detect_input_buf_wrapper.fd = app_ctx->input_mems_detect[0]->fd;
//     detect_input_buf_wrapper.width_stride = app_ctx->input_attrs_detect[0].w_stride;
//     detect_input_buf_wrapper.height_stride = app_ctx->input_attrs_detect[0].h_stride ? app_ctx->input_attrs_detect[0].h_stride : app_ctx->model1_height;

//     ret = convert_image_with_letterbox(src_img, &detect_input_buf_wrapper, &det_letter_box, bg_color);
//     if (ret < 0) { printf("ERROR: convert_image_with_letterbox (detect) failed! ret=%d\n", ret); return ret; }

//     // Sync input memory to device if rknn_mem_sync is available
// #ifdef RKNN_MEMORY_SYNC_TO_DEVICE
//     ret = rknn_mem_sync(app_ctx->rknn_ctx_detect, app_ctx->input_mems_detect[0], RKNN_MEMORY_SYNC_TO_DEVICE);
//     if (ret < 0) { printf("ERROR: rknn_mem_sync (detect input) failed! ret=%d\n", ret); return ret; }
// #else
//     // Fallback: No explicit sync, rely on driver default behavior
//     printf("INFO: rknn_mem_sync not available, skipping input sync for detection.\n");
// #endif

//     ret = rknn_run(app_ctx->rknn_ctx_detect, nullptr);
//     if (ret < 0) { printf("ERROR: rknn_run (detect) failed! ret=%d\n", ret); return ret; }

//     // Sync output memory from device if rknn_mem_sync is available
// #ifdef RKNN_MEMORY_SYNC_FROM_DEVICE
//     for (uint32_t i = 0; i < app_ctx->io_num_detect.n_output; ++i) {
//         ret = rknn_mem_sync(app_ctx->rknn_ctx_detect, app_ctx->output_mems_detect[i], RKNN_MEMORY_SYNC_FROM_DEVICE);
//         if (ret < 0) { printf("ERROR: rknn_mem_sync (detect output %u) failed! ret=%d\n", i, ret); return ret; }
//     }
// #else
//     // Fallback: No explicit sync, rely on driver default behavior
//     printf("INFO: rknn_mem_sync not available, skipping output sync for detection.\n");
// #endif

//     std::vector<DetectionCandidate> detected_faces;
//     ret = post_process_face_detection(app_ctx->output_mems_detect, app_ctx, &det_letter_box,
//                                       src_img->width, src_img->height, detected_faces);
//     if (ret < 0) { printf("ERROR: post_process_face_detection failed! ret=%d\n", ret); return ret; }

//     int num_faces_to_process = std::min((int)detected_faces.size(), MAX_FACE_RESULTS);
//     out_result->count = num_faces_to_process;
//     if (num_faces_to_process == 0) return 0;

//     // --- Setup Wrappers & Buffers for Loop ---
//     image_buffer_t landmark_input_buf_wrapper; // Wrapper for landmark NPU input
//     landmark_input_buf_wrapper.width = app_ctx->model2_width; 
//     landmark_input_buf_wrapper.height = app_ctx->model2_height; 
//     landmark_input_buf_wrapper.format = IMAGE_FORMAT_RGB888; 
//     landmark_input_buf_wrapper.virt_addr = (unsigned char*)app_ctx->input_mems_landmark[0]->virt_addr; 
//     landmark_input_buf_wrapper.size = app_ctx->input_mems_landmark[0]->size; 
//     landmark_input_buf_wrapper.fd = app_ctx->input_mems_landmark[0]->fd;
//     landmark_input_buf_wrapper.width_stride = app_ctx->input_attrs_landmark[0].w_stride; 
//     landmark_input_buf_wrapper.height_stride = app_ctx->input_attrs_landmark[0].h_stride ? app_ctx->input_attrs_landmark[0].h_stride : app_ctx->model2_height;

//     image_buffer_t iris_input_buf_wrapper; // Wrapper for iris NPU input
//     iris_input_buf_wrapper.width = app_ctx->model3_width; 
//     iris_input_buf_wrapper.height = app_ctx->model3_height; 
//     iris_input_buf_wrapper.format = IMAGE_FORMAT_RGB888; 
//     iris_input_buf_wrapper.virt_addr = (unsigned char*)app_ctx->input_mems_iris[0]->virt_addr; 
//     iris_input_buf_wrapper.size = app_ctx->input_mems_iris[0]->size; 
//     iris_input_buf_wrapper.fd = app_ctx->input_mems_iris[0]->fd;
//     iris_input_buf_wrapper.width_stride = app_ctx->input_attrs_iris[0].w_stride; 
//     iris_input_buf_wrapper.height_stride = app_ctx->input_attrs_iris[0].h_stride ? app_ctx->input_attrs_iris[0].h_stride : app_ctx->model3_height;

//     image_buffer_t cropped_img_cpu; 
//     memset(&cropped_img_cpu, 0, sizeof(image_buffer_t)); // Reusable temp CPU buffer

//     // --- Loop ---
//     for (int i = 0; i < num_faces_to_process; ++i) {
//         out_result->faces[i].box = detected_faces[i].box;
//         out_result->faces[i].score = detected_faces[i].score;
//         out_result->faces[i].face_landmarks_valid = false; 
//         out_result->faces[i].iris_landmarks_right_valid = false;

//         // --- Stage 2: Face Landmarks ---
//         letterbox_t lmk_letter_box; 
//         memset(&lmk_letter_box, 0, sizeof(letterbox_t));
//         box_rect_t face_crop_roi_lmk;
//         int center_x = (detected_faces[i].box.left + detected_faces[i].box.right) / 2; 
//         int center_y = (detected_faces[i].box.top + detected_faces[i].box.bottom) / 2; 
//         int box_w = detected_faces[i].box.right - detected_faces[i].box.left; 
//         int box_h = detected_faces[i].box.bottom - detected_faces[i].box.top; 
//         int crop_w_lmk = (int)(box_w * BOX_SCALE_X); 
//         int crop_h_lmk = (int)(box_h * BOX_SCALE_Y);
//         face_crop_roi_lmk.left = center_x - crop_w_lmk / 2; 
//         face_crop_roi_lmk.top = center_y - crop_h_lmk / 2; 
//         face_crop_roi_lmk.right = face_crop_roi_lmk.left + crop_w_lmk; 
//         face_crop_roi_lmk.bottom = face_crop_roi_lmk.top + crop_h_lmk;

//         cropped_img_cpu.virt_addr = NULL;
//         ret = crop_image_simple(src_img, &cropped_img_cpu, face_crop_roi_lmk);
//         if (ret < 0 || !cropped_img_cpu.virt_addr) { 
//             if(cropped_img_cpu.virt_addr) free(cropped_img_cpu.virt_addr); 
//             continue; 
//         }

//         ret = convert_image_with_letterbox(&cropped_img_cpu, &landmark_input_buf_wrapper, &lmk_letter_box, bg_color);
//         free(cropped_img_cpu.virt_addr); 
//         cropped_img_cpu.virt_addr = NULL;
//         if (ret < 0) { continue; }

//         // Sync landmark input memory to device if rknn_mem_sync is available
// #ifdef RKNN_MEMORY_SYNC_TO_DEVICE
//         ret = rknn_mem_sync(app_ctx->rknn_ctx_landmark, app_ctx->input_mems_landmark[0], RKNN_MEMORY_SYNC_TO_DEVICE);
//         if (ret < 0) { printf("ERROR: rknn_mem_sync (landmark input) failed! ret=%d\n", ret); continue; }
// #else
//         // Fallback: No explicit sync
//         printf("INFO: rknn_mem_sync not available, skipping input sync for landmarks.\n");
// #endif

//         ret = rknn_run(app_ctx->rknn_ctx_landmark, nullptr);
//         if (ret < 0) { continue; }

//         // Sync landmark output memory from device if rknn_mem_sync is available
// #ifdef RKNN_MEMORY_SYNC_FROM_DEVICE
//         ret = rknn_mem_sync(app_ctx->rknn_ctx_landmark, app_ctx->output_mems_landmark[0], RKNN_MEMORY_SYNC_FROM_DEVICE);
//         if (ret < 0) { printf("ERROR: rknn_mem_sync (landmark output) failed! ret=%d\n", ret); continue; }
// #else
//         // Fallback: No explicit sync
//         printf("INFO: rknn_mem_sync not available, skipping output sync for landmarks.\n");
// #endif

//         ret = post_process_face_landmarks(app_ctx->output_mems_landmark[0], app_ctx, face_crop_roi_lmk, &lmk_letter_box, out_result->faces[i].face_landmarks);
//         if (ret == 0) { out_result->faces[i].face_landmarks_valid = true; }
//         else { continue; } // Skip iris if landmarks failed

//         // --- Stage 3: Iris Landmarks ---
//         if (out_result->faces[i].face_landmarks_valid) {
//             letterbox_t iris_letter_box; // Reused for both eyes

//             // *** Define Eye ROI Landmark Indices HERE ***
//             const int LEFT_EYE_ROI_IDX1  = 33;  // Example: Left eye outer corner
//             const int LEFT_EYE_ROI_IDX2  = 133; // Example: Left eye inner corner
//             const int RIGHT_EYE_ROI_IDX1 = 362; // Example: Right eye inner corner
//             const int RIGHT_EYE_ROI_IDX2 = 263; // Example: Right eye outer corner
//             // *******************************************

//             // --- Process Left Eye ---
//             memset(&iris_letter_box, 0, sizeof(letterbox_t));
//             if (LEFT_EYE_ROI_IDX1 >= NUM_FACE_LANDMARKS || LEFT_EYE_ROI_IDX2 >= NUM_FACE_LANDMARKS) {
//                  printf("ERROR: Invalid LEFT eye landmark indices for ROI calculation.\n");
//             } else {
//                 point_t left_p1 = out_result->faces[i].face_landmarks[LEFT_EYE_ROI_IDX1];
//                 point_t left_p2 = out_result->faces[i].face_landmarks[LEFT_EYE_ROI_IDX2];
//                 int left_eye_cx = (left_p1.x + left_p2.x) / 2; 
//                 int left_eye_cy = (left_p1.y + left_p2.y) / 2;
//                 int left_eye_w = abs(left_p1.x - left_p2.x); 
//                 int left_eye_h = abs(left_p1.y - left_p2.y);
//                 int left_eye_size = (int)(std::max({left_eye_w, left_eye_h, 1}) * EYE_CROP_SCALE);
//                 box_rect_t left_eye_crop_roi;
//                 left_eye_crop_roi.left = left_eye_cx - left_eye_size / 2; 
//                 left_eye_crop_roi.top = left_eye_cy - left_eye_size / 2;
//                 left_eye_crop_roi.right = left_eye_crop_roi.left + left_eye_size; 
//                 left_eye_crop_roi.bottom = left_eye_crop_roi.top + left_eye_size;

//                 cropped_img_cpu.virt_addr = NULL;
//                 ret = crop_image_simple(src_img, &cropped_img_cpu, left_eye_crop_roi);
//                 if (ret == 0 && cropped_img_cpu.virt_addr) {
//                     ret = convert_image_with_letterbox(&cropped_img_cpu, &iris_input_buf_wrapper, &iris_letter_box, bg_color);
//                     free(cropped_img_cpu.virt_addr); 
//                     cropped_img_cpu.virt_addr = NULL;
//                     if (ret == 0) {
//                         // Sync iris input memory to device if rknn_mem_sync is available
// #ifdef RKNN_MEMORY_SYNC_TO_DEVICE
//                         ret = rknn_mem_sync(app_ctx->rknn_ctx_iris, app_ctx->input_mems_iris[0], RKNN_MEMORY_SYNC_TO_DEVICE);
//                         if (ret < 0) { printf("ERROR: rknn_mem_sync (iris input left) failed! ret=%d\n", ret); continue; }
// #else
//                         printf("INFO: rknn_mem_sync not available, skipping input sync for iris (left).\n");
// #endif

//                         ret = rknn_run(app_ctx->rknn_ctx_iris, nullptr);
//                         if (ret == 0) {
//                             // Sync iris output memory from device if rknn_mem_sync is available
// #ifdef RKNN_MEMORY_SYNC_FROM_DEVICE
//                             for (uint32_t j = 0; j < app_ctx->io_num_iris.n_output; ++j) {
//                                 ret = rknn_mem_sync(app_ctx->rknn_ctx_iris, app_ctx->output_mems_iris[j], RKNN_MEMORY_SYNC_FROM_DEVICE);
//                                 if (ret < 0) { printf("ERROR: rknn_mem_sync (iris output left %u) failed! ret=%d\n", j, ret); continue; }
//                             }
// #else
//                             printf("INFO: rknn_mem_sync not available, skipping output sync for iris (left).\n");
// #endif
//                             ret = post_process_iris_landmarks(app_ctx->output_mems_iris, app_ctx, left_eye_crop_roi, &iris_letter_box, 
//                                                               out_result->faces[i].eye_landmarks_left, out_result->faces[i].iris_landmarks_left);
//                             if (ret == 0) { 
//                                 out_result->faces[i].eye_landmarks_left_valid = true; 
//                                 out_result->faces[i].iris_landmarks_left_valid = true; 
//                             }
//                         }
//                     }
//                 } else { if (cropped_img_cpu.virt_addr) free(cropped_img_cpu.virt_addr); cropped_img_cpu.virt_addr = NULL;}
//             } // End left eye index check

//             // --- Process Right Eye ---
//             memset(&iris_letter_box, 0, sizeof(letterbox_t));
//             if (RIGHT_EYE_ROI_IDX1 >= NUM_FACE_LANDMARKS || RIGHT_EYE_ROI_IDX2 >= NUM_FACE_LANDMARKS) {
//                  printf("ERROR: Invalid RIGHT eye landmark indices for ROI calculation.\n");
//             } else {
//                 point_t right_p1 = out_result->faces[i].face_landmarks[RIGHT_EYE_ROI_IDX1];
//                 point_t right_p2 = out_result->faces[i].face_landmarks[RIGHT_EYE_ROI_IDX2];
//                 int right_eye_cx = (right_p1.x + right_p2.x) / 2; 
//                 int right_eye_cy = (right_p1.y + right_p2.y) / 2;
//                 int right_eye_w = abs(right_p1.x - right_p2.x); 
//                 int right_eye_h = abs(right_p1.y - right_p2.y);
//                 int right_eye_size = (int)(std::max({right_eye_w, right_eye_h, 1}) * EYE_CROP_SCALE);
//                 box_rect_t right_eye_crop_roi;
//                 right_eye_crop_roi.left = right_eye_cx - right_eye_size / 2; 
//                 right_eye_crop_roi.top = right_eye_cy - right_eye_size / 2;
//                 right_eye_crop_roi.right = right_eye_crop_roi.left + right_eye_size; 
//                 right_eye_crop_roi.bottom = right_eye_crop_roi.top + right_eye_size;

//                 cropped_img_cpu.virt_addr = NULL;
//                 ret = crop_image_simple(src_img, &cropped_img_cpu, right_eye_crop_roi);
//                 if (ret == 0 && cropped_img_cpu.virt_addr) {
//                     ret = convert_image_with_letterbox(&cropped_img_cpu, &iris_input_buf_wrapper, &iris_letter_box, bg_color);
//                     free(cropped_img_cpu.virt_addr); 
//                     cropped_img_cpu.virt_addr = NULL;
//                     if (ret == 0) {
//                         // Sync iris input memory to device if rknn_mem_sync is available
// #ifdef RKNN_MEMORY_SYNC_TO_DEVICE
//                         ret = rknn_mem_sync(app_ctx->rknn_ctx_iris, app_ctx->input_mems_iris[0], RKNN_MEMORY_SYNC_TO_DEVICE);
//                         if (ret < 0) { printf("ERROR: rknn_mem_sync (iris input right) failed! ret=%d\n", ret); continue; }
// #else
//                         printf("INFO: rknn_mem_sync not available, skipping input sync for iris (right).\n");
// #endif

//                         ret = rknn_run(app_ctx->rknn_ctx_iris, nullptr);
//                         if (ret == 0) {
//                             // Sync iris output memory from device if rknn_mem_sync is available
// #ifdef RKNN_MEMORY_SYNC_FROM_DEVICE
//                             for (uint32_t j = 0; j < app_ctx->io_num_iris.n_output; ++j) {
//                                 ret = rknn_mem_sync(app_ctx->rknn_ctx_iris, app_ctx->output_mems_iris[j], RKNN_MEMORY_SYNC_FROM_DEVICE);
//                                 if (ret < 0) { printf("ERROR: rknn_mem_sync (iris output right %u) failed! ret=%d\n", j, ret); continue; }
//                             }
// #else
//                             printf("INFO: rknn_mem_sync not available, skipping output sync for iris (right).\n");
// #endif
//                             ret = post_process_iris_landmarks(app_ctx->output_mems_iris, app_ctx, right_eye_crop_roi, &iris_letter_box, 
//                                                               out_result->faces[i].eye_landmarks_right, out_result->faces[i].iris_landmarks_right);
//                             if (ret == 0) { 
//                                 out_result->faces[i].eye_landmarks_right_valid = true; 
//                                 out_result->faces[i].iris_landmarks_right_valid = true; 
//                             }
//                         }
//                     }
//                 } else { if (cropped_img_cpu.virt_addr) free(cropped_img_cpu.virt_addr); cropped_img_cpu.virt_addr = NULL;}
//             } // End right eye index check

//         } // End if face landmarks valid
//     } // End loop over detected faces

//     // Cleanup any lingering temp buffer
//     if(cropped_img_cpu.virt_addr) free(cropped_img_cpu.virt_addr);

//     return 0; // Success
// }











// retinaface
#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#include <math.h>
#include <vector>
#include <algorithm> // For std::min, std::max, std::sort
#include <cmath>     // For expf, roundf, ldexpf, INFINITY, NAN

// Include RKNN API first
#include "rknn_api.h"

// Include common utilities
#include "common.h"      // For get_qnt_type_string etc. & letterbox_t
#include "file_utils.h"  // For read_data_from_file
#include "image_utils.h" // For image operations (crop, letterbox) & IMAGE_FORMAT_*
#include "rknn_box_priors.h" // For RetinaFace priors

// Include the header for this specific implementation last
#include "face_analyzer.h"

// --- Constants ---
#define RETINA_NMS_THRESHOLD 0.4f
#define RETINA_CONF_THRESHOLD 0.5f
#define RETINA_VIS_THRESHOLD 0.4f // Use this threshold for final boxes

// *** Landmark Crop Scaling (Use SYMMETRIC Scaling - TUNE THIS) ***
#define LANDMARK_CROP_SCALE 1.6f // Symmetric scale factor (Tune this: 1.5 - 1.8 common)

// *** RetinaFace Box Adjustment Factors (TUNE THESE) ***
// How much to reduce the height of the RetinaFace box, as a fraction (0.0 to ~0.3)
#define BOX_HEIGHT_REDUCTION_FACTOR 0.15f // Example: Reduce height by 15%
// How much of the height reduction applies to the top (0.0 to 1.0)
// 0.5 = reduce top and bottom equally
// 1.0 = reduce only from the top (move top boundary down)
// 0.0 = reduce only from the bottom (move bottom boundary up)
#define BOX_TOP_SHIFT_FACTOR 0.75f // Example: Apply 75% of reduction to the top

// *** Iris Crop Scaling ***
#define EYE_CROP_SCALE 1.8f


// --- Utility Functions ---
static inline float dequantize_int8_to_float(int8_t val, int32_t zp, float scale) {
    return ((float)val - (float)zp) * scale;
}
static inline float dequantize_uint8_to_float(uint8_t val, int32_t zp, float scale) {
    return ((float)val - (float)zp) * scale;
}

// FP16 conversion helper
#ifdef __ARM_FP16_FORMAT_IEEE
    typedef __fp16 fp16_t;
    static inline float fp16_to_fp32(fp16_t x) {
        return (float)x;
    }
#else
    typedef unsigned short fp16_t; // Fallback: Treat as raw bits
    static float fp16_to_fp32(fp16_t x) {
         int sign = (x >> 15) & 0x0001;
         int exponent = (x >> 10) & 0x001f;
         int mantissa = x & 0x03ff;
         float val;
         if (exponent == 0) { // Denormalized number
             val = ldexpf((float)mantissa / 1024.0f, -14);
         } else if (exponent == 31) { // Infinity or NaN
             val = (mantissa == 0) ? INFINITY : NAN;
         } else { // Normalized number
             val = ldexpf(1.0f + (float)mantissa / 1024.0f, exponent - 15);
         }
         return sign ? -val : val;
    }
#endif

static int clamp(int x, int min_val, int max_val) {
    return std::max(min_val, std::min(x, max_val));
}
static float clampf(float x, float min_val, float max_val) {
     return std::max(min_val, std::min(x, max_val));
}

static void dump_tensor_attr(rknn_tensor_attr *attr, const char* model_name) {
     printf("%s Idx:%d Name:%s Dims:[%d,%d,%d,%d] Fmt:%s Type:%s(%d) Qnt:%s ZP:%d Scale:%.3f Size:%u Stride:%u\n",
        model_name, attr->index, attr->name,
        attr->n_dims > 0 ? attr->dims[0] : -1,
        attr->n_dims > 1 ? attr->dims[1] : -1,
        attr->n_dims > 2 ? attr->dims[2] : -1,
        attr->n_dims > 3 ? attr->dims[3] : -1,
        get_format_string(attr->fmt), get_type_string(attr->type), attr->type, // Print type enum value
        get_qnt_type_string(attr->qnt_type), attr->zp, attr->scale,
        attr->size, attr->w_stride);
}

static int crop_image_simple(image_buffer_t *src_img, image_buffer_t *dst_img, box_rect_t crop_box) {
    if (!src_img || !src_img->virt_addr || !dst_img) return -1;

    int channels = 0;
    if (src_img->format == IMAGE_FORMAT_RGB888) {
        channels = 3;
    }
#ifdef IMAGE_FORMAT_BGR888 // Allow BGR if it's defined elsewhere
    else if (src_img->format == IMAGE_FORMAT_BGR888) {
        channels = 3;
    }
#endif
    else {
        printf("ERROR: crop_image_simple unsupported format %d\n", src_img->format);
        return -1; // Only support RGB/BGR for this simple version
    }

    int src_w = src_img->width;
    int src_h = src_img->height;
    int crop_x = crop_box.left;
    int crop_y = crop_box.top;
    int crop_w = crop_box.right - crop_box.left; // Use difference for width/height
    int crop_h = crop_box.bottom - crop_box.top;

    // Basic validation
    if (crop_w <= 0 || crop_h <= 0) {
        //printf("ERROR: Invalid crop ROI size (%dx%d) for face analyzer\n", crop_w, crop_h);
        return -1;
    }

    // Adjust ROI to fit within source bounds
    int x_start = std::max(0, crop_x);
    int y_start = std::max(0, crop_y);
    int x_end = std::min(src_w, crop_x + crop_w);
    int y_end = std::min(src_h, crop_y + crop_h);

    int valid_crop_w = x_end - x_start;
    int valid_crop_h = y_end - y_start;

    if (valid_crop_w <= 0 || valid_crop_h <= 0) {
         //printf("ERROR: Adjusted crop ROI has zero size.\n");
         return -1;
    }

    dst_img->width = crop_w; // Destination retains original requested crop size
    dst_img->height = crop_h;
    dst_img->format = src_img->format; // Keep original format
    dst_img->size = (size_t)crop_w * crop_h * channels;

    // Allocate destination buffer if not already allocated
    if (dst_img->virt_addr == NULL) {
        dst_img->virt_addr = (unsigned char*)malloc(dst_img->size);
        if (!dst_img->virt_addr) {
            printf("ERROR: Failed to allocate memory for cropped image (%zu bytes)\n", dst_img->size);
            return -1;
        }
    } else if (dst_img->size < (size_t)(crop_w * crop_h * channels)) {
         // Ensure buffer is large enough, realloc if needed (or handle as error)
         printf("ERROR: Provided destination buffer too small for crop.\n");
         // Example: Realloc (use with caution if buffer is shared/managed elsewhere)
         // unsigned char* new_addr = (unsigned char*)realloc(dst_img->virt_addr, dst_img->size);
         // if (!new_addr) { printf("ERROR: Failed realloc for crop\n"); return -1; }
         // dst_img->virt_addr = new_addr;
         return -1; // Treat as error for now
    }

    // Clear destination buffer (e.g., black background) for areas outside valid crop
    memset(dst_img->virt_addr, 0, dst_img->size);

    unsigned char* src_data = src_img->virt_addr;
    unsigned char* dst_data = dst_img->virt_addr;
    size_t src_stride = src_img->width_stride ? src_img->width_stride : (size_t)src_w * channels; // Use width_stride if available
    size_t dst_stride = (size_t)crop_w * channels;

    // Calculate where the valid source data should be placed in the destination
    int dst_x_offset = x_start - crop_x; // Offset within the destination buffer
    int dst_y_offset = y_start - crop_y;

    // Copy valid region row by row
    for (int y = 0; y < valid_crop_h; ++y) {
        unsigned char* src_row_ptr = src_data + (size_t)(y_start + y) * src_stride + (size_t)x_start * channels;
        unsigned char* dst_row_ptr = dst_data + (size_t)(dst_y_offset + y) * dst_stride + (size_t)dst_x_offset * channels;
        memcpy(dst_row_ptr, src_row_ptr, (size_t)valid_crop_w * channels);
    }

    return 0;
}


static int calculate_letterbox_rects(int src_w, int src_h, int dst_w, int dst_h, int allow_slight_change, letterbox_t* out_letterbox, image_rect_t* out_dst_box) {
     if (!out_letterbox || !out_dst_box || src_w <= 0 || src_h <= 0 || dst_w <= 0 || dst_h <= 0) { return -1; }
     float scale_w = (float)dst_w / src_w;
     float scale_h = (float)dst_h / src_h;
     float scale = std::min(scale_w, scale_h);
     int resize_w = (int)(src_w * scale);
     int resize_h = (int)(src_h * scale);

     if (allow_slight_change == 1) {
         // Align to 2 for simplicity, RGA might prefer 4 or 16 depending on format/HW
         if (resize_w % 2 != 0) resize_w--;
         if (resize_h % 2 != 0) resize_h--;
         if (resize_w <= 0 || resize_h <= 0) return -1; // Prevent zero size after alignment
     }

     int padding_w = dst_w - resize_w;
     int padding_h = dst_h - resize_h;

     out_dst_box->left = padding_w / 2;
     out_dst_box->top = padding_h / 2;
     // Ensure even alignment for padding start if needed (e.g., for YUV)
     if (out_dst_box->left % 2 != 0) out_dst_box->left--;
     if (out_dst_box->top % 2 != 0) out_dst_box->top--;

     out_dst_box->left = std::max(0, out_dst_box->left);
     out_dst_box->top = std::max(0, out_dst_box->top);

     // Calculate right/bottom based on aligned start and aligned size
     out_dst_box->right = out_dst_box->left + resize_w;
     out_dst_box->bottom = out_dst_box->top + resize_h;

     // Ensure right/bottom do not exceed destination dimensions
     out_dst_box->right = std::min(dst_w, out_dst_box->right);
     out_dst_box->bottom = std::min(dst_h, out_dst_box->bottom);

     // Recalculate effective width/height in destination buffer
     int effective_w = out_dst_box->right - out_dst_box->left;
     int effective_h = out_dst_box->bottom - out_dst_box->top;

     // Update scale based on the final effective size vs original source size
     // This helps if alignment changed the size slightly
     if (src_w > 0 && src_h > 0) {
         float final_scale_w = (float)effective_w / src_w;
         float final_scale_h = (float)effective_h / src_h;
         out_letterbox->scale = std::min(final_scale_w, final_scale_h); // More accurate scale
     } else {
         out_letterbox->scale = scale; // Fallback
     }

     out_letterbox->x_pad = out_dst_box->left;
     out_letterbox->y_pad = out_dst_box->top;

     return 0;
}

// --- RetinaFace Helpers ---
typedef struct { box_rect_t norm_box; float score; int index; } RetinaFaceDecodedCandidate;
static bool compareRetinaCandidates(const RetinaFaceDecodedCandidate& a, const RetinaFaceDecodedCandidate& b) { return a.score > b.score; }
static float calculate_normalized_overlap(const box_rect_t& box1, const box_rect_t& box2) {
    float xmin1 = box1.left / 10000.0f, ymin1 = box1.top / 10000.0f; float xmax1 = box1.right / 10000.0f, ymax1 = box1.bottom / 10000.0f;
    float xmin2 = box2.left / 10000.0f, ymin2 = box2.top / 10000.0f; float xmax2 = box2.right / 10000.0f, ymax2 = box2.bottom / 10000.0f;
    float inter_xmin = std::max(xmin1, xmin2); float inter_ymin = std::max(ymin1, ymin2);
    float inter_xmax = std::min(xmax1, xmax2); float inter_ymax = std::min(ymax1, ymax2);
    float inter_w = std::max(0.0f, inter_xmax - inter_xmin); float inter_h = std::max(0.0f, inter_ymax - inter_ymin);
    float inter_area = inter_w * inter_h;
    float area1 = (xmax1 - xmin1) * (ymax1 - ymin1); float area2 = (xmax2 - xmin2) * (ymax2 - ymin2);
    float union_area = area1 + area2 - inter_area;
    return (union_area <= 1e-6f) ? 0.0f : (inter_area / union_area);
}
static void nms_retina(std::vector<RetinaFaceDecodedCandidate>& candidates, float nms_threshold) {
     if (candidates.empty()) return;
     std::sort(candidates.begin(), candidates.end(), compareRetinaCandidates);
     std::vector<bool> removed(candidates.size(), false);
     std::vector<RetinaFaceDecodedCandidate> result_candidates;
     result_candidates.reserve(candidates.size());
    for (size_t i = 0; i < candidates.size(); ++i) {
        if (removed[i]) continue;
        result_candidates.push_back(candidates[i]);
        for (size_t j = i + 1; j < candidates.size(); ++j) {
            if (removed[j]) continue;
            float overlap = calculate_normalized_overlap(candidates[i].norm_box, candidates[j].norm_box);
            if (overlap > nms_threshold) {
                removed[j] = true;
            }
        }
    }
    candidates = std::move(result_candidates);
}

// --- Corrected Post Processing for RetinaFace Detection ---
static int post_process_retinaface_detection_only(
    rknn_tensor_mem* output_mems[], face_analyzer_app_context_t *app_ctx,
    const letterbox_t *det_letter_box, int src_img_width, int src_img_height,
    face_analyzer_result_t* out_result) {

    out_result->count = 0;
    if (!app_ctx || !output_mems || !out_result || app_ctx->io_num_detect.n_output < 3) {
        printf("Retina PP: Invalid args or output count < 3.\n"); return -1;
    }
    rknn_tensor_attr* loc_attr = &app_ctx->output_attrs_detect[0];
    rknn_tensor_attr* score_attr = &app_ctx->output_attrs_detect[1];
    // We don't use the 5-point landmark output (index 2) here, but ensure buffers exist
    void* loc_buf = output_mems[0]->virt_addr;
    void* score_buf = output_mems[1]->virt_addr;
    if (!loc_buf || !score_buf) {
        printf("Retina PP: Output buffer is NULL.\n"); return -1;
    }

    const float (*prior_ptr)[4] = nullptr;
    int num_priors = 0;
    if (app_ctx->model1_height == 320 && app_ctx->model1_width == 320) {
        num_priors = 4200; prior_ptr = BOX_PRIORS_320;
    } else if (app_ctx->model1_height == 640 && app_ctx->model1_width == 640) {
        num_priors = 16800; prior_ptr = BOX_PRIORS_640;
    } else {
        printf("Retina PP: Unsupported model size %dx%d. Only 320x320 or 640x640 priors defined.\n",
               app_ctx->model1_height, app_ctx->model1_width);
        return -1;
    }

    bool is_quant = app_ctx->is_quant_detect;
    std::vector<RetinaFaceDecodedCandidate> decoded_candidates;
    decoded_candidates.reserve(num_priors / 10); // Preallocate approximate size
    const float VARIANCES[2] = {0.1f, 0.2f};

    // --- Step 1: Filter and Decode ---
    for (int i = 0; i < num_priors; ++i) {
        float face_score;

        // Handle different score data types
        if (score_attr->type == RKNN_TENSOR_FLOAT16) {
            face_score = fp16_to_fp32(((fp16_t*)score_buf)[i * 2 + 1]);
        } else if (is_quant && score_attr->type == RKNN_TENSOR_INT8) {
            face_score = dequantize_int8_to_float(((int8_t*)score_buf)[i * 2 + 1], score_attr->zp, score_attr->scale);
        } else if (is_quant && score_attr->type == RKNN_TENSOR_UINT8) {
            face_score = dequantize_uint8_to_float(((uint8_t*)score_buf)[i * 2 + 1], score_attr->zp, score_attr->scale);
        } else if (score_attr->type == RKNN_TENSOR_FLOAT32){ // Explicitly check for FP32
             face_score = ((float*)score_buf)[i * 2 + 1];
        } else {
             printf("Retina PP: Unsupported score tensor type: %d\n", score_attr->type);
             face_score = 0.0f; // Skip if type is unknown
        }


        if (face_score >= RETINA_CONF_THRESHOLD) {
            RetinaFaceDecodedCandidate cand;
            cand.score = face_score;
            cand.index = i;

            // Decode location (box) - handle different types
            float dx, dy, dw, dh;
            if (loc_attr->type == RKNN_TENSOR_FLOAT16) {
                 dx = fp16_to_fp32(((fp16_t*)loc_buf)[i * 4 + 0]); dy = fp16_to_fp32(((fp16_t*)loc_buf)[i * 4 + 1]); dw = fp16_to_fp32(((fp16_t*)loc_buf)[i * 4 + 2]); dh = fp16_to_fp32(((fp16_t*)loc_buf)[i * 4 + 3]);
            } else if (is_quant && loc_attr->type == RKNN_TENSOR_INT8) {
                 dx = dequantize_int8_to_float(((int8_t*)loc_buf)[i * 4 + 0], loc_attr->zp, loc_attr->scale); dy = dequantize_int8_to_float(((int8_t*)loc_buf)[i * 4 + 1], loc_attr->zp, loc_attr->scale); dw = dequantize_int8_to_float(((int8_t*)loc_buf)[i * 4 + 2], loc_attr->zp, loc_attr->scale); dh = dequantize_int8_to_float(((int8_t*)loc_buf)[i * 4 + 3], loc_attr->zp, loc_attr->scale);
            } else if (is_quant && loc_attr->type == RKNN_TENSOR_UINT8) {
                 dx = dequantize_uint8_to_float(((uint8_t*)loc_buf)[i * 4 + 0], loc_attr->zp, loc_attr->scale); dy = dequantize_uint8_to_float(((uint8_t*)loc_buf)[i * 4 + 1], loc_attr->zp, loc_attr->scale); dw = dequantize_uint8_to_float(((uint8_t*)loc_buf)[i * 4 + 2], loc_attr->zp, loc_attr->scale); dh = dequantize_uint8_to_float(((uint8_t*)loc_buf)[i * 4 + 3], loc_attr->zp, loc_attr->scale);
            } else if (loc_attr->type == RKNN_TENSOR_FLOAT32) {
                 dx = ((float*)loc_buf)[i * 4 + 0]; dy = ((float*)loc_buf)[i * 4 + 1]; dw = ((float*)loc_buf)[i * 4 + 2]; dh = ((float*)loc_buf)[i * 4 + 3];
            } else {
                printf("Retina PP: Unsupported location tensor type: %d\n", loc_attr->type);
                continue; // Skip this candidate
            }


            // Apply prior and variance decoding
            float prior_cx = prior_ptr[i][0]; float prior_cy = prior_ptr[i][1];
            float prior_w  = prior_ptr[i][2]; float prior_h  = prior_ptr[i][3];
            float decoded_cx = dx * VARIANCES[0] * prior_w + prior_cx;
            float decoded_cy = dy * VARIANCES[0] * prior_h + prior_cy;
            float decoded_w  = expf(dw * VARIANCES[1]) * prior_w;
            float decoded_h  = expf(dh * VARIANCES[1]) * prior_h;
            float xmin_norm = decoded_cx - decoded_w * 0.5f;
            float ymin_norm = decoded_cy - decoded_h * 0.5f;
            float xmax_norm = decoded_cx + decoded_w * 0.5f;
            float ymax_norm = decoded_cy + decoded_h * 0.5f;

            // Store normalized coordinates (scaled by 10000 for integer storage)
            cand.norm_box.left   = (int)(clampf(xmin_norm, 0.0f, 1.0f) * 10000.0f);
            cand.norm_box.top    = (int)(clampf(ymin_norm, 0.0f, 1.0f) * 10000.0f);
            cand.norm_box.right  = (int)(clampf(xmax_norm, 0.0f, 1.0f) * 10000.0f);
            cand.norm_box.bottom = (int)(clampf(ymax_norm, 0.0f, 1.0f) * 10000.0f);

            decoded_candidates.push_back(cand);
        }
    } // End loop over priors

    // --- Step 2: NMS ---
    nms_retina(decoded_candidates, RETINA_NMS_THRESHOLD);

    // --- Step 3: Final Coordinate Transformation ---
    float model_in_w = (float)app_ctx->model1_width;
    float model_in_h = (float)app_ctx->model1_height;

    for (const auto& cand : decoded_candidates) {
        // Filter by final visibility threshold
        if (cand.score < RETINA_VIS_THRESHOLD) {
            continue;
        }

        // Convert normalized coordinates back to original image space
        float xmin_norm = cand.norm_box.left / 10000.0f; float ymin_norm = cand.norm_box.top / 10000.0f;
        float xmax_norm = cand.norm_box.right / 10000.0f; float ymax_norm = cand.norm_box.bottom / 10000.0f;
        float xmin_model = xmin_norm * model_in_w; float ymin_model = ymin_norm * model_in_h;
        float xmax_model = xmax_norm * model_in_w; float ymax_model = ymax_norm * model_in_h;
        float xmin_unpad = (xmin_model - det_letter_box->x_pad) / det_letter_box->scale;
        float ymin_unpad = (ymin_model - det_letter_box->y_pad) / det_letter_box->scale;
        float xmax_unpad = (xmax_model - det_letter_box->x_pad) / det_letter_box->scale;
        float ymax_unpad = (ymax_model - det_letter_box->y_pad) / det_letter_box->scale;

        if (out_result->count < MAX_FACE_RESULTS) {
            face_object_t* final_face = &out_result->faces[out_result->count];
            memset(final_face, 0, sizeof(face_object_t)); // Clear previous data
            final_face->box.left   = clamp((int)roundf(xmin_unpad), 0, src_img_width - 1);
            final_face->box.top    = clamp((int)roundf(ymin_unpad), 0, src_img_height - 1);
            final_face->box.right  = clamp((int)roundf(xmax_unpad), 0, src_img_width - 1);
            final_face->box.bottom = clamp((int)roundf(ymax_unpad), 0, src_img_height - 1);
            final_face->score      = cand.score;

            // Ensure box is valid before adding
            if (final_face->box.right > final_face->box.left && final_face->box.bottom > final_face->box.top) {
                out_result->count++;
            }
        } else {
            printf("WARN: Exceeded MAX_FACE_RESULTS (%d)\n", MAX_FACE_RESULTS);
            break; // Stop processing more faces
        }
    }
    // printf("RetinaFace Found %d faces after NMS.\n", out_result->count);
    return 0;
}


// --- post_process_face_landmarks (Keep original) ---
static int post_process_face_landmarks(
    rknn_tensor_mem* landmark_mem, face_analyzer_app_context_t *app_ctx,
    const box_rect_t& crop_roi, // Use box_rect_t consistently
    const letterbox_t *lmk_letter_box,
    point_t out_landmarks[NUM_FACE_LANDMARKS]) {

    rknn_tensor_attr* landmark_attr = &app_ctx->output_attrs_landmark[0];
    if (landmark_attr->type != RKNN_TENSOR_INT8) {
        printf("LmkPP: Wrong output type %d\n", landmark_attr->type); return -1;
    }

    bool valid_dims = false;
    int num_elements = 0;
    if (landmark_attr->n_dims >= 2) {
        num_elements = 1;
        for(uint32_t d=0; d<landmark_attr->n_dims; ++d) {
            num_elements *= landmark_attr->dims[d];
        }
        valid_dims = (num_elements == NUM_FACE_LANDMARKS * 3);
    }

    if (!valid_dims) {
        printf("LmkPP: Invalid dims/elems (%d elems) != %d\n", num_elements, NUM_FACE_LANDMARKS * 3);
        return -1;
    }

    int8_t* landmark_data = (int8_t*)landmark_mem->virt_addr;
    if (!landmark_data) {
        printf("LmkPP: Output virt_addr NULL\n"); return -1;
    }

    float landmark_scale = landmark_attr->scale;
    int32_t landmark_zp = landmark_attr->zp;

    float crop_roi_w = (float)(crop_roi.right - crop_roi.left);
    float crop_roi_h = (float)(crop_roi.bottom - crop_roi.top);
    if (crop_roi_w <= 0 || crop_roi_h <= 0) {
        printf("LmkPP: Invalid crop ROI %fx%f\n", crop_roi_w, crop_roi_h); return -1;
    }

    if (lmk_letter_box->scale <= 1e-6) { // Avoid division by zero or very small scale
         printf("LmkPP: Invalid letterbox scale %.6f\n", lmk_letter_box->scale); return -1;
    }

    for (int i = 0; i < NUM_FACE_LANDMARKS; ++i) {
        int offset = i * 3;
        if (offset + 1 >= num_elements) { // Only need x and y
            printf("LmkPP: Index out of bounds %d\n", offset+1); return -1;
        }
        float x_lmk_model = dequantize_int8_to_float(landmark_data[offset + 0], landmark_zp, landmark_scale);
        float y_lmk_model = dequantize_int8_to_float(landmark_data[offset + 1], landmark_zp, landmark_scale);
        // Z coordinate (index 2) is ignored here

        // Reverse letterbox
        float x_unpadded = (x_lmk_model - lmk_letter_box->x_pad);
        float y_unpadded = (y_lmk_model - lmk_letter_box->y_pad);

        // Scale back to original crop size
        float x_orig_crop_rel = x_unpadded / lmk_letter_box->scale;
        float y_orig_crop_rel = y_unpadded / lmk_letter_box->scale;

        // Translate to original image coordinate system
        float final_x = x_orig_crop_rel + crop_roi.left;
        float final_y = y_orig_crop_rel + crop_roi.top;

        out_landmarks[i].x = (int)roundf(final_x);
        out_landmarks[i].y = (int)roundf(final_y);
    }
    return 0;
}

// --- post_process_iris_landmarks (Keep original) ---
static int post_process_iris_landmarks(
    rknn_tensor_mem* iris_output_mems[], face_analyzer_app_context_t *app_ctx,
    const box_rect_t& eye_crop_roi, const letterbox_t *iris_letter_box,
    point_t out_eye_contour[NUM_EYE_CONTOUR_LANDMARKS], point_t out_iris[NUM_IRIS_LANDMARKS]) {

    if (app_ctx->io_num_iris.n_output < 2) { printf("IrisPP: Output count < 2\n"); return -1; }
    rknn_tensor_attr* eye_contour_attr = &app_ctx->output_attrs_iris[0];
    rknn_tensor_attr* iris_pts_attr = &app_ctx->output_attrs_iris[1];

    bool eye_dims_ok = (eye_contour_attr->n_elems == NUM_EYE_CONTOUR_LANDMARKS * 3);
    bool iris_dims_ok = (iris_pts_attr->n_elems == NUM_IRIS_LANDMARKS * 3);
    if (!eye_dims_ok || !iris_dims_ok || eye_contour_attr->type != RKNN_TENSOR_INT8 || iris_pts_attr->type != RKNN_TENSOR_INT8) {
        printf("IrisPP: Type/Dim mismatch (Eye %d/%d, Iris %d/%d, Types %d/%d)\n",
               eye_dims_ok, (int)eye_contour_attr->n_elems, iris_dims_ok, (int)iris_pts_attr->n_elems,
               eye_contour_attr->type, iris_pts_attr->type);
        return -1;
    }

    int8_t* eye_data = (int8_t*)iris_output_mems[0]->virt_addr;
    if (!eye_data) { printf("IrisPP: Eye virt addr NULL\n"); return -1; }
    float eye_scale = eye_contour_attr->scale;
    int32_t eye_zp = eye_contour_attr->zp;

    int8_t* iris_data = (int8_t*)iris_output_mems[1]->virt_addr;
    if (!iris_data) { printf("IrisPP: Iris virt addr NULL\n"); return -1; }
    float iris_scale = iris_pts_attr->scale;
    int32_t iris_zp = iris_pts_attr->zp;

    float eye_crop_roi_w = (float)(eye_crop_roi.right - eye_crop_roi.left);
    float eye_crop_roi_h = (float)(eye_crop_roi.bottom - eye_crop_roi.top);
    if (eye_crop_roi_w <= 0 || eye_crop_roi_h <= 0) { printf("IrisPP: Invalid crop ROI\n"); return -1; }

    if (iris_letter_box->scale <= 1e-6) { printf("IrisPP: Invalid letterbox scale %.6f\n", iris_letter_box->scale); return -1; }

    // Process Eye Contour Landmarks
    for (int i = 0; i < NUM_EYE_CONTOUR_LANDMARKS; ++i) {
        int offset = i * 3;
        if (offset + 1 >= eye_contour_attr->n_elems) { printf("IrisPP: Eye index OOB\n"); return -1;}
        float x_eye_model = dequantize_int8_to_float(eye_data[offset + 0], eye_zp, eye_scale);
        float y_eye_model = dequantize_int8_to_float(eye_data[offset + 1], eye_zp, eye_scale);
        float x_unpadded = (x_eye_model - iris_letter_box->x_pad);
        float y_unpadded = (y_eye_model - iris_letter_box->y_pad);
        float x_orig_eye_rel = x_unpadded / iris_letter_box->scale;
        float y_orig_eye_rel = y_unpadded / iris_letter_box->scale;
        float final_x = x_orig_eye_rel + eye_crop_roi.left;
        float final_y = y_orig_eye_rel + eye_crop_roi.top;
        out_eye_contour[i].x = (int)roundf(final_x);
        out_eye_contour[i].y = (int)roundf(final_y);
    }

    // Process Iris Center Landmarks
    for (int i = 0; i < NUM_IRIS_LANDMARKS; ++i) {
        int offset = i * 3;
        if (offset + 1 >= iris_pts_attr->n_elems) { printf("IrisPP: Iris index OOB\n"); return -1;}
        float x_iris_model = dequantize_int8_to_float(iris_data[offset + 0], iris_zp, iris_scale);
        float y_iris_model = dequantize_int8_to_float(iris_data[offset + 1], iris_zp, iris_scale);
        float x_unpadded = (x_iris_model - iris_letter_box->x_pad);
        float y_unpadded = (y_iris_model - iris_letter_box->y_pad);
        float x_orig_eye_rel = x_unpadded / iris_letter_box->scale;
        float y_orig_eye_rel = y_unpadded / iris_letter_box->scale;
        float final_x = x_orig_eye_rel + eye_crop_roi.left;
        float final_y = y_orig_eye_rel + eye_crop_roi.top;
        out_iris[i].x = (int)roundf(final_x);
        out_iris[i].y = (int)roundf(final_y);
    }
    return 0;
}

// --- init_face_analyzer (Keep original corrected version) ---
int init_face_analyzer(const char *retinaface_model_path, const char* landmark_model_path, const char* iris_model_path, face_analyzer_app_context_t *app_ctx) {
    // ... (Keep implementation from previous corrected version) ...
    // ... (Ensure it handles the 3 outputs of RetinaFace correctly) ...
    memset(app_ctx, 0, sizeof(face_analyzer_app_context_t)); int ret; int model_len = 0; unsigned char *model_buf = NULL;
    printf("Init Face Detect Model (RetinaFace): %s\n", retinaface_model_path); model_len = read_data_from_file(retinaface_model_path, reinterpret_cast<char**>(&model_buf)); if (!model_buf) { printf("ERROR: read detect model failed\n"); return -1; } ret = rknn_init(&app_ctx->rknn_ctx_detect, model_buf, model_len, 0, NULL); free(model_buf); model_buf = NULL; if (ret < 0) { printf("ERROR: init detect failed ret=%d\n", ret); return -1; } ret = rknn_set_core_mask(app_ctx->rknn_ctx_detect, RKNN_NPU_CORE_0); if (ret < 0) { printf("WARN: set core mask detect failed\n");} else { printf("INFO: Detect assigned Core 0.\n"); } ret = rknn_query(app_ctx->rknn_ctx_detect, RKNN_QUERY_IN_OUT_NUM, &app_ctx->io_num_detect, sizeof(app_ctx->io_num_detect)); if (ret != RKNN_SUCC || app_ctx->io_num_detect.n_output < 3) { printf("ERROR: RetinaFace model needs at least 3 outputs (loc, score, landms). Got %d\n", app_ctx->io_num_detect.n_output); goto cleanup_detect; }
    app_ctx->input_attrs_detect = (rknn_tensor_attr*)malloc(app_ctx->io_num_detect.n_input * sizeof(rknn_tensor_attr)); if (!app_ctx->input_attrs_detect) { ret = -1; printf("ERROR: malloc input attrs detect failed\n"); goto cleanup_detect; } memset(app_ctx->input_attrs_detect, 0, app_ctx->io_num_detect.n_input * sizeof(rknn_tensor_attr)); for (uint32_t i = 0; i < app_ctx->io_num_detect.n_input; i++) { app_ctx->input_attrs_detect[i].index = i; ret = rknn_query(app_ctx->rknn_ctx_detect, RKNN_QUERY_NATIVE_INPUT_ATTR, &app_ctx->input_attrs_detect[i], sizeof(rknn_tensor_attr)); if (ret != RKNN_SUCC) { printf("ERROR: query input attr %d detect failed ret=%d\n", i, ret); goto cleanup_detect; } dump_tensor_attr(&app_ctx->input_attrs_detect[i], "DetectIn(Native)"); }
    app_ctx->output_attrs_detect = (rknn_tensor_attr*)malloc(app_ctx->io_num_detect.n_output * sizeof(rknn_tensor_attr)); if (!app_ctx->output_attrs_detect) { ret = -1; printf("ERROR: malloc output attrs detect failed\n"); goto cleanup_detect; } memset(app_ctx->output_attrs_detect, 0, app_ctx->io_num_detect.n_output * sizeof(rknn_tensor_attr)); for (uint32_t i = 0; i < app_ctx->io_num_detect.n_output; i++) { app_ctx->output_attrs_detect[i].index = i; ret = rknn_query(app_ctx->rknn_ctx_detect, RKNN_QUERY_NATIVE_NHWC_OUTPUT_ATTR, &app_ctx->output_attrs_detect[i], sizeof(rknn_tensor_attr)); if (ret != RKNN_SUCC) { printf("ERROR: query output attr %d detect failed ret=%d\n", i, ret); goto cleanup_detect; } dump_tensor_attr(&app_ctx->output_attrs_detect[i], "DetectOut(Native)"); }
    if (app_ctx->io_num_detect.n_input > 0) { app_ctx->input_attrs_detect[0].type = RKNN_TENSOR_UINT8; app_ctx->input_attrs_detect[0].fmt = RKNN_TENSOR_NHWC; app_ctx->input_fmt_detect = RKNN_TENSOR_NHWC; }
    app_ctx->is_quant_detect = (app_ctx->output_attrs_detect[0].qnt_type != RKNN_TENSOR_QNT_NONE || app_ctx->output_attrs_detect[1].qnt_type != RKNN_TENSOR_QNT_NONE); printf("INFO: RetinaFace model quantization: %s\n", app_ctx->is_quant_detect ? "YES" : "NO");
    app_ctx->input_mems_detect = (rknn_tensor_mem**)malloc(sizeof(rknn_tensor_mem*) * app_ctx->io_num_detect.n_input); if (!app_ctx->input_mems_detect) { ret = -1; printf("ERROR: malloc input mems detect failed\n"); goto cleanup_detect; } memset(app_ctx->input_mems_detect, 0, sizeof(rknn_tensor_mem*) * app_ctx->io_num_detect.n_input); for (uint32_t i = 0; i < app_ctx->io_num_detect.n_input; ++i) { app_ctx->input_mems_detect[i] = rknn_create_mem(app_ctx->rknn_ctx_detect, app_ctx->input_attrs_detect[i].size_with_stride); if (!app_ctx->input_mems_detect[i]) { ret = -1; printf("ERROR: create input mem %d detect failed\n", i); goto cleanup_detect; } ret = rknn_set_io_mem(app_ctx->rknn_ctx_detect, app_ctx->input_mems_detect[i], &app_ctx->input_attrs_detect[i]); if (ret < 0) { printf("ERROR: set io mem input %d detect failed ret=%d\n", i, ret); goto cleanup_detect; } }
    app_ctx->output_mems_detect = (rknn_tensor_mem**)malloc(sizeof(rknn_tensor_mem*) * app_ctx->io_num_detect.n_output); if (!app_ctx->output_mems_detect) { ret = -1; printf("ERROR: malloc output mems detect failed\n"); goto cleanup_detect; } memset(app_ctx->output_mems_detect, 0, sizeof(rknn_tensor_mem*) * app_ctx->io_num_detect.n_output); for (uint32_t i = 0; i < app_ctx->io_num_detect.n_output; ++i) { app_ctx->output_mems_detect[i] = rknn_create_mem(app_ctx->rknn_ctx_detect, app_ctx->output_attrs_detect[i].size_with_stride); if (!app_ctx->output_mems_detect[i]) { ret = -1; printf("ERROR: create output mem %d detect failed\n", i); goto cleanup_detect; } ret = rknn_set_io_mem(app_ctx->rknn_ctx_detect, app_ctx->output_mems_detect[i], &app_ctx->output_attrs_detect[i]); if (ret < 0) { printf("ERROR: set io mem output %d detect failed ret=%d\n", i, ret); goto cleanup_detect; } }
    if (app_ctx->input_attrs_detect[0].fmt == RKNN_TENSOR_NCHW) { app_ctx->model1_channel = app_ctx->input_attrs_detect[0].dims[1]; app_ctx->model1_height = app_ctx->input_attrs_detect[0].dims[2]; app_ctx->model1_width = app_ctx->input_attrs_detect[0].dims[3]; } else { app_ctx->model1_height = app_ctx->input_attrs_detect[0].dims[1]; app_ctx->model1_width = app_ctx->input_attrs_detect[0].dims[2]; app_ctx->model1_channel = app_ctx->input_attrs_detect[0].dims[3]; } printf("RetinaFace input HxWxC: %dx%dx%d\n", app_ctx->model1_height, app_ctx->model1_width, app_ctx->model1_channel);
    // --- Init Landmark Model ---
    printf("Init Face Landmark Model: %s\n", landmark_model_path); model_len = read_data_from_file(landmark_model_path, reinterpret_cast<char**>(&model_buf)); if (!model_buf) { ret = -1; goto cleanup_detect; } ret = rknn_init(&app_ctx->rknn_ctx_landmark, model_buf, model_len, 0, NULL); free(model_buf); model_buf = NULL; if (ret < 0) { goto cleanup_detect; } ret = rknn_set_core_mask(app_ctx->rknn_ctx_landmark, RKNN_NPU_CORE_0); if(ret < 0) { printf("WARN: set core mask landmark failed\n"); } else { printf("INFO: Landmark assigned Core 0.\n"); } ret = rknn_query(app_ctx->rknn_ctx_landmark, RKNN_QUERY_IN_OUT_NUM, &app_ctx->io_num_landmark, sizeof(app_ctx->io_num_landmark)); if (ret != RKNN_SUCC) { goto cleanup_landmark; }
    app_ctx->input_attrs_landmark = (rknn_tensor_attr*)malloc(app_ctx->io_num_landmark.n_input * sizeof(rknn_tensor_attr)); if (!app_ctx->input_attrs_landmark) { ret = -1; goto cleanup_landmark; } memset(app_ctx->input_attrs_landmark, 0, app_ctx->io_num_landmark.n_input * sizeof(rknn_tensor_attr)); for (uint32_t i = 0; i < app_ctx->io_num_landmark.n_input; i++) { app_ctx->input_attrs_landmark[i].index = i; ret = rknn_query(app_ctx->rknn_ctx_landmark, RKNN_QUERY_NATIVE_INPUT_ATTR, &app_ctx->input_attrs_landmark[i], sizeof(rknn_tensor_attr)); if (ret != RKNN_SUCC) goto cleanup_landmark; dump_tensor_attr(&app_ctx->input_attrs_landmark[i], "LmkIn(Native)"); }
    app_ctx->output_attrs_landmark = (rknn_tensor_attr*)malloc(app_ctx->io_num_landmark.n_output * sizeof(rknn_tensor_attr)); if (!app_ctx->output_attrs_landmark) { ret = -1; goto cleanup_landmark; } memset(app_ctx->output_attrs_landmark, 0, app_ctx->io_num_landmark.n_output * sizeof(rknn_tensor_attr)); for (uint32_t i = 0; i < app_ctx->io_num_landmark.n_output; i++) { app_ctx->output_attrs_landmark[i].index = i; ret = rknn_query(app_ctx->rknn_ctx_landmark, RKNN_QUERY_NATIVE_NHWC_OUTPUT_ATTR, &app_ctx->output_attrs_landmark[i], sizeof(rknn_tensor_attr)); if (ret != RKNN_SUCC) goto cleanup_landmark; dump_tensor_attr(&app_ctx->output_attrs_landmark[i], "LmkOut(Native)"); }
    if (app_ctx->io_num_landmark.n_input > 0) { app_ctx->input_attrs_landmark[0].type = RKNN_TENSOR_UINT8; app_ctx->input_attrs_landmark[0].fmt = RKNN_TENSOR_NHWC; app_ctx->input_fmt_landmark = RKNN_TENSOR_NHWC; }
    app_ctx->input_mems_landmark = (rknn_tensor_mem**)malloc(sizeof(rknn_tensor_mem*) * app_ctx->io_num_landmark.n_input); if(!app_ctx->input_mems_landmark){ ret = -1; goto cleanup_landmark; } memset(app_ctx->input_mems_landmark, 0, sizeof(rknn_tensor_mem*) * app_ctx->io_num_landmark.n_input); for(uint32_t i = 0; i < app_ctx->io_num_landmark.n_input; ++i) { app_ctx->input_mems_landmark[i] = rknn_create_mem(app_ctx->rknn_ctx_landmark, app_ctx->input_attrs_landmark[i].size_with_stride); if(!app_ctx->input_mems_landmark[i]) { ret = -1; goto cleanup_landmark; } ret = rknn_set_io_mem(app_ctx->rknn_ctx_landmark, app_ctx->input_mems_landmark[i], &app_ctx->input_attrs_landmark[i]); if (ret < 0) goto cleanup_landmark; }
    app_ctx->output_mems_landmark = (rknn_tensor_mem**)malloc(sizeof(rknn_tensor_mem*) * app_ctx->io_num_landmark.n_output); if(!app_ctx->output_mems_landmark){ ret = -1; goto cleanup_landmark; } memset(app_ctx->output_mems_landmark, 0, sizeof(rknn_tensor_mem*) * app_ctx->io_num_landmark.n_output); for(uint32_t i = 0; i < app_ctx->io_num_landmark.n_output; ++i) { app_ctx->output_mems_landmark[i] = rknn_create_mem(app_ctx->rknn_ctx_landmark, app_ctx->output_attrs_landmark[i].size_with_stride); if(!app_ctx->output_mems_landmark[i]) { ret = -1; goto cleanup_landmark; } ret = rknn_set_io_mem(app_ctx->rknn_ctx_landmark, app_ctx->output_mems_landmark[i], &app_ctx->output_attrs_landmark[i]); if (ret < 0) goto cleanup_landmark; }
    app_ctx->model2_height = app_ctx->input_attrs_landmark[0].dims[1]; app_ctx->model2_width = app_ctx->input_attrs_landmark[0].dims[2]; app_ctx->model2_channel = app_ctx->input_attrs_landmark[0].dims[3];
    // --- Init Iris Model ---
    printf("Init Iris Landmark Model: %s\n", iris_model_path); model_len = read_data_from_file(iris_model_path, reinterpret_cast<char**>(&model_buf)); if (!model_buf) { ret = -1; goto cleanup_landmark; } ret = rknn_init(&app_ctx->rknn_ctx_iris, model_buf, model_len, 0, NULL); free(model_buf); model_buf = NULL; if (ret < 0) { goto cleanup_landmark; } ret = rknn_set_core_mask(app_ctx->rknn_ctx_iris, RKNN_NPU_CORE_0); if(ret < 0) { printf("WARN: set core mask iris failed\n"); } else { printf("INFO: Iris assigned Core 0.\n"); } ret = rknn_query(app_ctx->rknn_ctx_iris, RKNN_QUERY_IN_OUT_NUM, &app_ctx->io_num_iris, sizeof(app_ctx->io_num_iris)); if (ret != RKNN_SUCC || app_ctx->io_num_iris.n_output < 2) { goto cleanup_iris; }
    app_ctx->input_attrs_iris = (rknn_tensor_attr*)malloc(app_ctx->io_num_iris.n_input * sizeof(rknn_tensor_attr)); if (!app_ctx->input_attrs_iris) { ret = -1; goto cleanup_iris; } memset(app_ctx->input_attrs_iris, 0, app_ctx->io_num_iris.n_input * sizeof(rknn_tensor_attr)); for (uint32_t i = 0; i < app_ctx->io_num_iris.n_input; i++) { app_ctx->input_attrs_iris[i].index = i; ret = rknn_query(app_ctx->rknn_ctx_iris, RKNN_QUERY_NATIVE_INPUT_ATTR, &app_ctx->input_attrs_iris[i], sizeof(rknn_tensor_attr)); if (ret != RKNN_SUCC) goto cleanup_iris; dump_tensor_attr(&app_ctx->input_attrs_iris[i], "IrisIn(Native)"); }
    app_ctx->output_attrs_iris = (rknn_tensor_attr*)malloc(app_ctx->io_num_iris.n_output * sizeof(rknn_tensor_attr)); if (!app_ctx->output_attrs_iris) { ret = -1; goto cleanup_iris; } memset(app_ctx->output_attrs_iris, 0, app_ctx->io_num_iris.n_output * sizeof(rknn_tensor_attr)); for (uint32_t i = 0; i < app_ctx->io_num_iris.n_output; i++) { app_ctx->output_attrs_iris[i].index = i; ret = rknn_query(app_ctx->rknn_ctx_iris, RKNN_QUERY_NATIVE_NHWC_OUTPUT_ATTR, &app_ctx->output_attrs_iris[i], sizeof(rknn_tensor_attr)); if (ret != RKNN_SUCC) goto cleanup_iris; dump_tensor_attr(&app_ctx->output_attrs_iris[i], "IrisOut(Native)"); }
    if (app_ctx->io_num_iris.n_input > 0) { app_ctx->input_attrs_iris[0].type = RKNN_TENSOR_UINT8; app_ctx->input_attrs_iris[0].fmt = RKNN_TENSOR_NHWC; app_ctx->input_fmt_iris = RKNN_TENSOR_NHWC; }
    app_ctx->input_mems_iris = (rknn_tensor_mem**)malloc(sizeof(rknn_tensor_mem*) * app_ctx->io_num_iris.n_input); if(!app_ctx->input_mems_iris){ ret = -1; goto cleanup_iris; } memset(app_ctx->input_mems_iris, 0, sizeof(rknn_tensor_mem*) * app_ctx->io_num_iris.n_input); for(uint32_t i = 0; i < app_ctx->io_num_iris.n_input; ++i) { app_ctx->input_mems_iris[i] = rknn_create_mem(app_ctx->rknn_ctx_iris, app_ctx->input_attrs_iris[i].size_with_stride); if(!app_ctx->input_mems_iris[i]) { ret = -1; goto cleanup_iris; } ret = rknn_set_io_mem(app_ctx->rknn_ctx_iris, app_ctx->input_mems_iris[i], &app_ctx->input_attrs_iris[i]); if (ret < 0) goto cleanup_iris; }
    app_ctx->output_mems_iris = (rknn_tensor_mem**)malloc(sizeof(rknn_tensor_mem*) * app_ctx->io_num_iris.n_output); if(!app_ctx->output_mems_iris){ ret = -1; goto cleanup_iris; } memset(app_ctx->output_mems_iris, 0, sizeof(rknn_tensor_mem*) * app_ctx->io_num_iris.n_output); for(uint32_t i = 0; i < app_ctx->io_num_iris.n_output; ++i) { app_ctx->output_mems_iris[i] = rknn_create_mem(app_ctx->rknn_ctx_iris, app_ctx->output_attrs_iris[i].size_with_stride); if(!app_ctx->output_mems_iris[i]) { ret = -1; goto cleanup_iris; } ret = rknn_set_io_mem(app_ctx->rknn_ctx_iris, app_ctx->output_mems_iris[i], &app_ctx->output_attrs_iris[i]); if (ret < 0) goto cleanup_iris; }
    app_ctx->model3_height = app_ctx->input_attrs_iris[0].dims[1]; app_ctx->model3_width = app_ctx->input_attrs_iris[0].dims[2]; app_ctx->model3_channel = app_ctx->input_attrs_iris[0].dims[3];
    printf("INFO: Face Analyzer init successful.\n"); return 0; // Success

cleanup_iris:
    printf("ERROR: Cleanup iris stage, ret=%d\n", ret);
    if (app_ctx->rknn_ctx_iris != 0) rknn_destroy(app_ctx->rknn_ctx_iris);
    if (app_ctx->input_attrs_iris) free(app_ctx->input_attrs_iris);
    if (app_ctx->output_attrs_iris) free(app_ctx->output_attrs_iris);
    if(app_ctx->input_mems_iris) { for(uint32_t i=0; i<app_ctx->io_num_iris.n_input; ++i) if(app_ctx->input_mems_iris[i]) rknn_destroy_mem(app_ctx->rknn_ctx_iris, app_ctx->input_mems_iris[i]); free(app_ctx->input_mems_iris); }
    if(app_ctx->output_mems_iris) { for(uint32_t i=0; i<app_ctx->io_num_iris.n_output; ++i) if(app_ctx->output_mems_iris[i]) rknn_destroy_mem(app_ctx->rknn_ctx_iris, app_ctx->output_mems_iris[i]); free(app_ctx->output_mems_iris); }
    // Fall through to cleanup landmark
cleanup_landmark:
    printf("ERROR: Cleanup landmark stage, ret=%d\n", ret);
    if (app_ctx->rknn_ctx_landmark != 0) rknn_destroy(app_ctx->rknn_ctx_landmark);
    if (app_ctx->input_attrs_landmark) free(app_ctx->input_attrs_landmark);
    if (app_ctx->output_attrs_landmark) free(app_ctx->output_attrs_landmark);
    if(app_ctx->input_mems_landmark) { for(uint32_t i=0; i<app_ctx->io_num_landmark.n_input; ++i) if(app_ctx->input_mems_landmark[i]) rknn_destroy_mem(app_ctx->rknn_ctx_landmark, app_ctx->input_mems_landmark[i]); free(app_ctx->input_mems_landmark); }
    if(app_ctx->output_mems_landmark) { for(uint32_t i=0; i<app_ctx->io_num_landmark.n_output; ++i) if(app_ctx->output_mems_landmark[i]) rknn_destroy_mem(app_ctx->rknn_ctx_landmark, app_ctx->output_mems_landmark[i]); free(app_ctx->output_mems_landmark); }
    // Fall through to cleanup detection
cleanup_detect:
    printf("ERROR: Cleanup detect stage, ret=%d\n", ret);
    if (app_ctx->rknn_ctx_detect != 0) rknn_destroy(app_ctx->rknn_ctx_detect);
    if (app_ctx->input_attrs_detect) free(app_ctx->input_attrs_detect);
    if (app_ctx->output_attrs_detect) free(app_ctx->output_attrs_detect);
    if(app_ctx->input_mems_detect) { for(uint32_t i=0; i < app_ctx->io_num_detect.n_input; ++i) if(app_ctx->input_mems_detect[i]) rknn_destroy_mem(app_ctx->rknn_ctx_detect, app_ctx->input_mems_detect[i]); free(app_ctx->input_mems_detect); }
    if(app_ctx->output_mems_detect) { for(uint32_t i=0; i < app_ctx->io_num_detect.n_output; ++i) if(app_ctx->output_mems_detect[i]) rknn_destroy_mem(app_ctx->rknn_ctx_detect, app_ctx->output_mems_detect[i]); free(app_ctx->output_mems_detect); }
    // Final cleanup
    memset(app_ctx, 0, sizeof(face_analyzer_app_context_t));
    printf("ERROR: init_face_analyzer failed ret=%d\n", ret);
    return ret > 0 ? -ret : ret; // Ensure negative error code
}


// --- release_face_analyzer (Keep from previous correction) ---
int release_face_analyzer(face_analyzer_app_context_t *app_ctx) {
    // --- Release Iris Resources ---
    if(app_ctx->input_mems_iris) {
        for(uint32_t i=0; i<app_ctx->io_num_iris.n_input; ++i) {
            if(app_ctx->input_mems_iris[i]) {
                rknn_destroy_mem(app_ctx->rknn_ctx_iris, app_ctx->input_mems_iris[i]);
                app_ctx->input_mems_iris[i] = NULL;
            }
        }
        free(app_ctx->input_mems_iris); app_ctx->input_mems_iris = NULL;
    }
    if(app_ctx->output_mems_iris) {
        for(uint32_t i=0; i<app_ctx->io_num_iris.n_output; ++i) {
            if(app_ctx->output_mems_iris[i]) {
                rknn_destroy_mem(app_ctx->rknn_ctx_iris, app_ctx->output_mems_iris[i]);
                app_ctx->output_mems_iris[i] = NULL;
            }
        }
        free(app_ctx->output_mems_iris); app_ctx->output_mems_iris = NULL;
    }
    if (app_ctx->input_attrs_iris) { free(app_ctx->input_attrs_iris); app_ctx->input_attrs_iris = NULL; }
    if (app_ctx->output_attrs_iris) { free(app_ctx->output_attrs_iris); app_ctx->output_attrs_iris = NULL; }
    if (app_ctx->rknn_ctx_iris != 0) { rknn_destroy(app_ctx->rknn_ctx_iris); app_ctx->rknn_ctx_iris = 0; }

    // --- Release Landmark Resources ---
     if(app_ctx->input_mems_landmark) {
        for(uint32_t i=0; i<app_ctx->io_num_landmark.n_input; ++i) {
            if(app_ctx->input_mems_landmark[i]) {
                rknn_destroy_mem(app_ctx->rknn_ctx_landmark, app_ctx->input_mems_landmark[i]);
                app_ctx->input_mems_landmark[i] = NULL;
            }
        }
        free(app_ctx->input_mems_landmark); app_ctx->input_mems_landmark = NULL;
    }
    if(app_ctx->output_mems_landmark) {
        for(uint32_t i=0; i<app_ctx->io_num_landmark.n_output; ++i) {
            if(app_ctx->output_mems_landmark[i]) {
                rknn_destroy_mem(app_ctx->rknn_ctx_landmark, app_ctx->output_mems_landmark[i]);
                app_ctx->output_mems_landmark[i] = NULL;
            }
        }
        free(app_ctx->output_mems_landmark); app_ctx->output_mems_landmark = NULL;
    }
    if (app_ctx->input_attrs_landmark) { free(app_ctx->input_attrs_landmark); app_ctx->input_attrs_landmark = NULL; }
    if (app_ctx->output_attrs_landmark) { free(app_ctx->output_attrs_landmark); app_ctx->output_attrs_landmark = NULL; }
    if (app_ctx->rknn_ctx_landmark != 0) { rknn_destroy(app_ctx->rknn_ctx_landmark); app_ctx->rknn_ctx_landmark = 0; }

    // --- Release Detection Resources ---
     if(app_ctx->input_mems_detect) {
        for(uint32_t i=0; i < app_ctx->io_num_detect.n_input; ++i) {
             if(app_ctx->input_mems_detect[i]) {
                rknn_destroy_mem(app_ctx->rknn_ctx_detect, app_ctx->input_mems_detect[i]);
                app_ctx->input_mems_detect[i] = NULL;
            }
        }
        free(app_ctx->input_mems_detect); app_ctx->input_mems_detect = NULL;
    }
    if(app_ctx->output_mems_detect) {
        for(uint32_t i=0; i < app_ctx->io_num_detect.n_output; ++i) {
             if(app_ctx->output_mems_detect[i]) {
                rknn_destroy_mem(app_ctx->rknn_ctx_detect, app_ctx->output_mems_detect[i]);
                app_ctx->output_mems_detect[i] = NULL;
            }
        }
        free(app_ctx->output_mems_detect); app_ctx->output_mems_detect = NULL;
    }
    if (app_ctx->input_attrs_detect) { free(app_ctx->input_attrs_detect); app_ctx->input_attrs_detect = NULL; }
    if (app_ctx->output_attrs_detect) { free(app_ctx->output_attrs_detect); app_ctx->output_attrs_detect = NULL; }
    if (app_ctx->rknn_ctx_detect != 0) { rknn_destroy(app_ctx->rknn_ctx_detect); app_ctx->rknn_ctx_detect = 0; }

    printf("INFO: Face Analyzer released successfully.\n");
    return 0;
}


// --- inference_face_analyzer (With RetinaFace box adjustment) ---
int inference_face_analyzer(face_analyzer_app_context_t *app_ctx,
                            image_buffer_t *src_img,
                            face_analyzer_result_t *out_result) {
    int ret = 0;
    memset(out_result, 0, sizeof(face_analyzer_result_t));
    letterbox_t det_letter_box;
    memset(&det_letter_box, 0, sizeof(letterbox_t));
    int bg_color = 114;
    int src_w = src_img->width;
    int src_h = src_img->height;

    // --- Stage 1: Face Detection (RetinaFace) ---
    if (!app_ctx || !app_ctx->input_mems_detect || !app_ctx->input_mems_detect[0] || !app_ctx->input_mems_detect[0]->virt_addr) {
        printf("ERROR: Detect input mem null\n"); return -1;
    }
    image_buffer_t detect_input_buf_wrapper;
    detect_input_buf_wrapper.width = app_ctx->model1_width;
    detect_input_buf_wrapper.height = app_ctx->model1_height;
    detect_input_buf_wrapper.format = IMAGE_FORMAT_RGB888; // Assuming RGB output from letterbox
    detect_input_buf_wrapper.virt_addr = (unsigned char*)app_ctx->input_mems_detect[0]->virt_addr;
    detect_input_buf_wrapper.size = app_ctx->input_mems_detect[0]->size;
    detect_input_buf_wrapper.fd = app_ctx->input_mems_detect[0]->fd;
    detect_input_buf_wrapper.width_stride = app_ctx->input_attrs_detect[0].w_stride;
    detect_input_buf_wrapper.height_stride = app_ctx->input_attrs_detect[0].h_stride ? app_ctx->input_attrs_detect[0].h_stride : app_ctx->model1_height;

    ret = convert_image_with_letterbox(src_img, &detect_input_buf_wrapper, &det_letter_box, bg_color);
    if (ret < 0) { printf("ERROR: convert_image_with_letterbox (detect) failed! ret=%d\n", ret); return ret; }

    // Set IO Mem for Detect Input - Critical for Zero Copy
    ret = rknn_set_io_mem(app_ctx->rknn_ctx_detect, app_ctx->input_mems_detect[0], &app_ctx->input_attrs_detect[0]);
    if (ret < 0) { printf("ERROR: set io mem detect input failed! ret=%d\n", ret); return ret;}

    #ifdef ZERO_COPY // Optional: Only needed if ZERO_COPY is defined and supported
    ret = rknn_mem_sync(app_ctx->rknn_ctx_detect, app_ctx->input_mems_detect[0], RKNN_MEMORY_SYNC_TO_DEVICE);
    if (ret < 0) { printf("ERROR: sync detect input failed! ret=%d\n", ret); return ret; }
    #endif

    ret = rknn_run(app_ctx->rknn_ctx_detect, nullptr);
    if (ret < 0) { printf("ERROR: rknn_run (detect) failed! ret=%d\n", ret); return ret; }

    #ifdef ZERO_COPY // Optional
    for (uint32_t i = 0; i < app_ctx->io_num_detect.n_output; ++i) {
        ret = rknn_mem_sync(app_ctx->rknn_ctx_detect, app_ctx->output_mems_detect[i], RKNN_MEMORY_SYNC_FROM_DEVICE);
        if (ret < 0) { printf("ERROR: sync detect output %u failed! ret=%d\n", i, ret); return ret; }
    }
    #endif

    // Post-process RetinaFace results
    ret = post_process_retinaface_detection_only(app_ctx->output_mems_detect, app_ctx, &det_letter_box, src_w, src_h, out_result);
    if (ret < 0) { printf("ERROR: post_process_retinaface failed! ret=%d\n", ret); return ret; }

    int num_faces_to_process = out_result->count;
    if (num_faces_to_process == 0) return 0; // No faces found

    // --- Setup Wrappers for Landmark/Iris NPU Input Buffers ---
    image_buffer_t landmark_input_buf_wrapper;
    landmark_input_buf_wrapper.width=app_ctx->model2_width; landmark_input_buf_wrapper.height=app_ctx->model2_height; landmark_input_buf_wrapper.format=IMAGE_FORMAT_RGB888; landmark_input_buf_wrapper.virt_addr=(unsigned char*)app_ctx->input_mems_landmark[0]->virt_addr; landmark_input_buf_wrapper.size=app_ctx->input_mems_landmark[0]->size; landmark_input_buf_wrapper.fd=app_ctx->input_mems_landmark[0]->fd; landmark_input_buf_wrapper.width_stride=app_ctx->input_attrs_landmark[0].w_stride; landmark_input_buf_wrapper.height_stride=app_ctx->input_attrs_landmark[0].h_stride ? app_ctx->input_attrs_landmark[0].h_stride : app_ctx->model2_height;

    image_buffer_t iris_input_buf_wrapper;
    iris_input_buf_wrapper.width=app_ctx->model3_width; iris_input_buf_wrapper.height=app_ctx->model3_height; iris_input_buf_wrapper.format=IMAGE_FORMAT_RGB888; iris_input_buf_wrapper.virt_addr=(unsigned char*)app_ctx->input_mems_iris[0]->virt_addr; iris_input_buf_wrapper.size=app_ctx->input_mems_iris[0]->size; iris_input_buf_wrapper.fd=app_ctx->input_mems_iris[0]->fd; iris_input_buf_wrapper.width_stride=app_ctx->input_attrs_iris[0].w_stride; iris_input_buf_wrapper.height_stride=app_ctx->input_attrs_iris[0].h_stride ? app_ctx->input_attrs_iris[0].h_stride : app_ctx->model3_height;

    // --- Loop for Landmarks and Iris ---
    for (int i = 0; i < num_faces_to_process; ++i) {
        face_object_t* current_face_result = &out_result->faces[i];
        // Reset validity flags at the start of each face processing
        current_face_result->face_landmarks_valid = false;
        current_face_result->eye_landmarks_left_valid = false;
        current_face_result->iris_landmarks_left_valid = false;
        current_face_result->eye_landmarks_right_valid = false;
        current_face_result->iris_landmarks_right_valid = false;

        // --- Stage 2: Face Landmarks ---
        letterbox_t lmk_letter_box;
        image_rect_t src_rect_lmk;          // *Desired* source rect (potentially adjusted)
        image_rect_t clamped_src_rect_lmk;  // Source rect clamped to image bounds
        image_rect_t dst_rect_lmk;          // Target rect within landmark input buffer

        // Get Original RetinaFace Box properties
        int box_left   = current_face_result->box.left;
        int box_top    = current_face_result->box.top;
        int box_right  = current_face_result->box.right;
        int box_bottom = current_face_result->box.bottom;
        int box_w      = box_right - box_left;
        int box_h      = box_bottom - box_top;

        if (box_w <= 0 || box_h <= 0) {
            printf("WARN: Invalid RetinaFace box for face %d\n", i);
            continue;
        }

        // **** APPLY BOX ADJUSTMENT ****
        int height_reduction = static_cast<int>(box_h * BOX_HEIGHT_REDUCTION_FACTOR);
        int top_shift        = static_cast<int>(height_reduction * BOX_TOP_SHIFT_FACTOR);
        int bottom_shift     = height_reduction - top_shift;

        int adjusted_top    = box_top + top_shift;
        int adjusted_bottom = box_bottom - bottom_shift;
        int adjusted_h      = adjusted_bottom - adjusted_top;

        // Use the adjusted height (and original width) for scaling
        int crop_w_lmk = static_cast<int>(box_w * LANDMARK_CROP_SCALE); // Symmetric scale
        int crop_h_lmk = static_cast<int>(adjusted_h * LANDMARK_CROP_SCALE); // Scale the *adjusted* height symmetrically

        // Center the crop based on the *original* geometric center
        int crop_center_x = (box_left + box_right) / 2;
        int crop_center_y = (box_top + box_bottom) / 2;

        // Calculate the *desired* source rectangle using the adjusted dimensions
        src_rect_lmk.left   = crop_center_x - crop_w_lmk / 2;
        src_rect_lmk.top    = crop_center_y - crop_h_lmk / 2; // Note: Center is still original box center
        src_rect_lmk.right  = src_rect_lmk.left + crop_w_lmk;
        src_rect_lmk.bottom = src_rect_lmk.top + crop_h_lmk;
        // ****************************

        // Store the *adjusted* crop ROI for post-processing
        box_rect_t face_crop_roi_for_post = {src_rect_lmk.left, src_rect_lmk.top, src_rect_lmk.right, src_rect_lmk.bottom};

        // CLAMP the source rectangle to image boundaries
        clamped_src_rect_lmk.left   = clamp(src_rect_lmk.left, 0, src_w - 1);
        clamped_src_rect_lmk.top    = clamp(src_rect_lmk.top, 0, src_h - 1);
        clamped_src_rect_lmk.right  = clamp(src_rect_lmk.right, 0, src_w - 1);
        clamped_src_rect_lmk.bottom = clamp(src_rect_lmk.bottom, 0, src_h - 1);

        int clamped_crop_w_lmk = clamped_src_rect_lmk.right - clamped_src_rect_lmk.left;
        int clamped_crop_h_lmk = clamped_src_rect_lmk.bottom - clamped_src_rect_lmk.top;

        if (clamped_crop_w_lmk <= 0 || clamped_crop_h_lmk <= 0) {
            printf("WARN: Landmark crop rect invalid after clamping for face %d\n", i);
            continue;
        }

        // Calculate letterbox parameters based on the CLAMPED source size
        ret = calculate_letterbox_rects(clamped_crop_w_lmk, clamped_crop_h_lmk,
                                        app_ctx->model2_width, app_ctx->model2_height,
                                        1, &lmk_letter_box, &dst_rect_lmk);
        if (ret < 0) {
            printf("WARN: Failed calculate letterbox for landmarks face %d\n", i);
            continue;
        }

        // Perform RGA/CPU crop using the CLAMPED source rect
        ret = convert_image(src_img, &landmark_input_buf_wrapper, &clamped_src_rect_lmk, &dst_rect_lmk, bg_color);
        if (ret < 0) {
            printf("WARN: convert_image failed for landmarks face %d\n", i);
            continue;
        }

        // Set IO Mem, Sync, Run, Sync
        ret = rknn_set_io_mem(app_ctx->rknn_ctx_landmark, app_ctx->input_mems_landmark[0], &app_ctx->input_attrs_landmark[0]); if (ret < 0) { printf("Lmk set io err %d\n", ret); continue; }
        #ifdef ZERO_COPY
        ret = rknn_mem_sync(app_ctx->rknn_ctx_landmark, app_ctx->input_mems_landmark[0], RKNN_MEMORY_SYNC_TO_DEVICE); if (ret < 0) { printf("Lmk sync in err %d\n", ret); continue; }
        #endif
        ret = rknn_run(app_ctx->rknn_ctx_landmark, nullptr); if (ret < 0) { printf("Lmk run err %d\n", ret); continue; }
        #ifdef ZERO_COPY
        ret = rknn_mem_sync(app_ctx->rknn_ctx_landmark, app_ctx->output_mems_landmark[0], RKNN_MEMORY_SYNC_FROM_DEVICE); if (ret < 0) { printf("Lmk sync out err %d\n", ret); continue; }
        #endif

        // Post-process landmarks using the *adjusted* crop ROI as reference
        ret = post_process_face_landmarks(app_ctx->output_mems_landmark[0], app_ctx, face_crop_roi_for_post, &lmk_letter_box, current_face_result->face_landmarks);
        if (ret == 0) {
            current_face_result->face_landmarks_valid = true;
        } else {
            printf("WARN: post_process_face_landmarks failed face %d! ret=%d\n", i, ret);
            continue; // Skip iris if landmarks failed
        }

        // --- Stage 3: Iris Landmarks (Keep the clamping logic as before) ---
        if (current_face_result->face_landmarks_valid) {
            const int LEFT_EYE_ROI_IDX1 = 33; const int LEFT_EYE_ROI_IDX2 = 133;
            const int RIGHT_EYE_ROI_IDX1 = 362; const int RIGHT_EYE_ROI_IDX2 = 263;

            // --- Process Left Eye ---
            letterbox_t iris_letter_box_left; image_rect_t src_rect_iris_left; image_rect_t clamped_src_rect_iris_left; image_rect_t dst_rect_iris_left; box_rect_t left_eye_crop_roi_for_post;
            if (LEFT_EYE_ROI_IDX1 < NUM_FACE_LANDMARKS && LEFT_EYE_ROI_IDX2 < NUM_FACE_LANDMARKS) {
                // ... (Calculate desired ROI, store original for post-processing) ...
                point_t left_p1=current_face_result->face_landmarks[LEFT_EYE_ROI_IDX1]; point_t left_p2=current_face_result->face_landmarks[LEFT_EYE_ROI_IDX2]; int left_eye_cx=(left_p1.x+left_p2.x)/2; int left_eye_cy=(left_p1.y+left_p2.y)/2; int left_eye_w=abs(left_p1.x-left_p2.x); int left_eye_h=abs(left_p1.y-left_p2.y); int left_eye_size=(int)(std::max({left_eye_w,left_eye_h,1}) * EYE_CROP_SCALE);
                src_rect_iris_left.left=left_eye_cx-left_eye_size/2; src_rect_iris_left.top=left_eye_cy-left_eye_size/2; src_rect_iris_left.right=src_rect_iris_left.left+left_eye_size; src_rect_iris_left.bottom=src_rect_iris_left.top+left_eye_size;
                left_eye_crop_roi_for_post = {src_rect_iris_left.left, src_rect_iris_left.top, src_rect_iris_left.right, src_rect_iris_left.bottom};

                // ... (Clamp the source rectangle) ...
                clamped_src_rect_iris_left.left = clamp(src_rect_iris_left.left, 0, src_w - 1); clamped_src_rect_iris_left.top = clamp(src_rect_iris_left.top, 0, src_h - 1); clamped_src_rect_iris_left.right = clamp(src_rect_iris_left.right, 0, src_w - 1); clamped_src_rect_iris_left.bottom = clamp(src_rect_iris_left.bottom, 0, src_h - 1);
                int clamped_iris_size_left_w = clamped_src_rect_iris_left.right - clamped_src_rect_iris_left.left; int clamped_iris_size_left_h = clamped_src_rect_iris_left.bottom - clamped_src_rect_iris_left.top;

                if (clamped_iris_size_left_w > 0 && clamped_iris_size_left_h > 0) {
                    ret = calculate_letterbox_rects(clamped_iris_size_left_w, clamped_iris_size_left_h, app_ctx->model3_width, app_ctx->model3_height, 1, &iris_letter_box_left, &dst_rect_iris_left);
                    if (ret == 0) {
                        ret = convert_image(src_img, &iris_input_buf_wrapper, &clamped_src_rect_iris_left, &dst_rect_iris_left, bg_color);
                        if (ret == 0) {
                            ret = rknn_set_io_mem(app_ctx->rknn_ctx_iris, app_ctx->input_mems_iris[0], &app_ctx->input_attrs_iris[0]); if (ret < 0){ printf("IrisL set io err %d\n", ret); continue;}
                            #ifdef ZERO_COPY
                            ret = rknn_mem_sync(app_ctx->rknn_ctx_iris, app_ctx->input_mems_iris[0], RKNN_MEMORY_SYNC_TO_DEVICE); if (ret < 0){ printf("IrisL sync in err %d\n", ret); continue;}
                            #endif
                            ret = rknn_run(app_ctx->rknn_ctx_iris, nullptr);
                            if (ret == 0) {
                                #ifdef ZERO_COPY
                                for(uint32_t j = 0; j < app_ctx->io_num_iris.n_output; ++j) { ret = rknn_mem_sync(app_ctx->rknn_ctx_iris, app_ctx->output_mems_iris[j], RKNN_MEMORY_SYNC_FROM_DEVICE); if(ret < 0) break; }
                                #endif
                                if(ret == 0) { // Check sync result
                                    ret = post_process_iris_landmarks(app_ctx->output_mems_iris, app_ctx, left_eye_crop_roi_for_post, &iris_letter_box_left, current_face_result->eye_landmarks_left, current_face_result->iris_landmarks_left);
                                    if (ret == 0) { current_face_result->eye_landmarks_left_valid = true; current_face_result->iris_landmarks_left_valid = true; }
                                    else { /*warn*/ printf("WARN: pp iris left failed %d\n", ret);}
                                } else { /*warn*/ printf("WARN: sync iris left out failed %d\n", ret); }
                            } else { /*warn*/ printf("WARN: run iris left failed %d\n", ret); }
                        } else { /*warn*/ printf("WARN: convert iris left failed %d\n", ret); }
                    } else { /*warn*/ printf("WARN: calc letterbox iris left failed %d\n", ret); }
                } else { printf("WARN: Left iris crop rect invalid after clamping\n"); }
            } // end if left eye indices valid

            // --- Process Right Eye ---
            letterbox_t iris_letter_box_right; image_rect_t src_rect_iris_right; image_rect_t clamped_src_rect_iris_right; image_rect_t dst_rect_iris_right; box_rect_t right_eye_crop_roi_for_post;
             if (RIGHT_EYE_ROI_IDX1 < NUM_FACE_LANDMARKS && RIGHT_EYE_ROI_IDX2 < NUM_FACE_LANDMARKS) {
                 // ... (Calculate desired ROI, store original) ...
                 point_t right_p1=current_face_result->face_landmarks[RIGHT_EYE_ROI_IDX1]; point_t right_p2=current_face_result->face_landmarks[RIGHT_EYE_ROI_IDX2]; int right_eye_cx=(right_p1.x+right_p2.x)/2; int right_eye_cy=(right_p1.y+right_p2.y)/2; int right_eye_w=abs(right_p1.x-right_p2.x); int right_eye_h=abs(right_p1.y-right_p2.y); int right_eye_size=(int)(std::max({right_eye_w,right_eye_h,1}) * EYE_CROP_SCALE);
                 src_rect_iris_right.left=right_eye_cx-right_eye_size/2; src_rect_iris_right.top=right_eye_cy-right_eye_size/2; src_rect_iris_right.right=src_rect_iris_right.left+right_eye_size; src_rect_iris_right.bottom=src_rect_iris_right.top+right_eye_size;
                 right_eye_crop_roi_for_post = {src_rect_iris_right.left, src_rect_iris_right.top, src_rect_iris_right.right, src_rect_iris_right.bottom};

                 // ... (Clamp source rect) ...
                 clamped_src_rect_iris_right.left = clamp(src_rect_iris_right.left, 0, src_w - 1); clamped_src_rect_iris_right.top = clamp(src_rect_iris_right.top, 0, src_h - 1); clamped_src_rect_iris_right.right = clamp(src_rect_iris_right.right, 0, src_w - 1); clamped_src_rect_iris_right.bottom = clamp(src_rect_iris_right.bottom, 0, src_h - 1);
                 int clamped_iris_size_right_w = clamped_src_rect_iris_right.right - clamped_src_rect_iris_right.left; int clamped_iris_size_right_h = clamped_src_rect_iris_right.bottom - clamped_src_rect_iris_right.top;

                 if (clamped_iris_size_right_w > 0 && clamped_iris_size_right_h > 0) {
                     ret = calculate_letterbox_rects(clamped_iris_size_right_w, clamped_iris_size_right_h, app_ctx->model3_width, app_ctx->model3_height, 1, &iris_letter_box_right, &dst_rect_iris_right);
                     if (ret == 0) {
                         ret = convert_image(src_img, &iris_input_buf_wrapper, &clamped_src_rect_iris_right, &dst_rect_iris_right, bg_color);
                         if (ret == 0) {
                            ret = rknn_set_io_mem(app_ctx->rknn_ctx_iris, app_ctx->input_mems_iris[0], &app_ctx->input_attrs_iris[0]); if (ret < 0) { printf("IrisR set io err %d\n", ret); continue; }
                            #ifdef ZERO_COPY
                            ret = rknn_mem_sync(app_ctx->rknn_ctx_iris, app_ctx->input_mems_iris[0], RKNN_MEMORY_SYNC_TO_DEVICE); if (ret < 0) { printf("IrisR sync in err %d\n", ret); continue; }
                            #endif
                             ret = rknn_run(app_ctx->rknn_ctx_iris, nullptr);
                             if (ret == 0) {
                                 #ifdef ZERO_COPY
                                 for(uint32_t j = 0; j < app_ctx->io_num_iris.n_output; ++j) { ret = rknn_mem_sync(app_ctx->rknn_ctx_iris, app_ctx->output_mems_iris[j], RKNN_MEMORY_SYNC_FROM_DEVICE); if(ret < 0) break; }
                                 #endif
                                 if(ret == 0) { // Check sync result
                                     ret = post_process_iris_landmarks(app_ctx->output_mems_iris, app_ctx, right_eye_crop_roi_for_post, &iris_letter_box_right, current_face_result->eye_landmarks_right, current_face_result->iris_landmarks_right);
                                     if (ret == 0) { current_face_result->eye_landmarks_right_valid = true; current_face_result->iris_landmarks_right_valid = true; }
                                     else { printf("WARN: pp iris right failed %d\n", ret); }
                                 } else { printf("WARN: sync iris right out failed %d\n", ret); }
                             } else { printf("WARN: run iris right failed %d\n", ret); }
                         } else { printf("WARN: convert iris right failed %d\n", ret); }
                     } else { printf("WARN: calc letterbox iris right failed %d\n", ret); }
                 } else { printf("WARN: Right iris crop rect invalid after clamping\n"); }
             } // end if right eye indices valid
        } // End if face landmarks valid
    } // End loop over detected faces

    return 0; // Success
}

