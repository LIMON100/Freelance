{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyarabic in c:\\users\\user\\anaconda3\\envs\\llm-work1\\lib\\site-packages (0.6.15)\n",
      "Requirement already satisfied: six>=1.14.0 in c:\\users\\user\\anaconda3\\envs\\llm-work1\\lib\\site-packages (from pyarabic) (1.16.0)\n",
      "Requirement already satisfied: nltk in c:\\users\\user\\anaconda3\\envs\\llm-work1\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\user\\anaconda3\\envs\\llm-work1\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\user\\anaconda3\\envs\\llm-work1\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\user\\anaconda3\\envs\\llm-work1\\lib\\site-packages (from nltk) (2023.12.25)\n",
      "Requirement already satisfied: tqdm in c:\\users\\user\\anaconda3\\envs\\llm-work1\\lib\\site-packages (from nltk) (4.66.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\anaconda3\\envs\\llm-work1\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Collecting stanza\n",
      "  Downloading stanza-1.10.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: emoji in c:\\users\\user\\anaconda3\\envs\\llm-work1\\lib\\site-packages (from stanza) (2.14.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\user\\anaconda3\\envs\\llm-work1\\lib\\site-packages (from stanza) (1.26.4)\n",
      "Requirement already satisfied: protobuf>=3.15.0 in c:\\users\\user\\anaconda3\\envs\\llm-work1\\lib\\site-packages (from stanza) (4.25.3)\n",
      "Requirement already satisfied: requests in c:\\users\\user\\anaconda3\\envs\\llm-work1\\lib\\site-packages (from stanza) (2.31.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\user\\anaconda3\\envs\\llm-work1\\lib\\site-packages (from stanza) (3.3)\n",
      "Requirement already satisfied: torch>=1.3.0 in c:\\users\\user\\anaconda3\\envs\\llm-work1\\lib\\site-packages (from stanza) (2.3.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\user\\anaconda3\\envs\\llm-work1\\lib\\site-packages (from stanza) (4.66.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\user\\anaconda3\\envs\\llm-work1\\lib\\site-packages (from torch>=1.3.0->stanza) (3.13.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\user\\anaconda3\\envs\\llm-work1\\lib\\site-packages (from torch>=1.3.0->stanza) (4.11.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\user\\anaconda3\\envs\\llm-work1\\lib\\site-packages (from torch>=1.3.0->stanza) (1.12)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\user\\anaconda3\\envs\\llm-work1\\lib\\site-packages (from torch>=1.3.0->stanza) (3.1.3)\n",
      "Requirement already satisfied: fsspec in c:\\users\\user\\anaconda3\\envs\\llm-work1\\lib\\site-packages (from torch>=1.3.0->stanza) (2024.3.1)\n",
      "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in c:\\users\\user\\anaconda3\\envs\\llm-work1\\lib\\site-packages (from torch>=1.3.0->stanza) (2021.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\user\\anaconda3\\envs\\llm-work1\\lib\\site-packages (from requests->stanza) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\anaconda3\\envs\\llm-work1\\lib\\site-packages (from requests->stanza) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\anaconda3\\envs\\llm-work1\\lib\\site-packages (from requests->stanza) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\anaconda3\\envs\\llm-work1\\lib\\site-packages (from requests->stanza) (2024.2.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\anaconda3\\envs\\llm-work1\\lib\\site-packages (from tqdm->stanza) (0.4.6)\n",
      "Requirement already satisfied: intel-openmp==2021.* in c:\\users\\user\\anaconda3\\envs\\llm-work1\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch>=1.3.0->stanza) (2021.4.0)\n",
      "Requirement already satisfied: tbb==2021.* in c:\\users\\user\\anaconda3\\envs\\llm-work1\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch>=1.3.0->stanza) (2021.12.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\user\\anaconda3\\envs\\llm-work1\\lib\\site-packages (from jinja2->torch>=1.3.0->stanza) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\user\\anaconda3\\envs\\llm-work1\\lib\\site-packages (from sympy->torch>=1.3.0->stanza) (1.3.0)\n",
      "Downloading stanza-1.10.1-py3-none-any.whl (1.1 MB)\n",
      "   ---------------------------------------- 0.0/1.1 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.0/1.1 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.1/1.1 MB 787.7 kB/s eta 0:00:02\n",
      "   ----- ---------------------------------- 0.1/1.1 MB 1.1 MB/s eta 0:00:01\n",
      "   ------ --------------------------------- 0.2/1.1 MB 1.1 MB/s eta 0:00:01\n",
      "   ---------- ----------------------------- 0.3/1.1 MB 1.3 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 0.4/1.1 MB 1.4 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 0.5/1.1 MB 1.4 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 0.6/1.1 MB 1.5 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 0.7/1.1 MB 1.6 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 0.7/1.1 MB 1.6 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 0.8/1.1 MB 1.6 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 0.9/1.1 MB 1.7 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 1.0/1.1 MB 1.6 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 1.0/1.1 MB 1.7 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 1.1/1.1 MB 1.5 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 1.1/1.1 MB 1.5 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 1.1/1.1 MB 1.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.1/1.1 MB 1.3 MB/s eta 0:00:00\n",
      "Installing collected packages: stanza\n",
      "Successfully installed stanza-1.10.1\n"
     ]
    }
   ],
   "source": [
    "!pip install pyarabic\n",
    "!pip install nltk\n",
    "!pip install stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Alef normalization: هَذَا كِتَابٌ مُهِمٌّ.\n",
      "After removing diacritics: هذا كتاب مهم.\n",
      "Tokens: ['هذا', 'كتاب', 'مهم.']\n"
     ]
    }
   ],
   "source": [
    "import pyarabic.araby as araby\n",
    "\n",
    "arabic_text = \"هَذَا كِتَابٌ مُهِمٌّ.\"\n",
    "\n",
    "# Normalize Alef forms\n",
    "normalized_text_alef = araby.normalize_alef(arabic_text)\n",
    "print(f\"After Alef normalization: {normalized_text_alef}\")\n",
    "\n",
    "# Remove diacritics\n",
    "stripped_text = araby.strip_tashkeel(normalized_text_alef)\n",
    "print(f\"After removing diacritics: {stripped_text}\")\n",
    "\n",
    "# You can continue with other normalizations as needed\n",
    "\n",
    "# Tokenization (basic whitespace split - you might use a better tokenizer later)\n",
    "tokens = stripped_text.split()\n",
    "print(f\"Tokens: {tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\User/nltk_data'\n    - 'c:\\\\Users\\\\User\\\\anaconda3\\\\envs\\\\llm-work1\\\\nltk_data'\n    - 'c:\\\\Users\\\\User\\\\anaconda3\\\\envs\\\\llm-work1\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\User\\\\anaconda3\\\\envs\\\\llm-work1\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\User\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 37\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mH:/OherTask/hadiah_app/docs/data/allBooks/alkawakib_aldiraru_fi_Sharh_Sahih_albukhari_015.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m     35\u001b[0m     arabic_text \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m---> 37\u001b[0m preprocessed_sentences \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocess_arabic_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43marabic_text\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFirst 3 Preprocessed Sentences:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m preprocessed_sentences[:\u001b[38;5;241m3\u001b[39m]:\n",
      "Cell \u001b[1;32mIn[14], line 10\u001b[0m, in \u001b[0;36mpreprocess_arabic_text\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Preprocesses Arabic text for NLP tasks.\"\"\"\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# 1. Sentence Segmentation (before normalization for better sentence boundaries)\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m sentences \u001b[38;5;241m=\u001b[39m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# Basic sentence segmentation - may need refinement for Arabic nuances\u001b[39;00m\n\u001b[0;32m     12\u001b[0m processed_sentences \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m sentences:\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;66;03m# 2. Normalize Alef forms (ا, أ, إ, آ)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\llm-work1\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:119\u001b[0m, in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    110\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 119\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43m_get_punkt_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\llm-work1\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:105\u001b[0m, in \u001b[0;36m_get_punkt_tokenizer\u001b[1;34m(language)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mlru_cache\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_punkt_tokenizer\u001b[39m(language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     98\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;124;03m    A constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;124;03m    a lru cache for performance.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;124;03m    :type language: str\u001b[39;00m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPunktTokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\llm-work1\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1744\u001b[0m, in \u001b[0;36mPunktTokenizer.__init__\u001b[1;34m(self, lang)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   1743\u001b[0m     PunktSentenceTokenizer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m-> 1744\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_lang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\llm-work1\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1749\u001b[0m, in \u001b[0;36mPunktTokenizer.load_lang\u001b[1;34m(self, lang)\u001b[0m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_lang\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   1747\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[1;32m-> 1749\u001b[0m     lang_dir \u001b[38;5;241m=\u001b[39m \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtokenizers/punkt_tab/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlang\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1750\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_params \u001b[38;5;241m=\u001b[39m load_punkt_params(lang_dir)\n\u001b[0;32m   1751\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lang \u001b[38;5;241m=\u001b[39m lang\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\llm-work1\\Lib\\site-packages\\nltk\\data.py:579\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    577\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[0;32m    578\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 579\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\User/nltk_data'\n    - 'c:\\\\Users\\\\User\\\\anaconda3\\\\envs\\\\llm-work1\\\\nltk_data'\n    - 'c:\\\\Users\\\\User\\\\anaconda3\\\\envs\\\\llm-work1\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\User\\\\anaconda3\\\\envs\\\\llm-work1\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\User\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "import pyarabic.araby as araby\n",
    "import nltk\n",
    "# nltk.download('punkt') # No need to download again after you've done it once\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "def preprocess_arabic_text(text):\n",
    "    \"\"\"Preprocesses Arabic text for NLP tasks.\"\"\"\n",
    "\n",
    "    # 1. Sentence Segmentation (before normalization for better sentence boundaries)\n",
    "    sentences = sent_tokenize(text) # Basic sentence segmentation - may need refinement for Arabic nuances\n",
    "\n",
    "    processed_sentences = []\n",
    "    for sentence in sentences:\n",
    "        # 2. Normalize Alef forms (ا, أ, إ, آ)\n",
    "        normalized_text = araby.normalize_alef(sentence)\n",
    "\n",
    "        # 3. Normalize Alef Maksura (ى) to Yeh (ي) using string replacement:\n",
    "        normalized_text = normalized_text.replace('ى', 'ي')\n",
    "\n",
    "        # 4. Whitespace Normalization\n",
    "        normalized_text = ' '.join(normalized_text.split())\n",
    "\n",
    "        # 5. Tokenization (using NLTK)\n",
    "        tokens = nltk.word_tokenize(normalized_text)\n",
    "\n",
    "        # 6. Lemmatization (using PyArabic - basic example)\n",
    "        lemmatized_tokens = [araby.lemmatize(token) for token in tokens]\n",
    "\n",
    "        processed_sentences.append(\" \".join(lemmatized_tokens)) # Join tokens back into sentence strings\n",
    "\n",
    "    return processed_sentences # Return a list of preprocessed sentences\n",
    "\n",
    "# Load your text file (replace \"your_arabic_file.txt\")\n",
    "with open(\"H:/OherTask/hadiah_app/docs/data/allBooks/alkawakib_aldiraru_fi_Sharh_Sahih_albukhari_015.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    arabic_text = f.read()\n",
    "\n",
    "preprocessed_sentences = preprocess_arabic_text(arabic_text)\n",
    "print(\"First 3 Preprocessed Sentences:\")\n",
    "for sent in preprocessed_sentences[:3]:\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-19 17:28:03 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d6fd4335c9745aabb71f5451df6cbd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json:   0%|  …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-19 17:28:03 INFO: Downloaded file to C:\\Users\\User\\stanza_resources\\resources.json\n",
      "2025-02-19 17:28:03 WARNING: Language ar package default expects mwt, which has been added\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9c8a72316c34ff694bb21a6f983bdda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://huggingface.co/stanfordnlp/stanza-ar/resolve/v1.10.0/models/tokenize/padt.pt:   0%|       …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e80f34f52b864dd8bc1b0f5c2fe8ecbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://huggingface.co/stanfordnlp/stanza-ar/resolve/v1.10.0/models/mwt/padt.pt:   0%|          | …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-19 17:28:07 INFO: Loading these models for language: ar (Arabic):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | padt    |\n",
      "| mwt       | padt    |\n",
      "=======================\n",
      "\n",
      "2025-02-19 17:28:07 INFO: Using device: cpu\n",
      "2025-02-19 17:28:07 INFO: Loading: tokenize\n",
      "2025-02-19 17:28:07 INFO: Loading: mwt\n",
      "2025-02-19 17:28:07 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First 3 Sentences using Stanza:\n",
      "الكواكب الدراري في شرح صحيح البخاري - جـ ١٥\n",
      "الكواكب الدراري في شرح صحيح البخاري   \n",
      "(الكرماني، شمس الدين)\n",
      "القسم:\n",
      " شروح الحديث\n",
      "الكتاب\n",
      ":\n",
      " الكواكب الدراري في شرح صحيح البخاري\n",
      "المؤلف\n",
      ":\n",
      " محمد بن يوسف بن علي بن سعيد\n",
      "،\n",
      " شمس الدين الكرماني \n",
      "(\n",
      "ت 786هـ\n",
      ")\n",
      "الناشر\n",
      ":\n",
      " دار إحياء التراث العربي\n",
      "،\n",
      " بيروت\n",
      "-\n",
      "لبنان\n",
      "طبعة أولى\n",
      ":\n",
      " 1356هـ \n",
      "-\n",
      " 1937م\n",
      "طبعة ثانية\n",
      ":\n",
      " 1401هـ \n",
      "-\n",
      " 1981م\n",
      "عدد الأجزاء\n",
      ":\n",
      " 25\n",
      "أعده للشاملة\n",
      ":\n",
      " فريق رابطة النساخ برعاية \n",
      "(\n",
      "مركز النخب العلمية\n",
      ")\n",
      "[\n",
      "ترقيم الكتاب موافق للمطبوع\n",
      "]\n",
      "تاريخ النشر بالشاملة\n",
      ":\n",
      " 26 ذو القعدة 1435\n",
      "الكواكب الدراري في شرح صحيح البخاري - جـ ١٥\n",
      "(ص: ٢)\n",
      "‌\n",
      "‌بَاب مَنَاقِبِ جَعْفَرِ بْنِ أَبِي طَالِبٍ الْهَاشِمِيِّ رضي الله عنه\n",
      "وَقَالَ لَهُ النَّبِيُّ صلى الله عليه وسلم أَشْبَهْتَ خَلْقِي وَخُلُقِي\n",
      "3471 -\n",
      " حَدَّثَنَا أَحْمَدُ بْنُ أَبِي بَكْرٍ حَدَّثَنَا مُحَمَّدُ بْنُ إِبْرَاهِيمَ بْنِ دِينَارٍ أَبُو عَبْدِ اللَّهِ الْجُهَنِيُّ عَنْ ابْنِ أَبِي ذِئْبٍ عَنْ سَعِيدٍ الْمَقْبُرِيِّ عَنْ أَبِي هُرَيْرَةَ رضي الله عنه أَنَّ النَّاسَ كَانُوا يَقُولُونَ أَكْثَرَ أَبُو هُرَيْرَةَ وَإِنِّي كُنْتُ أَلْزَمُ رَسُولَ اللَّهِ صلى الله عليه وسلم بِشِبَعِ بَطْنِي حَتَّى لَا آكُلُ الْخَمِيرَ وَلَا أَلْبَسُ\n",
      "ــ\n",
      "(باب مناقب جعفر بن أبي طالب رضي الله تعالى عنه) وهو أسن من علي بعشر سنين وكنيته أبو عبد الله الطيار ذو الجناحين وذو الهجرتين الشجاع الجواد كان متقدم الإسلام هاجر إلى الحبشة وكان هو سبب إسلام النجاشي ثم هاجر إلى المدينة ثم أمره رسول الله صلى الله عليه وسلم على جيش غزوة مؤتة بضم الميم وبالفوقانية بعد زيد بن حارثة واستشهد فيها سنة ثمان من الهجرة ووجدوا به يومئذ بضعا وتسعين طعنة أو رمية في مقدمه وقال صلى الله عليه وسلم في جعفر: رأيت جعفرا يطير في الجنة مع الملائكة وقال أيضا حين قطعت يداه في غزاة مؤتة جعل الله له جناحين في الجنة يطير بهما رضي الله تعالى عنه. قوله (ابن أبي ذئب) بلفظ الحيوان المشهور هو محمد مر الإسناد في باب حفظ العلم و (أكثر) أي رواية الحديث و (الخمير) الخبز الذي خمر وجعل في عجينة الخميرة وفي بعضها الخبيز أي الخبز المأدوم و (الخبرة) بضم المعجمة وسكون الموحدة وبالراء الأدم و (الحبير)\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "\n",
    "# Download Arabic Stanza model if you haven't already\n",
    "# stanza.download('ar')\n",
    "\n",
    "nlp = stanza.Pipeline('ar', processors='tokenize') # Initialize Stanza pipeline for Arabic\n",
    "\n",
    "def stanza_segment_sentences(text):\n",
    "    doc = nlp(text)\n",
    "    sentences = [sentence.text for sentence in doc.sentences]\n",
    "    return sentences\n",
    "\n",
    "sentences_stanza = stanza_segment_sentences(arabic_text)\n",
    "print(\"\\nFirst 3 Sentences using Stanza:\")\n",
    "for sent in sentences_stanza[:3]:\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\llm-work1\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\User/nltk_data'\n    - 'c:\\\\Users\\\\User\\\\anaconda3\\\\envs\\\\llm-work1\\\\nltk_data'\n    - 'c:\\\\Users\\\\User\\\\anaconda3\\\\envs\\\\llm-work1\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\User\\\\anaconda3\\\\envs\\\\llm-work1\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\User\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 58\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mH:/OherTask/hadiah_app/docs/data/allBooks/alkawakib_aldiraru_fi_Sharh_Sahih_albukhari_015.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f: \u001b[38;5;66;03m# Replace with your file name\u001b[39;00m\n\u001b[0;32m     56\u001b[0m     arabic_text \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m---> 58\u001b[0m preprocessed_sentences \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocess_arabic_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43marabic_text\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;66;03m# --- 3. Generate Sentence Embeddings (AraBERT) ---\u001b[39;00m\n\u001b[0;32m     61\u001b[0m embedding_model \u001b[38;5;241m=\u001b[39m SentenceTransformer(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maubmindlab/bert-base-arabertv02-v2\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[1], line 42\u001b[0m, in \u001b[0;36mpreprocess_arabic_text\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpreprocess_arabic_text\u001b[39m(text):\n\u001b[0;32m     41\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Preprocesses Arabic text for NLP tasks.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 42\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     43\u001b[0m     processed_sentences \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m sentences:\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\llm-work1\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:119\u001b[0m, in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    110\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 119\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43m_get_punkt_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\llm-work1\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:105\u001b[0m, in \u001b[0;36m_get_punkt_tokenizer\u001b[1;34m(language)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mlru_cache\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_punkt_tokenizer\u001b[39m(language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     98\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;124;03m    A constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;124;03m    a lru cache for performance.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;124;03m    :type language: str\u001b[39;00m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPunktTokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\llm-work1\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1744\u001b[0m, in \u001b[0;36mPunktTokenizer.__init__\u001b[1;34m(self, lang)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   1743\u001b[0m     PunktSentenceTokenizer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m-> 1744\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_lang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\llm-work1\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1749\u001b[0m, in \u001b[0;36mPunktTokenizer.load_lang\u001b[1;34m(self, lang)\u001b[0m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_lang\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   1747\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[1;32m-> 1749\u001b[0m     lang_dir \u001b[38;5;241m=\u001b[39m \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtokenizers/punkt_tab/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlang\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1750\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_params \u001b[38;5;241m=\u001b[39m load_punkt_params(lang_dir)\n\u001b[0;32m   1751\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lang \u001b[38;5;241m=\u001b[39m lang\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\llm-work1\\Lib\\site-packages\\nltk\\data.py:579\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    577\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[0;32m    578\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 579\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\User/nltk_data'\n    - 'c:\\\\Users\\\\User\\\\anaconda3\\\\envs\\\\llm-work1\\\\nltk_data'\n    - 'c:\\\\Users\\\\User\\\\anaconda3\\\\envs\\\\llm-work1\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\User\\\\anaconda3\\\\envs\\\\llm-work1\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\User\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "import pyarabic.araby as araby\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# --- 1. Preprocessing Functions (Refined) ---\n",
    "arabic_punctuations = '''`÷×؛<>_()*&^%][ـ،/:\"؟.,'{}~¦+|!”…“–ـ'''\n",
    "english_punctuations = string.punctuation\n",
    "punctuations_list = arabic_punctuations + english_punctuations\n",
    "\n",
    "arabic_diacritics = re.compile(\"\"\"\n",
    "                             ّ    | # Tashdid\n",
    "                             َ    | # Fatha\n",
    "                             ً    | # Tanwin Fath\n",
    "                             ُ    | # Damma\n",
    "                             ٌ    | # Tanwin Damm\n",
    "                             ِ    | # Kasra\n",
    "                             ٍ    | # Tanwin Kasr\n",
    "                             ْ    | # Sukun\n",
    "                             ـ     # Tatwil/Kashida\n",
    "                         \"\"\", re.VERBOSE)\n",
    "\n",
    "def normalize_arabic_text(text): # Combined normalization for better clarity\n",
    "    text = araby.normalize_alef(text)\n",
    "    text = text.replace('ى', 'ي')\n",
    "    return text\n",
    "\n",
    "def remove_diacritics_text(text):\n",
    "    return re.sub(arabic_diacritics, '', text)\n",
    "\n",
    "def remove_punctuations_text(text):\n",
    "    translator = str.maketrans('', '', punctuations_list)\n",
    "    return text.translate(translator)\n",
    "\n",
    "def preprocess_arabic_text(text):\n",
    "    \"\"\"Preprocesses Arabic text for NLP tasks.\"\"\"\n",
    "    sentences = sent_tokenize(text)\n",
    "    processed_sentences = []\n",
    "    for sentence in sentences:\n",
    "        normalized_text = normalize_arabic_text(sentence)\n",
    "        normalized_text = remove_diacritics_text(normalized_text)\n",
    "        normalized_text = remove_punctuations_text(normalized_text)\n",
    "        normalized_text = ' '.join(normalized_text.split()) # Whitespace normalization\n",
    "        tokens = nltk.word_tokenize(normalized_text)\n",
    "        lemmatized_tokens = [araby.lemmatize(token) for token in tokens]\n",
    "        processed_sentences.append(\" \".join(lemmatized_tokens))\n",
    "    return processed_sentences\n",
    "\n",
    "# --- 2. Load and Preprocess Text ---\n",
    "with open(\"H:/OherTask/hadiah_app/docs/data/allBooks/alkawakib_aldiraru_fi_Sharh_Sahih_albukhari_015.txt\", \"r\", encoding=\"utf-8\") as f: # Replace with your file name\n",
    "    arabic_text = f.read()\n",
    "\n",
    "preprocessed_sentences = preprocess_arabic_text(arabic_text)\n",
    "\n",
    "# --- 3. Generate Sentence Embeddings (AraBERT) ---\n",
    "embedding_model = SentenceTransformer('aubmindlab/bert-base-arabertv02-v2')\n",
    "sentence_embeddings = embedding_model.encode(preprocessed_sentences)\n",
    "\n",
    "# --- 4. Topic Extraction using K-Means Clustering ---\n",
    "num_clusters = 7  # Experiment with the number of clusters\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=42, n_init=10)\n",
    "cluster_labels = kmeans.fit_predict(sentence_embeddings)\n",
    "\n",
    "print(\"K-Means Cluster Labels (first 20 sentences):\", cluster_labels[:20])\n",
    "\n",
    "# --- 5. Basic Semantic Search Function ---\n",
    "def semantic_search(query, embeddings, sentences, model, top_k=5):\n",
    "    \"\"\"Performs semantic search and returns top_k relevant sentences.\"\"\"\n",
    "    query_embedding = model.encode([query])[0] # Embed the query\n",
    "    similarities = cosine_similarity([query_embedding], embeddings)[0] # Calculate cosine similarity\n",
    "    top_indices = np.argsort(similarities)[::-1][:top_k] # Get indices of top k most similar sentences\n",
    "    results = [(sentences[i], similarities[i]) for i in top_indices] # Get sentences and their similarity scores\n",
    "    return results\n",
    "\n",
    "# --- 6. Example Search Query and Results ---\n",
    "search_query = \"أهمية النية في الإسلام\"  # Example search query (Importance of intention in Islam)\n",
    "search_results = semantic_search(search_query, sentence_embeddings, preprocessed_sentences, embedding_model)\n",
    "\n",
    "print(f\"\\nSearch Query: '{search_query}'\")\n",
    "print(\"\\nTop 5 Semantic Search Results:\")\n",
    "for sentence, similarity_score in search_results:\n",
    "    print(f\"Similarity: {similarity_score:.4f} - Sentence: {sentence}\")\n",
    "\n",
    "# --- 7. Example: Print Sentences in Each Cluster (for Topic Analysis) ---\n",
    "print(\"\\n--- Sentences grouped by Cluster (K-Means Topics) ---\")\n",
    "for cluster_id in range(num_clusters):\n",
    "    print(f\"\\nCluster {cluster_id + 1} Sentences:\")\n",
    "    for i, label in enumerate(cluster_labels):\n",
    "        if label == cluster_id:\n",
    "            print(f\"- {preprocessed_sentences[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-work1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
