{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":3649,"databundleVersionId":46718,"sourceType":"competition"}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-05T00:35:22.910684Z","iopub.execute_input":"2024-12-05T00:35:22.910953Z","iopub.status.idle":"2024-12-05T00:35:23.894216Z","shell.execute_reply.started":"2024-12-05T00:35:22.910917Z","shell.execute_reply":"2024-12-05T00:35:23.893260Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/cifar-10/trainLabels.csv\n/kaggle/input/cifar-10/sampleSubmission.csv\n/kaggle/input/cifar-10/test.7z\n/kaggle/input/cifar-10/train.7z\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\nfrom tqdm import tqdm\nimport torch\nfrom torch.utils.data import DataLoader\nimport torchvision\nimport torchvision.transforms as transforms\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\nfrom PIL import Image\nimport random\nimport pandas as pd\n!pip install py7zr\nimport py7zr\nfrom io import BytesIO\nimport torch.optim.lr_scheduler as lr_scheduler\n\n# Define a basic residual block with dropout\nclass ResidualBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, stride=1):\n        super(ResidualBlock, self).__init__()\n        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(out_channels)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(out_channels)\n        self.dropout = nn.Dropout(p=0.3)  # Dropout to regularize\n\n        self.skip_connection = nn.Sequential()\n        if stride != 1 or in_channels != out_channels:\n            self.skip_connection = nn.Sequential(\n                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(out_channels)\n            )\n\n    def forward(self, x):\n        identity = self.skip_connection(x)\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.dropout(out)  # Apply dropout\n        out += identity\n        out = self.relu(out)\n        return out\n\n# Define ResNet-34 for CIFAR-10\nclass ResNet34(nn.Module):\n    def __init__(self, block, num_blocks, num_classes=10):\n        super(ResNet34, self).__init__()\n        self.in_channels = 64\n\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.relu = nn.ReLU(inplace=True)\n        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n        self.fc = nn.Linear(512, num_classes)\n\n    def _make_layer(self, block, out_channels, blocks, stride):\n        strides = [stride] + [1] * (blocks - 1)\n        layers = []\n        for stride in strides:\n            layers.append(block(self.in_channels, out_channels, stride))\n            self.in_channels = out_channels\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n        out = self.layer1(out)\n        out = self.layer2(out)\n        out = self.layer3(out)\n        out = self.layer4(out)\n        out = self.avg_pool(out)\n        out = torch.flatten(out, 1)\n        out = self.fc(out)\n        return out\n\n# Function to instantiate the ResNet-34 model\ndef get_resnet34():\n    return ResNet34(ResidualBlock, [3, 4, 6, 3])\n\n# Custom CutOut transformation\nclass CutOut(object):\n    def __init__(self, n_holes, length):\n        self.n_holes = n_holes\n        self.length = length\n\n    def __call__(self, img):\n        h, w = img.size(1), img.size(2)\n        mask = np.ones((h, w), np.float32)\n\n        for n in range(self.n_holes):\n            y = random.randint(0, h)\n            x = random.randint(0, w)\n\n            y1 = np.clip(y - self.length // 2, 0, h)\n            y2 = np.clip(y + self.length // 2, 0, h)\n            x1 = np.clip(x - self.length // 2, 0, w)\n            x2 = np.clip(x + self.length // 2, 0, w)\n\n            mask[y1:y2, x1:x2] = 0.\n\n        mask = torch.from_numpy(mask).expand_as(img)\n        img = img * mask\n        return img\n\n# Transformation function using PyTorch's transforms\ntransform_train = transforms.Compose([\n    transforms.RandomHorizontalFlip(p=0.5),  # Horizontal flip\n    transforms.RandomAffine(degrees=15, translate=(0.1, 0.1), scale=(0.9, 1.1)),  # Rotate, scale, and shift\n    transforms.RandomCrop(32, padding=4),  # Random crop with padding\n    transforms.ToTensor(),\n    transforms.Normalize(mean=(0.4914, 0.4822, 0.4465), std=(0.2023, 0.1994, 0.2010)),\n    CutOut(n_holes=1, length=8),  # Use CutOut after ToTensor for regularization\n])\n\ntransform_test = transforms.Compose([\n    transforms.Resize((32, 32)),  # Resize to 32x32\n    transforms.ToTensor(),\n    transforms.Normalize(mean=(0.4914, 0.4822, 0.4465), std=(0.2023, 0.1994, 0.2010)),\n])\n\n# Example of applying the transformations to the CIFAR-10 dataset\ntrain_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\ntest_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size=256, shuffle=True)\ntest_loader = torch.utils.data.DataLoader(test_dataset, batch_size=100, shuffle=False)\n\n# Define device (GPU/CPU)\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Create model, define loss function and optimizer\nmodel = get_resnet34().to(device)\ncriterion = nn.CrossEntropyLoss()\n\n# Use AdamW optimizer with weight decay\noptimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-4)\n# Calculate steps per epoch\nsteps_per_epoch = len(train_loader)  # Number of batches in the training set\n\n# Cosine annealing scheduler with warmup\n\n# Define the OneCycleLR scheduler\nscheduler = lr_scheduler.OneCycleLR(\n    optimizer, \n    max_lr=0.001,        # The maximum learning rate after warmup\n    steps_per_epoch=steps_per_epoch,  # Total steps in one epoch (train dataset size / batch size)\n    epochs=80,           # Total number of epochs\n    pct_start=0.3,       # Warmup period (30% of the total steps)\n    anneal_strategy='cos',  # Cosine annealing after warmup\n    div_factor=25.0      # Initial learning rate will be max_lr / div_factor\n)\n\n# Training loop\ndef train(epoch):\n    model.train()\n    running_loss = 0.0\n    for batch_idx, (inputs, targets) in enumerate(train_loader):\n        inputs, targets = inputs.to(device), targets.to(device)\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs, targets)\n        loss.backward()\n        optimizer.step()\n\n        # Update the learning rate at each batch step\n        scheduler.step()\n\n        running_loss += loss.item()\n        if batch_idx % 100 == 99:\n            print(f\"Epoch {epoch+1}, Batch {batch_idx+1}: Loss = {running_loss / 100:.4f}\")\n            running_loss = 0.0\n\ndef test(epoch):\n    model.eval()\n    test_loss = 0.0\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for batch_idx, (inputs, targets) in enumerate(test_loader):\n            inputs, targets = inputs.to(device), targets.to(device)\n            outputs = model(inputs)\n            loss = criterion(outputs, targets)\n            test_loss += loss.item()\n\n            _, predicted = outputs.max(1)\n            total += targets.size(0)\n            correct += predicted.eq(targets).sum().item()\n\n        print(f\"Epoch {epoch+1}: Test Loss = {test_loss / len(test_loader):.4f}, Accuracy = {100. * correct / total:.2f}%\")\n\n# Main training loop\nfor epoch in range(80):\n    train(epoch)\n    test(epoch)\n\n# Run the test for submission\n# Prediction process omitted for brevity; you can reuse your existing test set submission process.\n\n# Create submission file with predictions for test images from .7z archive\ntest_filenames = []\ntest_images = []\n\nwith py7zr.SevenZipFile('/kaggle/input/cifar-10/test.7z', mode='r') as z:\n    for name, file in z.readall().items():\n        if name.endswith('.png'):\n            img = Image.open(BytesIO(file.read()))\n            test_images.append(transform_test(img))\n            test_filenames.append(name)\n\ntest_images = torch.stack(test_images)\ntest_loader = DataLoader(test_images, batch_size=100, shuffle=False)\n\n# Prediction and CSV creation\nclasses = ('airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n\nresult = []\nwith torch.no_grad():\n    model.eval()\n    for inputs in tqdm(test_loader):\n        inputs = inputs.to(device)\n        outputs = model(inputs)\n        _, predicted = outputs.max(1)\n        result.extend(predicted.cpu().numpy())\n\n# Create submission DataFrame\nsubmission_df = pd.DataFrame({\n    'id': [os.path.basename(f).replace('.png', '') for f in test_filenames],  # Remove .png from filenames\n    'label': [classes[label] for label in result]\n})\n\n# Save submission file\nsubmission_df.to_csv('/kaggle/working/submission.csv', index=False)\n\nprint(\"Submission file created successfully!\")","metadata":{"execution":{"iopub.status.busy":"2024-11-30T06:08:32.871734Z","iopub.execute_input":"2024-11-30T06:08:32.872261Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Collecting py7zr\n  Downloading py7zr-0.22.0-py3-none-any.whl.metadata (16 kB)\nRequirement already satisfied: texttable in /opt/conda/lib/python3.10/site-packages (from py7zr) (1.7.0)\nCollecting pycryptodomex>=3.16.0 (from py7zr)\n  Downloading pycryptodomex-3.21.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\nCollecting pyzstd>=0.15.9 (from py7zr)\n  Downloading pyzstd-0.16.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.4 kB)\nCollecting pyppmd<1.2.0,>=1.1.0 (from py7zr)\n  Downloading pyppmd-1.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.7 kB)\nCollecting pybcj<1.1.0,>=1.0.0 (from py7zr)\n  Downloading pybcj-1.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\nCollecting multivolumefile>=0.2.3 (from py7zr)\n  Downloading multivolumefile-0.2.3-py3-none-any.whl.metadata (6.3 kB)\nCollecting inflate64<1.1.0,>=1.0.0 (from py7zr)\n  Downloading inflate64-1.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\nRequirement already satisfied: brotli>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from py7zr) (1.1.0)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from py7zr) (5.9.3)\nDownloading py7zr-0.22.0-py3-none-any.whl (67 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.9/67.9 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading inflate64-1.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (93 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.1/93.1 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading multivolumefile-0.2.3-py3-none-any.whl (17 kB)\nDownloading pybcj-1.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (49 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.7/49.7 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pycryptodomex-3.21.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m69.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pyppmd-1.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.9/138.9 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pyzstd-0.16.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (413 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m413.7/413.7 kB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: pyzstd, pyppmd, pycryptodomex, pybcj, multivolumefile, inflate64, py7zr\nSuccessfully installed inflate64-1.0.0 multivolumefile-0.2.3 py7zr-0.22.0 pybcj-1.0.2 pycryptodomex-3.21.0 pyppmd-1.1.0 pyzstd-0.16.2\nDownloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 170498071/170498071 [00:11<00:00, 14324935.05it/s]\n","output_type":"stream"},{"name":"stdout","text":"Extracting ./data/cifar-10-python.tar.gz to ./data\nFiles already downloaded and verified\nEpoch 1, Batch 100: Loss = 2.1488\nEpoch 1: Test Loss = 2.8656, Accuracy = 24.68%\nEpoch 2, Batch 100: Loss = 1.7243\nEpoch 2: Test Loss = 2.1155, Accuracy = 35.03%\nEpoch 3, Batch 100: Loss = 1.5692\nEpoch 3: Test Loss = 1.7380, Accuracy = 43.46%\nEpoch 4, Batch 100: Loss = 1.4308\nEpoch 4: Test Loss = 1.4408, Accuracy = 52.41%\nEpoch 5, Batch 100: Loss = 1.2982\nEpoch 5: Test Loss = 1.2856, Accuracy = 56.45%\nEpoch 6, Batch 100: Loss = 1.2103\nEpoch 6: Test Loss = 1.1827, Accuracy = 60.42%\nEpoch 7, Batch 100: Loss = 1.1042\nEpoch 7: Test Loss = 1.1319, Accuracy = 62.62%\nEpoch 8, Batch 100: Loss = 1.0405\nEpoch 8: Test Loss = 0.9317, Accuracy = 67.84%\nEpoch 9, Batch 100: Loss = 0.9763\nEpoch 9: Test Loss = 1.1378, Accuracy = 64.79%\nEpoch 10, Batch 100: Loss = 0.8954\nEpoch 10: Test Loss = 0.8063, Accuracy = 72.42%\nEpoch 11, Batch 100: Loss = 0.8460\nEpoch 11: Test Loss = 0.7982, Accuracy = 73.11%\nEpoch 12, Batch 100: Loss = 0.7929\nEpoch 12: Test Loss = 0.7993, Accuracy = 74.01%\nEpoch 13, Batch 100: Loss = 0.7540\nEpoch 13: Test Loss = 0.7271, Accuracy = 77.39%\nEpoch 14, Batch 100: Loss = 0.7069\nEpoch 14: Test Loss = 0.7190, Accuracy = 76.59%\nEpoch 15, Batch 100: Loss = 0.6826\nEpoch 15: Test Loss = 0.6140, Accuracy = 80.02%\nEpoch 16, Batch 100: Loss = 0.6604\nEpoch 16: Test Loss = 0.6655, Accuracy = 78.63%\nEpoch 17, Batch 100: Loss = 0.6269\nEpoch 17: Test Loss = 0.6090, Accuracy = 80.49%\nEpoch 18, Batch 100: Loss = 0.6007\nEpoch 18: Test Loss = 0.6740, Accuracy = 78.82%\nEpoch 19, Batch 100: Loss = 0.5721\nEpoch 19: Test Loss = 0.5747, Accuracy = 80.97%\nEpoch 20, Batch 100: Loss = 0.5581\nEpoch 20: Test Loss = 0.4367, Accuracy = 84.99%\nEpoch 21, Batch 100: Loss = 0.5360\nEpoch 21: Test Loss = 0.4819, Accuracy = 84.16%\nEpoch 22, Batch 100: Loss = 0.5144\nEpoch 22: Test Loss = 0.4989, Accuracy = 83.44%\nEpoch 23, Batch 100: Loss = 0.4990\nEpoch 23: Test Loss = 0.4003, Accuracy = 86.64%\nEpoch 24, Batch 100: Loss = 0.4697\nEpoch 24: Test Loss = 0.5571, Accuracy = 82.69%\nEpoch 25, Batch 100: Loss = 0.4617\nEpoch 25: Test Loss = 0.4994, Accuracy = 84.00%\nEpoch 26, Batch 100: Loss = 0.4417\nEpoch 26: Test Loss = 0.4128, Accuracy = 86.63%\nEpoch 27, Batch 100: Loss = 0.4297\nEpoch 27: Test Loss = 0.4577, Accuracy = 85.88%\nEpoch 28, Batch 100: Loss = 0.4171\nEpoch 28: Test Loss = 0.4333, Accuracy = 86.14%\nEpoch 29, Batch 100: Loss = 0.4121\nEpoch 29: Test Loss = 0.3591, Accuracy = 88.46%\nEpoch 30, Batch 100: Loss = 0.3895\nEpoch 30: Test Loss = 0.4150, Accuracy = 86.79%\nEpoch 31, Batch 100: Loss = 0.3773\nEpoch 31: Test Loss = 0.4503, Accuracy = 85.48%\nEpoch 32, Batch 100: Loss = 0.3641\nEpoch 32: Test Loss = 0.3346, Accuracy = 88.94%\nEpoch 33, Batch 100: Loss = 0.3490\nEpoch 33: Test Loss = 0.3611, Accuracy = 88.27%\nEpoch 34, Batch 100: Loss = 0.3498\nEpoch 34: Test Loss = 0.3111, Accuracy = 89.71%\nEpoch 35, Batch 100: Loss = 0.3312\nEpoch 35: Test Loss = 0.3273, Accuracy = 89.12%\nEpoch 36, Batch 100: Loss = 0.3258\nEpoch 36: Test Loss = 0.3167, Accuracy = 89.69%\nEpoch 37, Batch 100: Loss = 0.3116\nEpoch 37: Test Loss = 0.3317, Accuracy = 89.44%\nEpoch 38, Batch 100: Loss = 0.3018\nEpoch 38: Test Loss = 0.3016, Accuracy = 90.46%\nEpoch 39, Batch 100: Loss = 0.2967\nEpoch 39: Test Loss = 0.3319, Accuracy = 89.87%\nEpoch 40, Batch 100: Loss = 0.2793\nEpoch 40: Test Loss = 0.2747, Accuracy = 91.17%\nEpoch 41, Batch 100: Loss = 0.2732\nEpoch 41: Test Loss = 0.3522, Accuracy = 89.46%\nEpoch 42, Batch 100: Loss = 0.2543\nEpoch 42: Test Loss = 0.2760, Accuracy = 91.55%\nEpoch 43, Batch 100: Loss = 0.2507\nEpoch 43: Test Loss = 0.2633, Accuracy = 91.87%\nEpoch 44, Batch 100: Loss = 0.2490\nEpoch 44: Test Loss = 0.2972, Accuracy = 91.10%\nEpoch 45, Batch 100: Loss = 0.2363\nEpoch 45: Test Loss = 0.2684, Accuracy = 91.74%\nEpoch 46, Batch 100: Loss = 0.2269\nEpoch 46: Test Loss = 0.2598, Accuracy = 91.95%\nEpoch 47, Batch 100: Loss = 0.2192\nEpoch 47: Test Loss = 0.2879, Accuracy = 91.33%\nEpoch 48, Batch 100: Loss = 0.2168\nEpoch 48: Test Loss = 0.2510, Accuracy = 92.19%\nEpoch 49, Batch 100: Loss = 0.2004\nEpoch 49: Test Loss = 0.2505, Accuracy = 92.23%\nEpoch 50, Batch 100: Loss = 0.1976\nEpoch 50: Test Loss = 0.2500, Accuracy = 92.22%\nEpoch 51, Batch 100: Loss = 0.1869\nEpoch 51: Test Loss = 0.2482, Accuracy = 92.97%\nEpoch 52, Batch 100: Loss = 0.1787\nEpoch 52: Test Loss = 0.2325, Accuracy = 93.08%\nEpoch 53, Batch 100: Loss = 0.1704\nEpoch 53: Test Loss = 0.2232, Accuracy = 93.48%\nEpoch 54, Batch 100: Loss = 0.1622\nEpoch 54: Test Loss = 0.2241, Accuracy = 93.38%\nEpoch 55, Batch 100: Loss = 0.1590\nEpoch 55: Test Loss = 0.2423, Accuracy = 92.75%\nEpoch 56, Batch 100: Loss = 0.1505\nEpoch 56: Test Loss = 0.2349, Accuracy = 93.31%\nEpoch 57, Batch 100: Loss = 0.1376\nEpoch 57: Test Loss = 0.2293, Accuracy = 93.53%\nEpoch 58, Batch 100: Loss = 0.1327\nEpoch 58: Test Loss = 0.2208, Accuracy = 93.54%\nEpoch 59, Batch 100: Loss = 0.1318\nEpoch 59: Test Loss = 0.2288, Accuracy = 93.64%\nEpoch 60, Batch 100: Loss = 0.1177\nEpoch 60: Test Loss = 0.2293, Accuracy = 93.68%\nEpoch 61, Batch 100: Loss = 0.1169\nEpoch 61: Test Loss = 0.2248, Accuracy = 93.63%\nEpoch 62, Batch 100: Loss = 0.1084\nEpoch 62: Test Loss = 0.2163, Accuracy = 93.97%\nEpoch 63, Batch 100: Loss = 0.1031\nEpoch 63: Test Loss = 0.2249, Accuracy = 93.60%\nEpoch 64, Batch 100: Loss = 0.0990\nEpoch 64: Test Loss = 0.2209, Accuracy = 93.99%\nEpoch 65, Batch 100: Loss = 0.0944\nEpoch 65: Test Loss = 0.2246, Accuracy = 94.06%\nEpoch 66, Batch 100: Loss = 0.0905\nEpoch 66: Test Loss = 0.2323, Accuracy = 93.99%\nEpoch 67, Batch 100: Loss = 0.0875\nEpoch 67: Test Loss = 0.2171, Accuracy = 94.12%\nEpoch 68, Batch 100: Loss = 0.0813\nEpoch 68: Test Loss = 0.2201, Accuracy = 94.45%\nEpoch 69, Batch 100: Loss = 0.0805\nEpoch 69: Test Loss = 0.2205, Accuracy = 94.22%\nEpoch 70, Batch 100: Loss = 0.0773\nEpoch 70: Test Loss = 0.2209, Accuracy = 94.30%\nEpoch 71, Batch 100: Loss = 0.0701\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Pre-written test code","metadata":{}},{"cell_type":"code","source":"!pip install onnx-tool","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-01T04:44:10.975034Z","iopub.execute_input":"2024-12-01T04:44:10.975444Z","iopub.status.idle":"2024-12-01T04:44:23.712450Z","shell.execute_reply.started":"2024-12-01T04:44:10.975406Z","shell.execute_reply":"2024-12-01T04:44:23.710964Z"}},"outputs":[{"name":"stdout","text":"Collecting onnx-tool\n  Downloading onnx_tool-0.9.0-py3-none-any.whl.metadata (9.6 kB)\nRequirement already satisfied: onnx in /opt/conda/lib/python3.10/site-packages (from onnx-tool) (1.17.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from onnx-tool) (1.26.4)\nRequirement already satisfied: tabulate in /opt/conda/lib/python3.10/site-packages (from onnx-tool) (0.9.0)\nRequirement already satisfied: protobuf>=3.20.2 in /opt/conda/lib/python3.10/site-packages (from onnx->onnx-tool) (3.20.3)\nDownloading onnx_tool-0.9.0-py3-none-any.whl (44 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.5/44.5 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: onnx-tool\nSuccessfully installed onnx-tool-0.9.0\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms\nimport math\nimport re\nimport onnx_tool\nimport torch.onnx\nimport yaml\nimport os\n\n###################################################################################################\n##############################  This is just a crap code made by me. ############################## \n###################################################################################################\n\n\n# ========== Modifiable Parameters ==========\nnum_classes = 10  # Number of classes for CIFAR-10\nepochs = 50  # Number of training epochs\nbatch_size = 128  # Batch size for DataLoader\nwidth_factor = 0.36  # Width scaling factor for the model layers\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'  # Use CUDA if available, else CPU\nlr_values = [0.1]  # List of learning rates to try\n# ============================================\n\n# Function to format numbers for filenames\ndef format_number_filename(num):\n    if abs(num) >= 1_000_000:\n        return f'{int(num / 1_000_000)}M'  # Round to the nearest million\n    elif abs(num) >= 1_000:\n        return f'{int(num / 1_000)}k'  # Round to the nearest thousand\n    else:\n        return str(num)\n\n# Function to format large numbers for readability\ndef format_number(num):\n    if abs(num) >= 1_000_000:\n        return f'{num / 1_000_000:.1f}M'  # Format in millions\n    elif abs(num) >= 1_000:\n        return f'{num / 1_000:.1f}k'  # Format in thousands\n    else:\n        return str(num)\n\n# Function to round numbers to the nearest significant digit\ndef round_significant(x, digits=2):\n    if x == 0:\n        return 0\n    else:\n        return round(x, -int(math.floor(math.log10(abs(x))) - (digits - 1)))\n\n# Function to calculate FLOPs using ONNX\ndef calculate_flops_onnx(model):\n    # Generate a dummy input for the model with CIFAR-10 dimensions (3x32x32)\n    dummy_input = torch.randn(1, 3, 32, 32).to(device)\n    \n    # Paths to save the ONNX model and profile\n    onnx_path = \"tmp.onnx\"\n    profile_path = \"profile.txt\"\n    \n    # Export the PyTorch model to ONNX format\n    torch.onnx.export(model,\n                      dummy_input,\n                      onnx_path,\n                      export_params=True,\n                      opset_version=12,\n                      do_constant_folding=True,\n                      input_names=['input'],\n                      output_names=['output'],\n                      dynamic_axes=None)\n    \n    # Profile the ONNX model to calculate the number of MACs\n    onnx_tool.model_profile(onnx_path, save_profile=profile_path)\n    \n    # Read and parse the profile to extract total MACs\n    with open(profile_path, 'r') as file:\n        profile = file.read()\n    \n    # Use regex to find the total MACs in the profile\n    match = re.search(r'Total\\s+_\\s+([\\d,]+)\\s+100%', profile)\n    \n    if match:\n        total_macs = match.group(1)\n        total_macs = int(total_macs.replace(',', ''))  # Remove commas for calculation\n        total_macs = round_significant(total_macs)\n        return total_macs\n    else:\n        return None\n\n# ECABlock class that adds channel-wise attention to the model\nclass ECABlock(nn.Module):\n    def __init__(self, channels, gamma=4, b=24):\n        super(ECABlock, self).__init__()\n        \n        # Calculate kernel size based on input channel size\n        t = int(abs((math.log(channels, 2) + b) / gamma))\n        kernel_size = t if t % 2 else t + 1\n        \n        # Define average pooling and 1D convolution\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.conv = nn.Conv1d(1, 1, kernel_size=kernel_size, padding=(kernel_size - 1) // 2, bias=False)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        # Apply global average pooling and convolution to calculate channel-wise attention\n        y = self.avg_pool(x)\n        y = y.squeeze(-1).transpose(-1, -2)\n        y = self.conv(y)\n        y = self.sigmoid(y)\n        y = y.transpose(-1, -2).unsqueeze(-1)\n        \n        return x * y.expand_as(x)  # Element-wise multiplication for channel attention\n\n# InvertedResidual block that can optionally use ECABlock for attention\nclass InvertedResidual(nn.Module):\n    def __init__(self, inp, oup, stride, expand_ratio, use_eca=False):\n        super(InvertedResidual, self).__init__()\n        \n        # Hidden dimension after expansion\n        hidden_dim = int(inp * expand_ratio)\n        \n        # Check if residual connection is applicable\n        self.use_res_connect = (stride == 1 and inp == oup)\n\n        # Build layers: expansion, depthwise convolution, pointwise convolution\n        layers = []\n        if expand_ratio != 1:\n            layers.extend([nn.Conv2d(inp, hidden_dim, 1, bias=False),\n                           nn.BatchNorm2d(hidden_dim),\n                           nn.GELU()])  # Use GELU activation\n        \n        # Add depthwise convolution and batch norm\n        layers.extend([nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False),\n                       nn.BatchNorm2d(hidden_dim),\n                       nn.GELU()])\n        \n        # Optionally add ECABlock for attention\n        if use_eca:\n            layers.append(ECABlock(hidden_dim))\n\n        # Add final pointwise convolution\n        layers.extend([nn.Conv2d(hidden_dim, oup, 1, bias=False),\n                       nn.BatchNorm2d(oup)])\n\n        self.conv = nn.Sequential(*layers)  # Define the sequential model\n\n    def forward(self, x):\n        # Forward pass through convolution layers\n        out = self.conv(x)\n        \n        # Add residual connection if applicable\n        if self.use_res_connect:\n            return x + out\n        else:\n            return out\n\n\n# Define the MobileNetECA architecture\n#This class takes as input a  block_settings.yml which contains a list of different block settings, trains each block setting with a learning rate of 0.1, and saves the trained models.\n#But  now  learning rate is set to 0.1, trained, and later on  re-trained with different learning rates.\nclass MobileNetECA(nn.Module):\n    def __init__(self, num_classes=10, width_mult=0.2, block_settings=None):\n        super(MobileNetECA, self).__init__()\n\n        # Default block_settings if not provided\n        if block_settings is None:\n            block_settings = [\n                [2, 24, 2, 1, True],  # Block 1\n                [4, 24, 3, 2, True],  # Block 2\n                [8, 36, 3, 2, True],  # Block 3\n                [8, 44, 3, 1, True],  # Block 4\n            ]\n\n        \n        # Calculate input and output channel sizes based on width factor\n        input_channel = max(int(36 * width_mult), 8)\n        last_channel = max(int(144 * width_mult), 8)\n\n        # First convolution layer\n        self.features = [nn.Sequential(\n            nn.Conv2d(3, input_channel, 3, stride=1, padding=1, bias=False),\n            nn.BatchNorm2d(input_channel),\n            nn.GELU()\n        )]\n\n        # Add inverted residual blocks\n        for idx, (t, c, n, s, use_eca) in enumerate(block_settings):\n            output_channel = max(int(c * width_mult), 8)\n            for i in range(n):\n                stride = s if i == 0 else 1  # First layer in block may have stride > 1\n                self.features.append(InvertedResidual(input_channel, output_channel, stride, expand_ratio=t, use_eca=use_eca))\n                input_channel = output_channel\n\n        # Final convolution layer\n        self.features.append(nn.Sequential(\n            nn.Conv2d(input_channel, last_channel, 1, bias=False),\n            nn.BatchNorm2d(last_channel),\n            nn.GELU(),\n            nn.AdaptiveAvgPool2d(1)\n        ))\n\n        self.features = nn.Sequential(*self.features)  # Combine all layers\n\n        # Final classifier layer\n        self.pool = nn.AdaptiveAvgPool2d(1)\n        self.classifier = nn.Linear(last_channel, num_classes)\n\n        # Initialize weights\n        self._initialize_weights()\n\n    def forward(self, x):\n        # Forward pass through feature extractor and classifier\n        x = self.features(x)\n        x = self.pool(x).flatten(1)\n        x = self.classifier(x)\n        return x\n\n    # Function to initialize model weights\n    def _initialize_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n                if m.bias is not None:\n                    nn.init.zeros_(m.bias)\n            elif isinstance(m, nn.Linear):\n                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n                if m.bias is not None:\n                    nn.init.zeros_(m.bias)\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.ones_(m.weight)\n                nn.init.zeros_(m.bias)\n\n# Data augmentation and normalization for CIFAR-10 training dataset\ntrain_transforms = transforms.Compose([\n    transforms.RandomHorizontalFlip(),  # Random horizontal flip\n    transforms.RandomCrop(32, padding=4),  # Random crop with padding\n    transforms.ToTensor(),  # Convert image to tensor\n    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))  # Normalize based on dataset statistics\n])\n\n# Normalization for test data\ntest_transforms = transforms.Compose([\n    transforms.ToTensor(),  # Convert image to tensor\n    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))  # Normalize based on dataset statistics\n])\n\n# Load CIFAR-10 training and test datasets\ntrain_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=train_transforms)\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n\ntest_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=test_transforms)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n\n# Load block_settings from external YAML file\nwith open('block_settings.yaml', 'r') as f:\n    block_settings_dict = yaml.safe_load(f)\n\n\n\n# Function to train and evaluate the model with each learning rate\nfor block_settings_name, block_settings in block_settings_dict.items():\n    for lr in lr_values:\n        # Create the MobileNetECA model with the provided block_settings and move it to the appropriate device\n        model = MobileNetECA(num_classes=num_classes, width_mult=width_factor, block_settings=block_settings).to(device)\n        \n        # Calculate the number of parameters and FLOPs of the model\n        params = sum(p.numel() for p in model.parameters())\n        macs = calculate_flops_onnx(model)\n        \n\n        formatted_params = format_number(params)\n        formatted_macs = format_number(macs)\n        print(f\"Total number of parameters for lr={lr}: {formatted_params}\")\n        \n        # Set up the optimizer (SGD) and learning rate scheduler (Cosine Annealing)\n        optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=3e-4)\n        scheduler = CosineAnnealingLR(optimizer, T_max=epochs)\n\n        # Loss function: Cross-Entropy Loss\n        criterion = nn.CrossEntropyLoss()\n\n        # Function to train for one epoch\n        def train():\n            model.train()  # Set model to training mode\n            correct = 0\n            total = 0\n\n            for inputs, targets in train_loader:\n                inputs, targets = inputs.to(device), targets.to(device)\n                optimizer.zero_grad()  # Reset gradients\n                outputs = model(inputs)  # Forward pass\n                loss = criterion(outputs, targets)  # Calculate loss\n                loss.backward()  # Backpropagation\n                nn.utils.clip_grad_norm_(model.parameters(), max_norm=5)  # Gradient clipping\n                optimizer.step()  # Update weights\n\n                _, predicted = outputs.max(1)  # Get predicted class\n                total += targets.size(0)\n                correct += predicted.eq(targets).sum().item()\n\n            accuracy = 100. * correct / total  # Calculate accuracy\n            return accuracy\n\n        # Function to validate on the test set\n        def validate():\n            model.eval()  # Set model to evaluation mode\n            correct = 0\n            total = 0\n\n            with torch.no_grad():\n                for inputs, targets in test_loader:\n                    inputs, targets = inputs.to(device), targets.to(device)\n                    outputs = model(inputs)  # Forward pass\n                    loss = criterion(outputs, targets)  # Calculate loss\n\n                    _, predicted = outputs.max(1)  # Get predicted class\n                    total += targets.size(0)\n                    correct += predicted.eq(targets).sum().item()\n\n            accuracy = 100. * correct / total  # Calculate accuracy\n            return accuracy\n\n        # Calculate and print parameters and MACs\n        print(f\"------ Rounded Parameters for lr={lr} ------\")\n        params = sum(param.numel() for param in model.parameters())  # Total number of parameters\n        params = round_significant(params)\n        macs = calculate_flops_onnx(model)  # Calculate FLOPs using ONNX\n        formatted_params = format_number(params)\n        formatted_macs = format_number(macs)\n        print(f\"Params: {formatted_params}  MACS: {formatted_macs}\")\n\n        # Training loop for multiple epochs\n        for epoch in range(epochs):\n            acc_train = train()  # Train for one epoch\n            acc_valid = validate()  # Validate on the test set\n            scheduler.step()  # Update learning rate\n            print(f'Epoch {epoch+1} - Training Accuracy: {acc_train:.2f}% - Validation Accuracy: {acc_valid:.2f}%')\n\n        # Format parameters, MACs, accuracy, and learning rate for saving the model\n        params_str = format_number_filename(params)\n        macs_str = format_number_filename(macs)\n        acc_str = f\"{acc_valid:.1f}\".replace('.', '_')  # Format accuracy like 84.4% -> 84_4\n        lr_str = f\"{lr:.2f}\".replace('.', '_').rstrip('0').rstrip('_')  # Format learning rate\n\n        # Add the block_settings name to the filename\n        block_name_str = re.sub(r'\\W+', '_', block_settings_name)\n\n        # Save the trained model using TorchScript with a formatted filename\n        model_save_dir = '/kaggle/working/'\n        os.makedirs(model_save_dir, exist_ok=True)\n        model_path = os.path.join(model_save_dir, f'{block_name_str}_{params_str}_{macs_str}_{acc_str}_{lr_str}.pt')\n        scripted_model = torch.jit.script(model)\n        scripted_model.save(model_path)\n        print(f\"Model saved as '{model_path}'\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-01T04:45:09.580295Z","iopub.execute_input":"2024-12-01T04:45:09.580704Z","iopub.status.idle":"2024-12-01T04:45:11.601917Z","shell.execute_reply.started":"2024-12-01T04:45:09.580668Z","shell.execute_reply":"2024-12-01T04:45:11.600537Z"}},"outputs":[{"name":"stdout","text":"Files already downloaded and verified\nFiles already downloaded and verified\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[5], line 256\u001b[0m\n\u001b[1;32m    253\u001b[0m test_loader \u001b[38;5;241m=\u001b[39m DataLoader(test_dataset, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, pin_memory\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    255\u001b[0m \u001b[38;5;66;03m# Load block_settings from external YAML file\u001b[39;00m\n\u001b[0;32m--> 256\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mblock_settings.yaml\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    257\u001b[0m     block_settings_dict \u001b[38;5;241m=\u001b[39m yaml\u001b[38;5;241m.\u001b[39msafe_load(f)\n\u001b[1;32m    261\u001b[0m \u001b[38;5;66;03m# Function to train and evaluate the model with each learning rate\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py:310\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    305\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    306\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    307\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    308\u001b[0m     )\n\u001b[0;32m--> 310\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'block_settings.yaml'"],"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: 'block_settings.yaml'","output_type":"error"}],"execution_count":5},{"cell_type":"markdown","source":"### 97.5","metadata":{}},{"cell_type":"code","source":"!pip install py7zr","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T09:34:51.382969Z","iopub.execute_input":"2024-12-04T09:34:51.383812Z","iopub.status.idle":"2024-12-04T09:35:01.410670Z","shell.execute_reply.started":"2024-12-04T09:34:51.383776Z","shell.execute_reply":"2024-12-04T09:35:01.409544Z"}},"outputs":[{"name":"stdout","text":"Collecting py7zr\n  Downloading py7zr-0.22.0-py3-none-any.whl.metadata (16 kB)\nRequirement already satisfied: texttable in /opt/conda/lib/python3.10/site-packages (from py7zr) (1.7.0)\nCollecting pycryptodomex>=3.16.0 (from py7zr)\n  Downloading pycryptodomex-3.21.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\nCollecting pyzstd>=0.15.9 (from py7zr)\n  Downloading pyzstd-0.16.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.4 kB)\nCollecting pyppmd<1.2.0,>=1.1.0 (from py7zr)\n  Downloading pyppmd-1.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.7 kB)\nCollecting pybcj<1.1.0,>=1.0.0 (from py7zr)\n  Downloading pybcj-1.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\nCollecting multivolumefile>=0.2.3 (from py7zr)\n  Downloading multivolumefile-0.2.3-py3-none-any.whl.metadata (6.3 kB)\nCollecting inflate64<1.1.0,>=1.0.0 (from py7zr)\n  Downloading inflate64-1.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\nRequirement already satisfied: brotli>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from py7zr) (1.1.0)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from py7zr) (5.9.3)\nDownloading py7zr-0.22.0-py3-none-any.whl (67 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.9/67.9 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading inflate64-1.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (93 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.1/93.1 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading multivolumefile-0.2.3-py3-none-any.whl (17 kB)\nDownloading pybcj-1.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (49 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.7/49.7 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pycryptodomex-3.21.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m47.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading pyppmd-1.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.9/138.9 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pyzstd-0.16.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (413 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m413.7/413.7 kB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: pyzstd, pyppmd, pycryptodomex, pybcj, multivolumefile, inflate64, py7zr\nSuccessfully installed inflate64-1.0.0 multivolumefile-0.2.3 py7zr-0.22.0 pybcj-1.0.2 pycryptodomex-3.21.0 pyppmd-1.1.0 pyzstd-0.16.2\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"import os\nimport re\nimport math\nimport random\nimport yaml\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader, Dataset\nfrom tqdm import tqdm\nimport numpy as np\nfrom PIL import Image\nimport py7zr\nimport pandas as pd\nfrom io import BytesIO\nimport torch.optim.lr_scheduler as lr_scheduler\n\n# 1. Print all the files in the input directory to verify paths\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-01T09:38:36.154103Z","iopub.execute_input":"2024-12-01T09:38:36.154518Z","iopub.status.idle":"2024-12-01T09:38:43.751380Z","shell.execute_reply.started":"2024-12-01T09:38:36.154477Z","shell.execute_reply":"2024-12-01T09:38:43.750335Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/cifar-10/trainLabels.csv\n/kaggle/input/cifar-10/sampleSubmission.csv\n/kaggle/input/cifar-10/test.7z\n/kaggle/input/cifar-10/train.7z\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import py7zr\nimport os\n\n# Extract train and test images\nwith py7zr.SevenZipFile('/kaggle/input/cifar-10/train.7z', mode='r') as z:\n    z.extractall(path='/kaggle/working/train_images')\n    \nwith py7zr.SevenZipFile('/kaggle/input/cifar-10/test.7z', mode='r') as z:\n    z.extractall(path='/kaggle/working/test_images')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-01T09:38:43.752505Z","iopub.execute_input":"2024-12-01T09:38:43.752925Z","iopub.status.idle":"2024-12-01T09:41:16.081834Z","shell.execute_reply.started":"2024-12-01T09:38:43.752897Z","shell.execute_reply":"2024-12-01T09:41:16.080847Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"class CutOut(object):\n    def __init__(self, n_holes, length):\n        self.n_holes = n_holes\n        self.length = length\n\n    def __call__(self, img):\n        h, w = img.size(1), img.size(2)\n        mask = np.ones((h, w), np.float32)\n\n        for n in range(self.n_holes):\n            y = random.randint(0, h)\n            x = random.randint(0, w)\n\n            y1 = np.clip(y - self.length // 2, 0, h)\n            y2 = np.clip(y + self.length // 2, 0, h)\n            x1 = np.clip(x - self.length // 2, 0, w)\n            x2 = np.clip(x + self.length // 2, 0, w)\n\n            mask[y1:y2, x1:x2] = 0.\n\n        mask = torch.from_numpy(mask).expand_as(img)\n        img = img * mask\n        return img","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-01T09:42:06.081915Z","iopub.execute_input":"2024-12-01T09:42:06.082567Z","iopub.status.idle":"2024-12-01T09:42:06.088785Z","shell.execute_reply.started":"2024-12-01T09:42:06.082532Z","shell.execute_reply":"2024-12-01T09:42:06.087939Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# import py7zr\n# import os\n\n# # Extract train and test images\n# with py7zr.SevenZipFile('/kaggle/input/cifar-10/train.7z', mode='r') as z:\n#     z.extractall(path='/kaggle/working/train_images')\n    \n# with py7zr.SevenZipFile('/kaggle/input/cifar-10/test.7z', mode='r') as z:\n#     z.extractall(path='/kaggle/working/test_images')# Load the train labels\ntrain_labels_df = pd.read_csv('/kaggle/input/cifar-10/trainLabels.csv')\n\n# Map image filenames with labels\ntrain_filenames = [os.path.join('/kaggle/working/train_images/train', f\"{i}.png\") for i in train_labels_df['id']]\ntrain_labels = train_labels_df['label'].values\n\n# CIFAR-10 classes (convert labels from string to index)\nclasses = ('airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\nlabel_to_index = {label: idx for idx, label in enumerate(classes)}\ntrain_labels = [label_to_index[label] for label in train_labels]\n\n# Data Augmentation with CutOut (RandomErasing) for training data\n# train_transform = transforms.Compose([\n#     transforms.RandomHorizontalFlip(),\n#     transforms.RandomCrop(32, padding=4),\n#     transforms.ToTensor(),\n#     transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n#     RandomErasing(scale=(0.02, 0.33))\n# ])\n\n# # Define a simpler transform for test data\n# test_transform = transforms.Compose([\n#     transforms.ToTensor(),\n#     transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n# ])\n\ntrain_transform = transforms.Compose([\n    transforms.RandomHorizontalFlip(p=0.5),\n    transforms.RandomAffine(degrees=15, translate=(0.1, 0.1), scale=(0.9, 1.1)),  # Rotate, scale, shift\n    transforms.RandomCrop(32, padding=4),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=(0.4914, 0.4822, 0.4465), std=(0.2023, 0.1994, 0.2010)),\n    CutOut(n_holes=1, length=8),\n])\n\ntest_transform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize(mean=(0.4914, 0.4822, 0.4465), std=(0.2023, 0.1994, 0.2010)),\n])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-01T05:21:23.707492Z","iopub.execute_input":"2024-12-01T05:21:23.707759Z","iopub.status.idle":"2024-12-01T05:21:23.810843Z","shell.execute_reply.started":"2024-12-01T05:21:23.707723Z","shell.execute_reply":"2024-12-01T05:21:23.810033Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# Create a custom dataset class for the training data\nclass CIFAR10CustomDataset(Dataset):\n    def __init__(self, image_paths, labels, transform=None):\n        self.image_paths = image_paths\n        self.labels = labels\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.image_paths)\n\n    def __getitem__(self, idx):\n        img_path = self.image_paths[idx]\n        image = Image.open(img_path)\n        label = self.labels[idx]\n\n        if self.transform:\n            image = self.transform(image)\n\n        return image, label\n\n# Create the dataset and dataloader for training data\ntrain_dataset = CIFAR10CustomDataset(train_filenames, train_labels, transform=train_transform)\ntrain_loader = DataLoader(dataset=train_dataset, batch_size=128, shuffle=True, num_workers=2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-01T05:21:23.811810Z","iopub.execute_input":"2024-12-01T05:21:23.812050Z","iopub.status.idle":"2024-12-01T05:21:23.817801Z","shell.execute_reply.started":"2024-12-01T05:21:23.812025Z","shell.execute_reply":"2024-12-01T05:21:23.816973Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# Define the Residual Block and ResNet Model (same as before)\n# class ResidualBlock(nn.Module):\n#     def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n#         super(ResidualBlock, self).__init__()\n#         self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n#         self.bn1 = nn.BatchNorm2d(out_channels)\n#         self.relu = nn.ReLU(inplace=True)\n#         self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False)\n#         self.bn2 = nn.BatchNorm2d(out_channels)\n#         self.downsample = downsample\n\n#     def forward(self, x):\n#         residual = x\n#         out = self.conv1(x)\n#         out = self.bn1(out)\n#         out = self.relu(out)\n#         out = self.conv2(out)\n#         out = self.bn2(out)\n#         if self.downsample:\n#             residual = self.downsample(x)\n#         out += residual\n#         out = self.relu(out)\n#         return out\n\n# class ResNet(nn.Module):\n#     def __init__(self, block, layers, num_classes=10):\n#         super(ResNet, self).__init__()\n#         self.in_channels = 64\n#         self.conv = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n#         self.bn = nn.BatchNorm2d(64)\n#         self.relu = nn.ReLU(inplace=True)\n#         self.layer1 = self.make_layer(block, 64, layers[0])\n#         self.layer2 = self.make_layer(block, 128, layers[1], stride=2)\n#         self.layer3 = self.make_layer(block, 256, layers[2], stride=2)\n#         self.layer4 = self.make_layer(block, 512, layers[3], stride=2)\n#         self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n#         self.fc = nn.Linear(512, num_classes)\n\n#     def make_layer(self, block, out_channels, blocks, stride=1):\n#         downsample = None\n#         if stride != 1 or self.in_channels != out_channels:\n#             downsample = nn.Sequential(\n#                 nn.Conv2d(self.in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n#                 nn.BatchNorm2d(out_channels)\n#             )\n#         layers = []\n#         layers.append(block(self.in_channels, out_channels, stride, downsample))\n#         self.in_channels = out_channels\n#         for _ in range(1, blocks):\n#             layers.append(block(out_channels, out_channels))\n#         return nn.Sequential(*layers)\n\n#     def forward(self, x):\n#         x = self.conv(x)\n#         x = self.bn(x)\n#         x = self.relu(x)\n#         x = self.layer1(x)\n#         x = self.layer2(x)\n#         x = self.layer3(x)\n#         x = self.layer4(x)\n#         x = self.avg_pool(x)\n#         x = torch.flatten(x, 1)\n#         x = self.fc(x)\n#         return x\n\nclass ResidualBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n        super(ResidualBlock, self).__init__()\n        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(out_channels)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(out_channels)\n        self.downsample = downsample\n\n    def forward(self, x):\n        residual = x\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n        out = self.conv2(out)\n        out = self.bn2(out)\n        if self.downsample:\n            residual = self.downsample(x)\n        out += residual\n        out = self.relu(out)\n        return out\n\n\nclass ResNet(nn.Module):\n    def __init__(self, block, layers, num_classes=10, width_mult=1.0):\n        super(ResNet, self).__init__()\n        self.in_channels = int(64 * width_mult)  # Crucial: Use int()\n        self.conv1 = nn.Conv2d(3, self.in_channels, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(self.in_channels)\n        self.relu = nn.ReLU(inplace=True)\n        self.layer1 = self.make_layer(block, int(64 * width_mult), layers[0], stride=1)\n        self.layer2 = self.make_layer(block, int(128 * width_mult), layers[1], stride=2)\n        self.layer3 = self.make_layer(block, int(256 * width_mult), layers[2], stride=2)\n        self.layer4 = self.make_layer(block, int(512 * width_mult), layers[3], stride=2)\n        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n        self.fc = nn.Linear(int(512 * width_mult), num_classes)  # Correct output\n\n    def make_layer(self, block, out_channels, blocks, stride=1):\n        downsample = None\n        if stride != 1 or self.in_channels != out_channels:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(out_channels)\n            )\n        layers = []\n        layers.append(block(self.in_channels, out_channels, stride, downsample))\n        self.in_channels = out_channels\n        for _ in range(1, blocks):\n            layers.append(block(out_channels, out_channels))\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        x = self.avg_pool(x)\n        x = torch.flatten(x, 1)\n        x = self.fc(x)\n        return x\n\n# Initialize model, loss function, optimizer, and learning rate scheduler\n# model = ResNet(ResidualBlock, [2, 2, 2, 2]).cuda()\n# criterion = nn.CrossEntropyLoss()\n# optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-4)\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = ResNet(ResidualBlock, [2, 2, 2, 2], width_mult=0.5).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=3e-4)\nscheduler = lr_scheduler.OneCycleLR(\n    optimizer,\n    max_lr=0.1,  # Adjust max_lr\n    steps_per_epoch=len(train_loader),\n    epochs=50,\n    pct_start=0.3,\n    anneal_strategy='cos',\n    div_factor=25.0\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-01T05:21:23.818849Z","iopub.execute_input":"2024-12-01T05:21:23.819151Z","iopub.status.idle":"2024-12-01T05:21:24.158628Z","shell.execute_reply.started":"2024-12-01T05:21:23.819111Z","shell.execute_reply":"2024-12-01T05:21:24.157576Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# Cosine Annealing with Warmup\ndef warmup_cosine_lr_scheduler(optimizer, warmup_iters, max_iters):\n    def lr_lambda(current_iter):\n        if current_iter < warmup_iters:\n            return float(current_iter) / float(warmup_iters)  # Warmup phase\n        else:\n            # Cosine annealing phase\n            return 0.5 * (1 + math.cos(float(current_iter - warmup_iters) / float(max_iters - warmup_iters) * math.pi))\n    return LambdaLR(optimizer, lr_lambda)\n\n# Training and Testing Functions (same as before)\ndef train(model, train_loader, criterion, optimizer, scheduler):\n    model.train()\n    running_loss = 0.0\n    correct = 0\n    total = 0\n    for images, labels in tqdm(train_loader):\n        images, labels = images.cuda(), labels.cuda()\n        optimizer.zero_grad()\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item()\n        _, predicted = outputs.max(1)\n        total += labels.size(0)\n        correct += predicted.eq(labels).sum().item()\n    scheduler.step()\n    print(f\"Train Loss: {running_loss/len(train_loader)}, Train Accuracy: {100 * correct/total:.2f}%\")\n\ndef test(model, test_loader, criterion):\n    model.eval()\n    running_loss = 0.0\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for images, labels in tqdm(test_loader):\n            images, labels = images.cuda(), labels.cuda()\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            running_loss += loss.item()\n            _, predicted = outputs.max(1)\n            total += labels.size(0)\n            correct += predicted.eq(labels).sum().item()\n    print(f\"Test Loss: {running_loss/len(test_loader)}, Test Accuracy: {100 * correct/total:.2f}%\")\n    return 100 * correct/total\n\n# Main training loop\nmax_iters = 50\nwarmup_iters = 20  # Warmup phase epochs\n# scheduler = warmup_cosine_lr_scheduler(optimizer, warmup_iters, max_iters)\n\nbest_acc = 0\n\nfor epoch in range(max_iters):\n    print(f\"Epoch {epoch+1}/{max_iters}\")\n    train(model, train_loader, criterion, optimizer, scheduler)\n    acc = test(model, train_loader, criterion)\n    if acc > best_acc:\n        best_acc = acc\n        torch.save(model.state_dict(), \"best_new_model.pth\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-01T05:21:24.159989Z","iopub.execute_input":"2024-12-01T05:21:24.160229Z","iopub.status.idle":"2024-12-01T05:21:47.021499Z","shell.execute_reply.started":"2024-12-01T05:21:24.160205Z","shell.execute_reply":"2024-12-01T05:21:47.019946Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/50\n","output_type":"stream"},{"name":"stderr","text":" 78%|███████▊  | 305/391 [00:22<00:06, 13.70it/s]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[8], line 57\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_iters):\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmax_iters\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 57\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m     acc \u001b[38;5;241m=\u001b[39m test(model, train_loader, criterion)\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m acc \u001b[38;5;241m>\u001b[39m best_acc:\n","Cell \u001b[0;32mIn[8], line 17\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, criterion, optimizer, scheduler)\u001b[0m\n\u001b[1;32m     15\u001b[0m correct \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     16\u001b[0m total \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m images, labels \u001b[38;5;129;01min\u001b[39;00m tqdm(train_loader):\n\u001b[1;32m     18\u001b[0m     images, labels \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mcuda(), labels\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[1;32m     19\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1327\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1324\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[1;32m   1326\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1327\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1328\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1329\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[1;32m   1330\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1293\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1289\u001b[0m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[1;32m   1290\u001b[0m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[1;32m   1291\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1292\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m-> 1293\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1294\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m   1295\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1131\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1118\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m_utils\u001b[38;5;241m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[1;32m   1119\u001b[0m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[1;32m   1120\u001b[0m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1128\u001b[0m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[1;32m   1129\u001b[0m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[1;32m   1130\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1131\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1132\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[1;32m   1133\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1134\u001b[0m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[1;32m   1135\u001b[0m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[1;32m   1136\u001b[0m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/multiprocessing/queues.py:113\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m block:\n\u001b[1;32m    112\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m deadline \u001b[38;5;241m-\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic()\n\u001b[0;32m--> 113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    114\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll():\n","File \u001b[0;32m/opt/conda/lib/python3.10/multiprocessing/connection.py:257\u001b[0m, in \u001b[0;36m_ConnectionBase.poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_closed()\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_readable()\n\u001b[0;32m--> 257\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/multiprocessing/connection.py:424\u001b[0m, in \u001b[0;36mConnection._poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    423\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_poll\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout):\n\u001b[0;32m--> 424\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    425\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(r)\n","File \u001b[0;32m/opt/conda/lib/python3.10/multiprocessing/connection.py:931\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    928\u001b[0m     deadline \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic() \u001b[38;5;241m+\u001b[39m timeout\n\u001b[1;32m    930\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 931\u001b[0m     ready \u001b[38;5;241m=\u001b[39m \u001b[43mselector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    932\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ready:\n\u001b[1;32m    933\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [key\u001b[38;5;241m.\u001b[39mfileobj \u001b[38;5;28;01mfor\u001b[39;00m (key, events) \u001b[38;5;129;01min\u001b[39;00m ready]\n","File \u001b[0;32m/opt/conda/lib/python3.10/selectors.py:416\u001b[0m, in \u001b[0;36m_PollLikeSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    414\u001b[0m ready \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 416\u001b[0m     fd_event_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_selector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":8},{"cell_type":"markdown","source":"### UPDATE POE WITH PREIVOUS code","metadata":{}},{"cell_type":"code","source":"import os\nimport random\nimport numpy as np\nimport math\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision import datasets, transforms\nfrom torch.optim.lr_scheduler import LambdaLR\nfrom tqdm import tqdm\n\n# Custom CutOut augmentation\nclass CutOut(object):\n    def __init__(self, n_holes, length):\n        self.n_holes = n_holes\n        self.length = length\n\n    def __call__(self, img):\n        h, w = img.size(1), img.size(2)\n        mask = np.ones((h, w), np.float32)\n\n        for n in range(self.n_holes):\n            y = random.randint(0, h - 1)\n            x = random.randint(0, w - 1)\n\n            y1 = np.clip(y - self.length // 2, 0, h)\n            y2 = np.clip(y + self.length // 2, 0, h)\n            x1 = np.clip(x - self.length // 2, 0, w)\n            x2 = np.clip(x + self.length // 2, 0, w)\n\n            mask[y1:y2, x1:x2] = 0.\n\n        mask = torch.from_numpy(mask).expand_as(img)\n        img = img * mask\n        return img\n\n# Data transformations\ntrain_transform = transforms.Compose([\n    transforms.RandomHorizontalFlip(p=0.5),\n    transforms.RandomAffine(degrees=15, translate=(0.1, 0.1), scale=(0.9, 1.1)),  # Rotate, scale, shift\n    transforms.RandomCrop(32, padding=4),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=(0.4914, 0.4822, 0.4465), std=(0.2023, 0.1994, 0.2010)),\n    CutOut(n_holes=1, length=8),\n])\n\ntest_transform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize(mean=(0.4914, 0.4822, 0.4465), std=(0.2023, 0.1994, 0.2010)),\n])\n\n# Load CIFAR-10 dataset using torchvision\ntrain_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=train_transform)\ntest_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=test_transform)\n\ntrain_loader = DataLoader(dataset=train_dataset, batch_size=128, shuffle=True, num_workers=4)\ntest_loader = DataLoader(dataset=test_dataset, batch_size=128, shuffle=False, num_workers=4)\n\n# Define the Residual Block and ResNet classes (as defined in your original code)\nclass ResidualBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n        super(ResidualBlock, self).__init__()\n        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(out_channels)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(out_channels)\n        self.downsample = downsample\n\n    def forward(self, x):\n        residual = x\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n        out = self.conv2(out)\n        out = self.bn2(out)\n        if self.downsample:\n            residual = self.downsample(x)\n        out += residual\n        out = self.relu(out)\n        return out\n\nclass ResNet(nn.Module):\n    def __init__(self, block, layers, num_classes=10, width_mult=1.0):\n        super(ResNet, self).__init__()\n        self.in_channels = int(64 * width_mult)\n        self.conv1 = nn.Conv2d(3, self.in_channels, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(self.in_channels)\n        self.relu = nn.ReLU(inplace=True)\n        self.layer1 = self.make_layer(block, int(64 * width_mult), layers[0], stride=1)\n        self.layer2 = self.make_layer(block, int(128 * width_mult), layers[1], stride=2)\n        self.layer3 = self.make_layer(block, int(256 * width_mult), layers[2], stride=2)\n        self.layer4 = self.make_layer(block, int(512 * width_mult), layers[3], stride=2)\n        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n        self.fc = nn.Linear(int(512 * width_mult), num_classes)\n\n    def make_layer(self, block, out_channels, blocks, stride=1):\n        downsample = None\n        if stride != 1 or self.in_channels != out_channels:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(out_channels)\n            )\n        layers = []\n        layers.append(block(self.in_channels, out_channels, stride, downsample))\n        self.in_channels = out_channels\n        for _ in range(1, blocks):\n            layers.append(block(out_channels, out_channels))\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        x = self.avg_pool(x)\n        x = torch.flatten(x, 1)\n        x = self.fc(x)\n        return x\n\n# Initialize model, loss function, optimizer, and learning rate scheduler\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = ResNet(ResidualBlock, [2, 2, 2, 2], width_mult=0.5).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=3e-4)\nscheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=50)\n\n# Training and Testing Functions\ndef train(model, train_loader, criterion, optimizer, scheduler):\n    model.train()\n    running_loss = 0.0\n    correct = 0\n    total = 0\n    for images, labels in tqdm(train_loader):\n        images, labels = images.to(device), labels.to(device)\n        optimizer.zero_grad()\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item()\n        _, predicted = outputs.max(1)\n        total += labels.size(0)\n        correct += predicted.eq(labels).sum().item()\n    scheduler.step()\n    print(f\"Train Loss: {running_loss/len(train_loader):.4f}, Train Accuracy: {100 * correct/total:.2f}%\")\n\ndef test(model, test_loader, criterion):\n    model.eval()\n    running_loss = 0.0\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for images, labels in tqdm(test_loader):\n            images, labels = images.to(device), labels.to(device)\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            running_loss += loss.item()\n            _, predicted = outputs.max(1)\n            total += labels.size(0)\n            correct += predicted.eq(labels).sum().item()\n    print(f\"Test Loss: {running_loss/len(test_loader):.4f}, Test Accuracy: {100 * correct/total:.2f}%\")\n    return 100 * correct / total\n\n# Main training loop\nbest_acc = 0\nfor epoch in range(60):  # 50 epochs\n    print(f\"Epoch {epoch+1}/50\")\n    train(model, train_loader, criterion, optimizer, scheduler)\n    acc = test(model, test_loader, criterion)\n    if acc > best_acc:\n        best_acc = acc\n        torch.save(model.state_dict(), \"best_new_model60.pth\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-01T10:22:06.543666Z","iopub.execute_input":"2024-12-01T10:22:06.544034Z","iopub.status.idle":"2024-12-01T10:44:45.149252Z","shell.execute_reply.started":"2024-12-01T10:22:06.544003Z","shell.execute_reply":"2024-12-01T10:44:45.148170Z"}},"outputs":[{"name":"stdout","text":"Files already downloaded and verified\nFiles already downloaded and verified\nEpoch 1/50\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:19<00:00, 19.72it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.7778, Train Accuracy: 33.98%\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 79/79 [00:01<00:00, 53.15it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 1.6130, Test Accuracy: 40.39%\nEpoch 2/50\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:20<00:00, 19.36it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.4185, Train Accuracy: 47.84%\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 79/79 [00:01<00:00, 54.26it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 1.2092, Test Accuracy: 57.49%\nEpoch 3/50\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:20<00:00, 19.30it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.1706, Train Accuracy: 58.14%\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 79/79 [00:01<00:00, 53.76it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 1.0593, Test Accuracy: 61.23%\nEpoch 4/50\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:20<00:00, 18.94it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.0230, Train Accuracy: 63.73%\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 79/79 [00:01<00:00, 54.28it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 0.9470, Test Accuracy: 66.10%\nEpoch 5/50\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:21<00:00, 18.61it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.9133, Train Accuracy: 67.52%\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 79/79 [00:01<00:00, 53.28it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 0.8820, Test Accuracy: 68.76%\nEpoch 6/50\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:21<00:00, 18.27it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.8280, Train Accuracy: 70.91%\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 79/79 [00:01<00:00, 46.26it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 0.8081, Test Accuracy: 72.58%\nEpoch 7/50\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:21<00:00, 18.55it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.7682, Train Accuracy: 72.95%\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 79/79 [00:01<00:00, 53.37it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 0.7462, Test Accuracy: 74.50%\nEpoch 8/50\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:21<00:00, 18.56it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.7256, Train Accuracy: 74.61%\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 79/79 [00:01<00:00, 54.48it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 0.6368, Test Accuracy: 78.26%\nEpoch 9/50\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:21<00:00, 18.36it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.6942, Train Accuracy: 75.69%\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 79/79 [00:01<00:00, 51.59it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 0.6191, Test Accuracy: 78.58%\nEpoch 10/50\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:21<00:00, 18.45it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.6606, Train Accuracy: 77.09%\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 79/79 [00:01<00:00, 52.98it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 0.6570, Test Accuracy: 78.34%\nEpoch 11/50\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:21<00:00, 18.56it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.6308, Train Accuracy: 77.95%\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 79/79 [00:01<00:00, 54.93it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 0.5799, Test Accuracy: 80.54%\nEpoch 12/50\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:21<00:00, 18.49it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.6151, Train Accuracy: 78.56%\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 79/79 [00:01<00:00, 53.10it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 0.6471, Test Accuracy: 78.50%\nEpoch 13/50\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:21<00:00, 18.58it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.5972, Train Accuracy: 79.32%\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 79/79 [00:01<00:00, 53.58it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 0.5679, Test Accuracy: 81.17%\nEpoch 14/50\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:21<00:00, 18.40it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.5757, Train Accuracy: 79.91%\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 79/79 [00:01<00:00, 54.14it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 0.5475, Test Accuracy: 80.64%\nEpoch 15/50\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:21<00:00, 18.44it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.5617, Train Accuracy: 80.50%\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 79/79 [00:01<00:00, 53.23it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 0.5353, Test Accuracy: 81.84%\nEpoch 16/50\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:21<00:00, 18.38it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.5461, Train Accuracy: 80.97%\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 79/79 [00:01<00:00, 53.00it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 0.5263, Test Accuracy: 82.32%\nEpoch 17/50\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:21<00:00, 18.46it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.5292, Train Accuracy: 81.50%\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 79/79 [00:01<00:00, 53.21it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 0.4664, Test Accuracy: 84.12%\nEpoch 18/50\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:21<00:00, 18.46it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.5165, Train Accuracy: 82.22%\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 79/79 [00:01<00:00, 54.17it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 0.5014, Test Accuracy: 83.59%\nEpoch 19/50\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:21<00:00, 18.48it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.5014, Train Accuracy: 82.61%\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 79/79 [00:01<00:00, 54.22it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 0.5328, Test Accuracy: 82.66%\nEpoch 20/50\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:21<00:00, 18.58it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.4936, Train Accuracy: 82.91%\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 79/79 [00:01<00:00, 52.21it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 0.4956, Test Accuracy: 83.22%\nEpoch 21/50\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:21<00:00, 18.48it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.4783, Train Accuracy: 83.54%\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 79/79 [00:01<00:00, 52.93it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 0.4605, Test Accuracy: 84.53%\nEpoch 22/50\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:21<00:00, 18.49it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.4688, Train Accuracy: 83.84%\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 79/79 [00:01<00:00, 53.99it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 0.4544, Test Accuracy: 84.65%\nEpoch 23/50\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:21<00:00, 18.54it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.4532, Train Accuracy: 84.26%\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 79/79 [00:01<00:00, 45.10it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 0.5264, Test Accuracy: 82.95%\nEpoch 24/50\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:21<00:00, 18.39it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.4392, Train Accuracy: 84.69%\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 79/79 [00:01<00:00, 53.73it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 0.4119, Test Accuracy: 86.15%\nEpoch 25/50\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:21<00:00, 18.45it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.4261, Train Accuracy: 84.94%\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 79/79 [00:01<00:00, 54.00it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 0.4049, Test Accuracy: 86.33%\nEpoch 26/50\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:21<00:00, 18.47it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.4159, Train Accuracy: 85.44%\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 79/79 [00:01<00:00, 53.62it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 0.4556, Test Accuracy: 84.62%\nEpoch 27/50\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:21<00:00, 18.45it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.4030, Train Accuracy: 86.02%\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 79/79 [00:01<00:00, 53.14it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 0.4187, Test Accuracy: 86.14%\nEpoch 28/50\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:21<00:00, 18.40it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.3920, Train Accuracy: 86.33%\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 79/79 [00:01<00:00, 54.52it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 0.4096, Test Accuracy: 86.30%\nEpoch 29/50\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:21<00:00, 18.43it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.3754, Train Accuracy: 86.89%\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 79/79 [00:01<00:00, 51.56it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 0.4467, Test Accuracy: 85.17%\nEpoch 30/50\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:21<00:00, 18.53it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.3643, Train Accuracy: 87.33%\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 79/79 [00:01<00:00, 54.66it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 0.3379, Test Accuracy: 88.60%\nEpoch 31/50\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:21<00:00, 18.47it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.3491, Train Accuracy: 87.81%\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 79/79 [00:01<00:00, 53.14it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 0.3382, Test Accuracy: 88.77%\nEpoch 32/50\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:21<00:00, 18.47it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.3347, Train Accuracy: 88.30%\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 79/79 [00:01<00:00, 54.93it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 0.3308, Test Accuracy: 89.08%\nEpoch 33/50\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:21<00:00, 18.46it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.3237, Train Accuracy: 88.68%\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 79/79 [00:01<00:00, 50.60it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 0.3181, Test Accuracy: 89.41%\nEpoch 34/50\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:21<00:00, 18.50it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.3125, Train Accuracy: 89.14%\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 79/79 [00:01<00:00, 54.28it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 0.3286, Test Accuracy: 89.19%\nEpoch 35/50\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:21<00:00, 18.43it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.2965, Train Accuracy: 89.59%\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 79/79 [00:01<00:00, 51.49it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 0.3320, Test Accuracy: 88.89%\nEpoch 36/50\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:21<00:00, 18.51it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.2827, Train Accuracy: 89.98%\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 79/79 [00:01<00:00, 53.43it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 0.2843, Test Accuracy: 90.42%\nEpoch 37/50\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:21<00:00, 18.54it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.2641, Train Accuracy: 90.79%\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 79/79 [00:01<00:00, 53.75it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 0.2970, Test Accuracy: 90.14%\nEpoch 38/50\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:21<00:00, 18.50it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.2486, Train Accuracy: 91.23%\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 79/79 [00:01<00:00, 54.49it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 0.2862, Test Accuracy: 90.81%\nEpoch 39/50\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:21<00:00, 18.58it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.2379, Train Accuracy: 91.56%\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 79/79 [00:01<00:00, 52.79it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 0.2718, Test Accuracy: 90.97%\nEpoch 40/50\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:21<00:00, 18.57it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.2221, Train Accuracy: 92.24%\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 79/79 [00:01<00:00, 52.19it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 0.2629, Test Accuracy: 91.49%\nEpoch 41/50\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:21<00:00, 18.38it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.2108, Train Accuracy: 92.55%\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 79/79 [00:01<00:00, 53.48it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 0.2530, Test Accuracy: 91.76%\nEpoch 42/50\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:21<00:00, 18.38it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.1959, Train Accuracy: 93.06%\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 79/79 [00:01<00:00, 53.80it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 0.2485, Test Accuracy: 91.98%\nEpoch 43/50\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:21<00:00, 18.43it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.1845, Train Accuracy: 93.46%\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 79/79 [00:01<00:00, 53.95it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 0.2423, Test Accuracy: 92.30%\nEpoch 44/50\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:21<00:00, 18.58it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.1708, Train Accuracy: 94.04%\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 79/79 [00:01<00:00, 53.51it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 0.2413, Test Accuracy: 92.28%\nEpoch 45/50\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:21<00:00, 18.54it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.1599, Train Accuracy: 94.42%\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 79/79 [00:01<00:00, 54.45it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 0.2341, Test Accuracy: 92.67%\nEpoch 46/50\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:21<00:00, 18.53it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.1553, Train Accuracy: 94.51%\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 79/79 [00:01<00:00, 54.54it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 0.2299, Test Accuracy: 92.84%\nEpoch 47/50\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:21<00:00, 18.50it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.1480, Train Accuracy: 94.74%\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 79/79 [00:01<00:00, 53.91it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 0.2265, Test Accuracy: 92.81%\nEpoch 48/50\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:21<00:00, 18.45it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.1401, Train Accuracy: 95.08%\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 79/79 [00:01<00:00, 54.40it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 0.2268, Test Accuracy: 92.71%\nEpoch 49/50\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:21<00:00, 18.44it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.1403, Train Accuracy: 95.21%\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 79/79 [00:01<00:00, 54.08it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 0.2246, Test Accuracy: 92.80%\nEpoch 50/50\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:21<00:00, 18.48it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.1408, Train Accuracy: 95.08%\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 79/79 [00:01<00:00, 42.89it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 0.2249, Test Accuracy: 92.82%\nEpoch 51/50\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:21<00:00, 18.45it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.1328, Train Accuracy: 95.35%\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 79/79 [00:01<00:00, 53.82it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 0.2262, Test Accuracy: 92.77%\nEpoch 52/50\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:21<00:00, 18.42it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.1374, Train Accuracy: 95.15%\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 79/79 [00:01<00:00, 53.01it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 0.2235, Test Accuracy: 92.69%\nEpoch 53/50\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:21<00:00, 18.44it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.1388, Train Accuracy: 95.15%\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 79/79 [00:01<00:00, 53.44it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 0.2239, Test Accuracy: 92.81%\nEpoch 54/50\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:21<00:00, 18.49it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.1391, Train Accuracy: 95.22%\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 79/79 [00:01<00:00, 54.24it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 0.2257, Test Accuracy: 92.76%\nEpoch 55/50\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:21<00:00, 18.47it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.1396, Train Accuracy: 95.13%\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 79/79 [00:01<00:00, 54.78it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 0.2351, Test Accuracy: 92.68%\nEpoch 56/50\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:21<00:00, 18.46it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.1452, Train Accuracy: 94.91%\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 79/79 [00:01<00:00, 53.74it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 0.2308, Test Accuracy: 92.79%\nEpoch 57/50\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:21<00:00, 18.50it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.1486, Train Accuracy: 94.74%\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 79/79 [00:01<00:00, 54.21it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 0.2352, Test Accuracy: 92.43%\nEpoch 58/50\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:21<00:00, 18.47it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.1519, Train Accuracy: 94.74%\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 79/79 [00:01<00:00, 53.29it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 0.2450, Test Accuracy: 92.46%\nEpoch 59/50\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:21<00:00, 18.56it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.1615, Train Accuracy: 94.27%\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 79/79 [00:01<00:00, 53.79it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 0.2431, Test Accuracy: 92.50%\nEpoch 60/50\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:21<00:00, 18.47it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.1724, Train Accuracy: 93.80%\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 79/79 [00:01<00:00, 49.48it/s]","output_type":"stream"},{"name":"stdout","text":"Test Loss: 0.2568, Test Accuracy: 92.04%\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"total_params = sum(p.numel() for p in model.parameters())\nprint(f\"Total number of parameters: {total_params}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-01T10:48:24.566823Z","iopub.execute_input":"2024-12-01T10:48:24.567194Z","iopub.status.idle":"2024-12-01T10:48:24.573158Z","shell.execute_reply.started":"2024-12-01T10:48:24.567167Z","shell.execute_reply":"2024-12-01T10:48:24.572285Z"}},"outputs":[{"name":"stdout","text":"Total number of parameters: 2797610\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"## 99","metadata":{}},{"cell_type":"code","source":"## poe\nimport os\nimport random\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras import layers, models\nfrom tensorflow.keras.datasets import cifar10\nfrom tensorflow.keras.utils import to_categorical\nfrom tqdm import tqdm\n\n# Set random seeds for reproducibility\nrandom.seed(123)\nnp.random.seed(123)\ntf.random.set_seed(1234)\nos.environ['PYTHONHASHSEED'] = '0'\n\n# Load CIFAR-10 dataset\n(x_train, y_train), (x_test, y_test) = cifar10.load_data()\nx_train = x_train.astype('float32') / 255.0\nx_test = x_test.astype('float32') / 255.0\ny_train = to_categorical(y_train, 10)\ny_test = to_categorical(y_test, 10)\n\n# Define the new ResNet architecture\ndef build_model():\n    input_layer = layers.Input(shape=(32, 32, 3))\n    x = layers.Normalization()(input_layer)\n\n    filters_l = [32, 64, 128, 128]\n    \n    # Initial convolution\n    x = layers.Conv2D(filters=filters_l[0], kernel_size=(3, 3), kernel_initializer='he_uniform', activation='elu', padding='same')(x)\n\n    for i, filters in enumerate(filters_l):\n        res_con1 = x  \n        x = layers.BatchNormalization()(x)\n        x = layers.ELU()(x)\n        x = layers.Conv2D(filters=filters, kernel_size=(3, 3), kernel_initializer='he_uniform', padding='same')(x)\n        x = layers.BatchNormalization()(x)\n        x = layers.ELU()(x)\n        x = layers.Conv2D(filters=filters, kernel_size=(3, 3), kernel_initializer='he_uniform', padding='same')(x)\n        x = layers.Add()([res_con1, x])\n\n        # Additional residual connections\n        for _ in range(2):\n            res_con2 = x  \n            x = layers.BatchNormalization()(x)\n            x = layers.ELU()(x)\n            x = layers.Conv2D(filters=filters, kernel_size=(3, 3), kernel_initializer='he_uniform', padding='same')(x)\n            x = layers.BatchNormalization()(x)\n            x = layers.ELU()(x)\n            x = layers.Conv2D(filters=filters, kernel_size=(3, 3), kernel_initializer='he_uniform', padding='same')(x)\n            x = layers.Add()([res_con1, res_con2, x])\n\n        res_con = x\n        downsample = i != len(filters_l) - 1\n        \n        if downsample:\n            res_con = layers.BatchNormalization()(res_con)\n            res_con = layers.ELU()(res_con)\n            res_con = layers.Conv2D(filters=filters_l[i + 1], kernel_size=(3, 3), kernel_initializer='he_uniform', strides=2, padding='same')(res_con)\n\n        x = layers.BatchNormalization()(x)\n        x = layers.ELU()(x)\n        x = layers.Conv2D(filters=filters, kernel_size=(3, 3), kernel_initializer='he_uniform', padding='same')(x)\n        x = layers.BatchNormalization()(x)\n        x = layers.ELU()(x)\n        x = layers.Conv2D(filters=(filters_l[i + 1] if downsample else filters), kernel_size=(3, 3), kernel_initializer='he_uniform', strides=(2 if downsample else 1), padding='same')(x)\n        x = layers.Add()([res_con, x])\n    \n    x = layers.GlobalAveragePooling2D()(x)\n    x = layers.Dense(10, activation='softmax')(x)\n    \n    model = models.Model(inputs=input_layer, outputs=x)\n    model.compile(optimizer=tf.keras.optimizers.Adam(0.001),\n                  loss=tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.1),\n                  metrics=['accuracy'])\n    return model\n\n# Initialize model\nmodel = build_model()\n\n# Training parameters\nbatch_size = 128\nepochs = 100\n\n# Training loop\nbest_acc = 0\nfor epoch in range(epochs):\n    print(f\"Epoch {epoch + 1}/{epochs}\")\n    model.fit(x_train, y_train, batch_size=batch_size, epochs=1, verbose=1, validation_data=(x_test, y_test))  # Train for one epoch at a time\n\n    # Evaluate the model\n    test_loss, test_acc = model.evaluate(x_test, y_test, verbose=0)\n    print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc * 100:.2f}%\")\n\n    # Save the best model\n    if test_acc > best_acc:\n        best_acc = test_acc\n        model.save(\"test_model_poe3.h5\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T04:31:13.861179Z","iopub.execute_input":"2024-12-02T04:31:13.861585Z","iopub.status.idle":"2024-12-02T04:57:21.708607Z","shell.execute_reply.started":"2024-12-02T04:31:13.861549Z","shell.execute_reply":"2024-12-02T04:57:21.707214Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/100\n\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 110ms/step - accuracy: 0.3719 - loss: 2.6425 - val_accuracy: 0.5438 - val_loss: 1.5640\nTest Loss: 1.5640, Test Accuracy: 54.38%\nEpoch 2/100\n\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 81ms/step - accuracy: 0.6238 - loss: 1.3693 - val_accuracy: 0.6649 - val_loss: 1.3124\nTest Loss: 1.3124, Test Accuracy: 66.49%\nEpoch 3/100\n\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 81ms/step - accuracy: 0.7219 - loss: 1.1715 - val_accuracy: 0.7058 - val_loss: 1.2193\nTest Loss: 1.2193, Test Accuracy: 70.58%\nEpoch 4/100\n\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 81ms/step - accuracy: 0.7822 - loss: 1.0519 - val_accuracy: 0.7176 - val_loss: 1.1991\nTest Loss: 1.1991, Test Accuracy: 71.76%\nEpoch 5/100\n\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 81ms/step - accuracy: 0.8334 - loss: 0.9581 - val_accuracy: 0.7113 - val_loss: 1.2138\nTest Loss: 1.2138, Test Accuracy: 71.13%\nEpoch 6/100\n\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 81ms/step - accuracy: 0.8718 - loss: 0.8888 - val_accuracy: 0.6935 - val_loss: 1.2821\nTest Loss: 1.2821, Test Accuracy: 69.35%\nEpoch 7/100\n\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 81ms/step - accuracy: 0.8987 - loss: 0.8396 - val_accuracy: 0.7041 - val_loss: 1.2649\nTest Loss: 1.2649, Test Accuracy: 70.41%\nEpoch 8/100\n\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 81ms/step - accuracy: 0.9222 - loss: 0.7920 - val_accuracy: 0.7171 - val_loss: 1.2501\nTest Loss: 1.2501, Test Accuracy: 71.71%\nEpoch 9/100\n\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 81ms/step - accuracy: 0.9444 - loss: 0.7502 - val_accuracy: 0.7272 - val_loss: 1.2170\nTest Loss: 1.2170, Test Accuracy: 72.72%\nEpoch 10/100\n\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 81ms/step - accuracy: 0.9675 - loss: 0.7026 - val_accuracy: 0.7303 - val_loss: 1.2140\nTest Loss: 1.2140, Test Accuracy: 73.03%\nEpoch 11/100\n\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 81ms/step - accuracy: 0.9798 - loss: 0.6708 - val_accuracy: 0.7111 - val_loss: 1.2471\nTest Loss: 1.2471, Test Accuracy: 71.11%\nEpoch 12/100\n\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 81ms/step - accuracy: 0.9888 - loss: 0.6459 - val_accuracy: 0.7075 - val_loss: 1.2539\nTest Loss: 1.2539, Test Accuracy: 70.75%\nEpoch 13/100\n\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 81ms/step - accuracy: 0.9925 - loss: 0.6270 - val_accuracy: 0.7376 - val_loss: 1.1774\nTest Loss: 1.1774, Test Accuracy: 73.76%\nEpoch 14/100\n\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 81ms/step - accuracy: 0.9956 - loss: 0.6096 - val_accuracy: 0.7539 - val_loss: 1.1389\nTest Loss: 1.1389, Test Accuracy: 75.39%\nEpoch 15/100\n\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 81ms/step - accuracy: 0.9977 - loss: 0.5926 - val_accuracy: 0.7562 - val_loss: 1.1235\nTest Loss: 1.1235, Test Accuracy: 75.62%\nEpoch 16/100\n\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 81ms/step - accuracy: 0.9989 - loss: 0.5823 - val_accuracy: 0.7578 - val_loss: 1.1185\nTest Loss: 1.1185, Test Accuracy: 75.78%\nEpoch 17/100\n\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 81ms/step - accuracy: 0.9992 - loss: 0.5759 - val_accuracy: 0.7563 - val_loss: 1.1182\nTest Loss: 1.1182, Test Accuracy: 75.63%\nEpoch 18/100\n\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 81ms/step - accuracy: 0.9985 - loss: 0.5757 - val_accuracy: 0.7462 - val_loss: 1.1467\nTest Loss: 1.1467, Test Accuracy: 74.62%\nEpoch 19/100\n\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 81ms/step - accuracy: 0.9990 - loss: 0.5771 - val_accuracy: 0.7402 - val_loss: 1.1571\nTest Loss: 1.1571, Test Accuracy: 74.02%\nEpoch 20/100\n\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 81ms/step - accuracy: 0.9986 - loss: 0.5781 - val_accuracy: 0.7442 - val_loss: 1.1458\nTest Loss: 1.1458, Test Accuracy: 74.42%\nEpoch 21/100\n\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 81ms/step - accuracy: 0.9988 - loss: 0.5747 - val_accuracy: 0.7530 - val_loss: 1.1191\nTest Loss: 1.1191, Test Accuracy: 75.30%\nEpoch 22/100\n\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 81ms/step - accuracy: 0.9986 - loss: 0.5718 - val_accuracy: 0.7548 - val_loss: 1.1237\nTest Loss: 1.1237, Test Accuracy: 75.48%\nEpoch 23/100\n\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 81ms/step - accuracy: 0.9990 - loss: 0.5670 - val_accuracy: 0.7686 - val_loss: 1.0842\nTest Loss: 1.0842, Test Accuracy: 76.86%\nEpoch 24/100\n\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 81ms/step - accuracy: 0.9996 - loss: 0.5597 - val_accuracy: 0.7798 - val_loss: 1.0593\nTest Loss: 1.0593, Test Accuracy: 77.98%\nEpoch 25/100\n\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 81ms/step - accuracy: 0.9997 - loss: 0.5547 - val_accuracy: 0.7680 - val_loss: 1.0811\nTest Loss: 1.0811, Test Accuracy: 76.80%\nEpoch 26/100\n\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 81ms/step - accuracy: 0.9997 - loss: 0.5501 - val_accuracy: 0.7723 - val_loss: 1.0759\nTest Loss: 1.0759, Test Accuracy: 77.23%\nEpoch 27/100\n\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 81ms/step - accuracy: 0.9998 - loss: 0.5482 - val_accuracy: 0.7693 - val_loss: 1.0794\nTest Loss: 1.0794, Test Accuracy: 76.93%\nEpoch 28/100\n\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 81ms/step - accuracy: 0.9997 - loss: 0.5478 - val_accuracy: 0.7718 - val_loss: 1.0788\nTest Loss: 1.0788, Test Accuracy: 77.18%\nEpoch 29/100\n\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 81ms/step - accuracy: 0.9997 - loss: 0.5458 - val_accuracy: 0.7657 - val_loss: 1.0853\nTest Loss: 1.0853, Test Accuracy: 76.57%\nEpoch 30/100\n\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 80ms/step - accuracy: 0.9998 - loss: 0.5461 - val_accuracy: 0.7528 - val_loss: 1.1059\nTest Loss: 1.1059, Test Accuracy: 75.28%\nEpoch 31/100\n\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 80ms/step - accuracy: 0.9997 - loss: 0.5430 - val_accuracy: 0.7772 - val_loss: 1.0631\nTest Loss: 1.0631, Test Accuracy: 77.72%\nEpoch 32/100\n\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 80ms/step - accuracy: 0.9996 - loss: 0.5402 - val_accuracy: 0.7848 - val_loss: 1.0388\nTest Loss: 1.0388, Test Accuracy: 78.48%\nEpoch 33/100\n\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 81ms/step - accuracy: 0.9999 - loss: 0.5361 - val_accuracy: 0.7773 - val_loss: 1.0524\nTest Loss: 1.0524, Test Accuracy: 77.73%\nEpoch 34/100\n\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 81ms/step - accuracy: 0.9999 - loss: 0.5328 - val_accuracy: 0.7785 - val_loss: 1.0524\nTest Loss: 1.0524, Test Accuracy: 77.85%\nEpoch 35/100\n\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 81ms/step - accuracy: 0.9999 - loss: 0.5308 - val_accuracy: 0.7816 - val_loss: 1.0384\nTest Loss: 1.0384, Test Accuracy: 78.16%\nEpoch 36/100\n\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 81ms/step - accuracy: 0.9999 - loss: 0.5303 - val_accuracy: 0.7829 - val_loss: 1.0424\nTest Loss: 1.0424, Test Accuracy: 78.29%\nEpoch 37/100\n\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 81ms/step - accuracy: 0.9998 - loss: 0.5294 - val_accuracy: 0.7895 - val_loss: 1.0291\nTest Loss: 1.0291, Test Accuracy: 78.95%\nEpoch 38/100\n\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 81ms/step - accuracy: 0.9998 - loss: 0.5279 - val_accuracy: 0.7865 - val_loss: 1.0308\nTest Loss: 1.0308, Test Accuracy: 78.65%\nEpoch 39/100\n\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 81ms/step - accuracy: 1.0000 - loss: 0.5243 - val_accuracy: 0.7939 - val_loss: 1.0130\nTest Loss: 1.0130, Test Accuracy: 79.39%\nEpoch 40/100\n\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 81ms/step - accuracy: 0.9998 - loss: 0.5218 - val_accuracy: 0.7935 - val_loss: 1.0082\nTest Loss: 1.0082, Test Accuracy: 79.35%\nEpoch 41/100\n\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 81ms/step - accuracy: 0.9999 - loss: 0.5202 - val_accuracy: 0.7887 - val_loss: 1.0308\nTest Loss: 1.0308, Test Accuracy: 78.87%\nEpoch 42/100\n\u001b[1m249/391\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 76ms/step - accuracy: 1.0000 - loss: 0.5195","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[5], line 91\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m     90\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 91\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Train for one epoch at a time\u001b[39;00m\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;66;03m# Evaluate the model\u001b[39;00m\n\u001b[1;32m     94\u001b[0m     test_loss, test_acc \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(x_test, y_test, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py:314\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    312\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator\u001b[38;5;241m.\u001b[39menumerate_epoch():\n\u001b[1;32m    313\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m--> 314\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    315\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pythonify_logs(logs)\n\u001b[1;32m    316\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_end(step, logs)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:878\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    876\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[1;32m    877\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[0;32m--> 878\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[1;32m    882\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    883\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m     args,\n\u001b[1;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1326\u001b[0m     executing_eagerly)\n\u001b[1;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[1;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[1;32m    261\u001b[0m     )\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/context.py:1500\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1498\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1499\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1500\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1501\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1503\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1504\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1505\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1506\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1507\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1508\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1509\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1510\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1514\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1515\u001b[0m   )\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":5},{"cell_type":"code","source":"import os\nimport random\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom torchvision import datasets, transforms\nfrom tqdm import tqdm\n\n# Set random seeds for reproducibility\nrandom.seed(123)\nnp.random.seed(123)\ntorch.manual_seed(1234)\nos.environ['PYTHONHASHSEED'] = '0'\n\n# Check if CUDA (GPU) is available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Load CIFAR-10 dataset\ntransform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n])\n\ntrain_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\ntest_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n\ntrain_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n\n# Define the new ResNet architecture in PyTorch\nclass ResNetModel(nn.Module):\n    def __init__(self):\n        super(ResNetModel, self).__init__()\n        \n        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n        self.relu = nn.ELU()\n        \n        self.layer1 = self._make_layer(32, 64)\n        self.layer2 = self._make_layer(64, 128)\n        self.layer3 = self._make_layer(128, 128)\n        \n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.fc = nn.Linear(128, 10)\n        \n    def _make_layer(self, in_channels, out_channels):\n        layers = []\n        \n        # Initial convolution block\n        layers.append(nn.BatchNorm2d(in_channels))\n        layers.append(self.relu)\n        layers.append(nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1))\n        \n        layers.append(nn.BatchNorm2d(out_channels))\n        layers.append(self.relu)\n        layers.append(nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1))\n        \n        # Residual connection\n        layers.append(nn.Identity())\n        \n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.relu(self.conv1(x))\n        \n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        \n        x = self.avg_pool(x)\n        x = torch.flatten(x, 1)\n        \n        x = self.fc(x)\n        return x\n\n# Initialize model and move it to the appropriate device (GPU or CPU)\nmodel = ResNetModel().to(device)\n\n# Define loss and optimizer\ncriterion = nn.CrossEntropyLoss(label_smoothing=0.1)\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\n# Training parameters\nepochs = 50\nbest_acc = 0\n\n# Training loop\nfor epoch in range(epochs):\n    print(f\"Epoch {epoch + 1}/{epochs}\")\n    \n    # Training phase\n    model.train()\n    for data, target in tqdm(train_loader):\n        data, target = data.to(device), target.to(device)  # Move data and target to device\n        optimizer.zero_grad()\n        output = model(data)\n        loss = criterion(output, target)\n        loss.backward()\n        optimizer.step()\n    \n    # Validation phase\n    model.eval()\n    correct = 0\n    total = 0\n    test_loss = 0\n    with torch.no_grad():\n        for data, target in test_loader:\n            data, target = data.to(device), target.to(device)  # Move data and target to device\n            output = model(data)\n            loss = criterion(output, target)\n            test_loss += loss.item()\n            \n            _, predicted = torch.max(output, 1)\n            total += target.size(0)\n            correct += (predicted == target).sum().item()\n    \n    test_acc = correct / total\n    test_loss /= len(test_loader)\n    \n    print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc * 100:.2f}%\")\n    \n    # Save the best model\n    if test_acc > best_acc:\n        best_acc = test_acc\n        torch.save(model.state_dict(), \"test_model_poe1gpt.pth\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T03:05:36.868424Z","iopub.execute_input":"2024-12-02T03:05:36.869576Z","iopub.status.idle":"2024-12-02T03:37:59.779890Z","shell.execute_reply.started":"2024-12-02T03:05:36.869535Z","shell.execute_reply":"2024-12-02T03:37:59.778880Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\nDownloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 170498071/170498071 [00:05<00:00, 29319361.25it/s]\n","output_type":"stream"},{"name":"stdout","text":"Extracting ./data/cifar-10-python.tar.gz to ./data\nFiles already downloaded and verified\nEpoch 1/50\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:32<00:00, 12.07it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 1.6655, Test Accuracy: 46.38%\nEpoch 2/50\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:32<00:00, 12.18it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 1.5202, Test Accuracy: 53.81%\nEpoch 3/50\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:33<00:00, 11.52it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 1.4524, Test Accuracy: 56.92%\nEpoch 4/50\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:34<00:00, 11.37it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 1.3862, Test Accuracy: 61.12%\nEpoch 5/50\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:33<00:00, 11.56it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 1.3246, Test Accuracy: 64.14%\nEpoch 6/50\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:34<00:00, 11.40it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 1.3015, Test Accuracy: 65.41%\nEpoch 7/50\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:34<00:00, 11.45it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 1.2628, Test Accuracy: 67.38%\nEpoch 8/50\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:34<00:00, 11.44it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 1.2246, Test Accuracy: 69.33%\nEpoch 9/50\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:34<00:00, 11.42it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 1.2234, Test Accuracy: 69.77%\nEpoch 10/50\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:34<00:00, 11.40it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 1.1877, Test Accuracy: 71.22%\nEpoch 11/50\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:34<00:00, 11.41it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 1.1624, Test Accuracy: 72.22%\nEpoch 12/50\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:34<00:00, 11.41it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 1.1401, Test Accuracy: 73.04%\nEpoch 13/50\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:34<00:00, 11.40it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 1.1284, Test Accuracy: 74.30%\nEpoch 14/50\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:34<00:00, 11.39it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 1.1166, Test Accuracy: 75.01%\nEpoch 15/50\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:34<00:00, 11.39it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 1.1053, Test Accuracy: 75.48%\nEpoch 16/50\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:34<00:00, 11.41it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 1.0877, Test Accuracy: 76.65%\nEpoch 17/50\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:34<00:00, 11.43it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 1.0877, Test Accuracy: 76.88%\nEpoch 18/50\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:34<00:00, 11.42it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 1.0857, Test Accuracy: 77.06%\nEpoch 19/50\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:34<00:00, 11.43it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 1.0757, Test Accuracy: 77.58%\nEpoch 20/50\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:34<00:00, 11.42it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 1.0932, Test Accuracy: 76.56%\nEpoch 21/50\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:34<00:00, 11.42it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 1.1028, Test Accuracy: 75.84%\nEpoch 22/50\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:34<00:00, 11.42it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 1.0837, Test Accuracy: 76.57%\nEpoch 23/50\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:34<00:00, 11.41it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 1.0466, Test Accuracy: 79.07%\nEpoch 24/50\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:34<00:00, 11.38it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 1.0483, Test Accuracy: 78.73%\nEpoch 25/50\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:34<00:00, 11.40it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 1.0794, Test Accuracy: 76.72%\nEpoch 26/50\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:34<00:00, 11.42it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 1.0983, Test Accuracy: 77.35%\nEpoch 27/50\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:34<00:00, 11.42it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 1.0413, Test Accuracy: 79.50%\nEpoch 28/50\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:34<00:00, 11.42it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 1.0023, Test Accuracy: 80.96%\nEpoch 29/50\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:34<00:00, 11.43it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 1.0148, Test Accuracy: 79.92%\nEpoch 30/50\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:34<00:00, 11.42it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 1.2519, Test Accuracy: 69.94%\nEpoch 31/50\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:34<00:00, 11.41it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 1.0220, Test Accuracy: 80.70%\nEpoch 32/50\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:34<00:00, 11.41it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 1.0251, Test Accuracy: 80.42%\nEpoch 33/50\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:34<00:00, 11.42it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 0.9887, Test Accuracy: 81.81%\nEpoch 34/50\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:34<00:00, 11.42it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 1.0675, Test Accuracy: 78.52%\nEpoch 35/50\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:34<00:00, 11.42it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 1.0348, Test Accuracy: 80.19%\nEpoch 36/50\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:34<00:00, 11.39it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 0.9829, Test Accuracy: 82.06%\nEpoch 37/50\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:34<00:00, 11.37it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 1.0095, Test Accuracy: 80.86%\nEpoch 38/50\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:34<00:00, 11.38it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 0.9932, Test Accuracy: 81.53%\nEpoch 39/50\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:34<00:00, 11.39it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 1.1164, Test Accuracy: 76.98%\nEpoch 40/50\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:34<00:00, 11.41it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 1.0231, Test Accuracy: 80.27%\nEpoch 41/50\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:34<00:00, 11.42it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 1.1045, Test Accuracy: 76.20%\nEpoch 42/50\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:34<00:00, 11.42it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 1.1401, Test Accuracy: 76.86%\nEpoch 43/50\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:34<00:00, 11.43it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 1.1232, Test Accuracy: 76.29%\nEpoch 44/50\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:34<00:00, 11.42it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 1.1471, Test Accuracy: 75.25%\nEpoch 45/50\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:34<00:00, 11.43it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 1.2763, Test Accuracy: 70.26%\nEpoch 46/50\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:34<00:00, 11.40it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 1.0676, Test Accuracy: 78.93%\nEpoch 47/50\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:34<00:00, 11.41it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 1.1841, Test Accuracy: 75.10%\nEpoch 48/50\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:34<00:00, 11.42it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 1.1968, Test Accuracy: 73.10%\nEpoch 49/50\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:34<00:00, 11.42it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 1.0837, Test Accuracy: 78.66%\nEpoch 50/50\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:34<00:00, 11.42it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 1.1683, Test Accuracy: 75.73%\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## CHAT MODEL CODES","metadata":{}},{"cell_type":"code","source":"!pip install torchprofile","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T04:57:41.023117Z","iopub.execute_input":"2024-12-02T04:57:41.024104Z","iopub.status.idle":"2024-12-02T04:57:51.669484Z","shell.execute_reply.started":"2024-12-02T04:57:41.024068Z","shell.execute_reply":"2024-12-02T04:57:51.668180Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"Collecting torchprofile\n  Downloading torchprofile-0.0.4-py3-none-any.whl.metadata (303 bytes)\nRequirement already satisfied: numpy>=1.14 in /opt/conda/lib/python3.10/site-packages (from torchprofile) (1.26.4)\nRequirement already satisfied: torch>=1.4 in /opt/conda/lib/python3.10/site-packages (from torchprofile) (2.4.0)\nRequirement already satisfied: torchvision>=0.4 in /opt/conda/lib/python3.10/site-packages (from torchprofile) (0.19.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.4->torchprofile) (3.15.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch>=1.4->torchprofile) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.4->torchprofile) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.4->torchprofile) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.4->torchprofile) (3.1.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.4->torchprofile) (2024.6.1)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision>=0.4->torchprofile) (10.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.4->torchprofile) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.4->torchprofile) (1.3.0)\nDownloading torchprofile-0.0.4-py3-none-any.whl (7.7 kB)\nInstalling collected packages: torchprofile\nSuccessfully installed torchprofile-0.0.4\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"## Gemini\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision import datasets, transforms\nfrom tqdm import tqdm\nimport torch.nn.functional as F\nfrom torchprofile import profile_macs\nimport os\n\n\n# Improved Lightweight CNN Architecture with Depthwise Separable Convolutions\nclass EfficientCIFAR10Net(nn.Module):\n    def __init__(self):\n        super(EfficientCIFAR10Net, self).__init__()\n        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n        self.bn1 = nn.BatchNorm2d(16)\n        self.relu = nn.ReLU(inplace=True)\n        self.dw_conv1 = nn.Conv2d(16, 16, kernel_size=3, padding=1, groups=16) # Depthwise separable\n        self.bn2 = nn.BatchNorm2d(16)\n        self.pool1 = nn.MaxPool2d(2)\n\n\n        self.dw_conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1, groups=16) # Depthwise separable\n        self.bn3 = nn.BatchNorm2d(32)\n        self.pool2 = nn.MaxPool2d(2)\n\n        self.dw_conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1, groups=32) # Depthwise separable\n        self.bn4 = nn.BatchNorm2d(64)\n        self.pool3 = nn.MaxPool2d(2)\n\n        self.fc1 = nn.Linear(64 * 4 * 4, 128)\n        self.fc2 = nn.Linear(128, 10)\n\n    def forward(self, x):\n        x = self.relu(self.bn1(self.conv1(x)))\n        x = self.relu(self.bn2(self.dw_conv1(x)))\n        x = self.pool1(x)\n        x = self.relu(self.bn3(self.dw_conv2(x)))\n        x = self.pool2(x)\n        x = self.relu(self.bn4(self.dw_conv3(x)))\n        x = self.pool3(x)\n        x = x.view(-1, 64 * 4 * 4)\n        x = self.relu(self.fc1(x))\n        x = self.fc2(x)\n        return x\n\n\n# Data Transformations (same as before)\ntransform_train = transforms.Compose([\n    transforms.RandomCrop(32, padding=4),\n    transforms.RandomHorizontalFlip(),\n    transforms.ToTensor(),\n    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n])\n\ntransform_test = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n])\n\n\n# Data Loaders (same as before)\ntrain_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\ntest_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\ntrain_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=2)\ntest_loader = DataLoader(test_dataset, batch_size=128, shuffle=False, num_workers=2)\n\n\n\n# Model, Optimizer, and Loss Function\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = EfficientCIFAR10Net().to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=3e-4)\nscheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=50)\n\n# Training Loop (same as before)\nnum_epochs = 54\nfor epoch in range(num_epochs):\n    model.train()\n    running_loss = 0.0\n    for i, data in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")):\n        inputs, labels = data[0].to(device), data[1].to(device)\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item()\n    scheduler.step()\n    print(f\"Epoch {epoch+1}, Training Loss: {running_loss / len(train_loader)}\")\n\n\n# Testing (same as before)\nmodel.eval()\ncorrect = 0\ntotal = 0\nwith torch.no_grad():\n    for data in tqdm(test_loader, desc=\"Testing\"):\n        images, labels = data[0].to(device), data[1].to(device)\n        outputs = model(images)\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\naccuracy = 100 * correct / total\nprint(f\"Test Accuracy: {accuracy:.2f}%\")\n\n\n# Parameter and FLOPs calculation\nnum_params = sum(p.numel() for p in model.parameters())\nmacs = profile_macs(model, (torch.randn(1,3,32,32,).to(device),))\nprint(f\"Number of Parameters: {num_params}\")\nprint(f\"Millions of MACs (FLOPs): {macs/1e6:.2f}M\")\n\ntorch.save(model.state_dict(), 'efficient_cifar10_model_final22.pth')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-01T07:08:30.723427Z","iopub.execute_input":"2024-12-01T07:08:30.723754Z","iopub.status.idle":"2024-12-01T07:18:36.798958Z","shell.execute_reply.started":"2024-12-01T07:08:30.723727Z","shell.execute_reply":"2024-12-01T07:18:36.797954Z"}},"outputs":[{"name":"stdout","text":"Files already downloaded and verified\nFiles already downloaded and verified\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/54: 100%|██████████| 391/391 [00:11<00:00, 35.02it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1, Training Loss: 1.5364239596954696\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/54: 100%|██████████| 391/391 [00:11<00:00, 32.89it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2, Training Loss: 1.2560509928047199\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/54: 100%|██████████| 391/391 [00:11<00:00, 33.95it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3, Training Loss: 1.1826129923086337\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/54: 100%|██████████| 391/391 [00:11<00:00, 34.61it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4, Training Loss: 1.147909636235298\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/54: 100%|██████████| 391/391 [00:11<00:00, 34.41it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5, Training Loss: 1.1073205099081445\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6/54: 100%|██████████| 391/391 [00:11<00:00, 35.03it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6, Training Loss: 1.091283948982463\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7/54: 100%|██████████| 391/391 [00:11<00:00, 34.28it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7, Training Loss: 1.0888570922110088\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8/54: 100%|██████████| 391/391 [00:11<00:00, 34.86it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8, Training Loss: 1.0523556711728617\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9/54: 100%|██████████| 391/391 [00:11<00:00, 35.33it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 9, Training Loss: 1.0339647909564436\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10/54: 100%|██████████| 391/391 [00:11<00:00, 33.57it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10, Training Loss: 1.0061696351641585\n","output_type":"stream"},{"name":"stderr","text":"Epoch 11/54: 100%|██████████| 391/391 [00:11<00:00, 35.43it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 11, Training Loss: 1.0040201849644752\n","output_type":"stream"},{"name":"stderr","text":"Epoch 12/54: 100%|██████████| 391/391 [00:11<00:00, 35.02it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 12, Training Loss: 0.9842502801009761\n","output_type":"stream"},{"name":"stderr","text":"Epoch 13/54: 100%|██████████| 391/391 [00:11<00:00, 33.67it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 13, Training Loss: 0.9904604959670845\n","output_type":"stream"},{"name":"stderr","text":"Epoch 14/54: 100%|██████████| 391/391 [00:10<00:00, 35.83it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 14, Training Loss: 0.966610344779461\n","output_type":"stream"},{"name":"stderr","text":"Epoch 15/54: 100%|██████████| 391/391 [00:10<00:00, 36.18it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 15, Training Loss: 0.9556394321534335\n","output_type":"stream"},{"name":"stderr","text":"Epoch 16/54: 100%|██████████| 391/391 [00:11<00:00, 35.04it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 16, Training Loss: 0.9431104478628739\n","output_type":"stream"},{"name":"stderr","text":"Epoch 17/54: 100%|██████████| 391/391 [00:10<00:00, 36.27it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 17, Training Loss: 0.9378941734428601\n","output_type":"stream"},{"name":"stderr","text":"Epoch 18/54: 100%|██████████| 391/391 [00:10<00:00, 36.01it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 18, Training Loss: 0.9182587270541569\n","output_type":"stream"},{"name":"stderr","text":"Epoch 19/54: 100%|██████████| 391/391 [00:11<00:00, 33.93it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 19, Training Loss: 0.9079578424353734\n","output_type":"stream"},{"name":"stderr","text":"Epoch 20/54: 100%|██████████| 391/391 [00:11<00:00, 35.42it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 20, Training Loss: 0.8959687314070094\n","output_type":"stream"},{"name":"stderr","text":"Epoch 21/54: 100%|██████████| 391/391 [00:10<00:00, 36.08it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 21, Training Loss: 0.8879362975849825\n","output_type":"stream"},{"name":"stderr","text":"Epoch 22/54: 100%|██████████| 391/391 [00:11<00:00, 34.05it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 22, Training Loss: 0.8846084364234944\n","output_type":"stream"},{"name":"stderr","text":"Epoch 23/54: 100%|██████████| 391/391 [00:11<00:00, 35.46it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 23, Training Loss: 0.867362433534754\n","output_type":"stream"},{"name":"stderr","text":"Epoch 24/54: 100%|██████████| 391/391 [00:11<00:00, 35.47it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 24, Training Loss: 0.860125606169786\n","output_type":"stream"},{"name":"stderr","text":"Epoch 25/54: 100%|██████████| 391/391 [00:10<00:00, 35.74it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 25, Training Loss: 0.847225135854443\n","output_type":"stream"},{"name":"stderr","text":"Epoch 26/54: 100%|██████████| 391/391 [00:10<00:00, 36.48it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 26, Training Loss: 0.8362181098259929\n","output_type":"stream"},{"name":"stderr","text":"Epoch 27/54: 100%|██████████| 391/391 [00:11<00:00, 34.69it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 27, Training Loss: 0.8233318299893528\n","output_type":"stream"},{"name":"stderr","text":"Epoch 28/54: 100%|██████████| 391/391 [00:10<00:00, 36.11it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 28, Training Loss: 0.8185485031293787\n","output_type":"stream"},{"name":"stderr","text":"Epoch 29/54: 100%|██████████| 391/391 [00:10<00:00, 36.00it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 29, Training Loss: 0.7974282634227782\n","output_type":"stream"},{"name":"stderr","text":"Epoch 30/54: 100%|██████████| 391/391 [00:11<00:00, 35.14it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 30, Training Loss: 0.7959852116492093\n","output_type":"stream"},{"name":"stderr","text":"Epoch 31/54: 100%|██████████| 391/391 [00:10<00:00, 35.68it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 31, Training Loss: 0.786235325324261\n","output_type":"stream"},{"name":"stderr","text":"Epoch 32/54: 100%|██████████| 391/391 [00:10<00:00, 36.38it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 32, Training Loss: 0.7711078763922767\n","output_type":"stream"},{"name":"stderr","text":"Epoch 33/54: 100%|██████████| 391/391 [00:11<00:00, 34.52it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 33, Training Loss: 0.7697922616358608\n","output_type":"stream"},{"name":"stderr","text":"Epoch 34/54: 100%|██████████| 391/391 [00:10<00:00, 35.91it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 34, Training Loss: 0.7537462361480879\n","output_type":"stream"},{"name":"stderr","text":"Epoch 35/54: 100%|██████████| 391/391 [00:10<00:00, 35.59it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 35, Training Loss: 0.7477378136361651\n","output_type":"stream"},{"name":"stderr","text":"Epoch 36/54: 100%|██████████| 391/391 [00:11<00:00, 34.77it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 36, Training Loss: 0.7370004952716096\n","output_type":"stream"},{"name":"stderr","text":"Epoch 37/54: 100%|██████████| 391/391 [00:10<00:00, 36.27it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 37, Training Loss: 0.7245345413684845\n","output_type":"stream"},{"name":"stderr","text":"Epoch 38/54: 100%|██████████| 391/391 [00:11<00:00, 35.52it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 38, Training Loss: 0.7142928818150249\n","output_type":"stream"},{"name":"stderr","text":"Epoch 39/54: 100%|██████████| 391/391 [00:11<00:00, 33.92it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 39, Training Loss: 0.7021586065707\n","output_type":"stream"},{"name":"stderr","text":"Epoch 40/54: 100%|██████████| 391/391 [00:11<00:00, 35.54it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 40, Training Loss: 0.6916785901769653\n","output_type":"stream"},{"name":"stderr","text":"Epoch 41/54: 100%|██████████| 391/391 [00:11<00:00, 35.30it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 41, Training Loss: 0.6859340689828634\n","output_type":"stream"},{"name":"stderr","text":"Epoch 42/54: 100%|██████████| 391/391 [00:11<00:00, 34.19it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 42, Training Loss: 0.6779215898355255\n","output_type":"stream"},{"name":"stderr","text":"Epoch 43/54: 100%|██████████| 391/391 [00:11<00:00, 34.90it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 43, Training Loss: 0.6693788829361996\n","output_type":"stream"},{"name":"stderr","text":"Epoch 44/54: 100%|██████████| 391/391 [00:11<00:00, 34.90it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 44, Training Loss: 0.6609987685137697\n","output_type":"stream"},{"name":"stderr","text":"Epoch 45/54: 100%|██████████| 391/391 [00:11<00:00, 34.25it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 45, Training Loss: 0.6585582670805704\n","output_type":"stream"},{"name":"stderr","text":"Epoch 46/54: 100%|██████████| 391/391 [00:11<00:00, 35.43it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 46, Training Loss: 0.6530845629437195\n","output_type":"stream"},{"name":"stderr","text":"Epoch 47/54: 100%|██████████| 391/391 [00:11<00:00, 34.60it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 47, Training Loss: 0.6467831187388476\n","output_type":"stream"},{"name":"stderr","text":"Epoch 48/54: 100%|██████████| 391/391 [00:11<00:00, 35.13it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 48, Training Loss: 0.6455500016889304\n","output_type":"stream"},{"name":"stderr","text":"Epoch 49/54: 100%|██████████| 391/391 [00:11<00:00, 35.42it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 49, Training Loss: 0.6413988310205357\n","output_type":"stream"},{"name":"stderr","text":"Epoch 50/54: 100%|██████████| 391/391 [00:11<00:00, 33.88it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 50, Training Loss: 0.6356624158294609\n","output_type":"stream"},{"name":"stderr","text":"Epoch 51/54: 100%|██████████| 391/391 [00:11<00:00, 34.96it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 51, Training Loss: 0.6404678108137282\n","output_type":"stream"},{"name":"stderr","text":"Epoch 52/54: 100%|██████████| 391/391 [00:11<00:00, 34.41it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 52, Training Loss: 0.637955033641947\n","output_type":"stream"},{"name":"stderr","text":"Epoch 53/54: 100%|██████████| 391/391 [00:11<00:00, 34.15it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 53, Training Loss: 0.6411362423768738\n","output_type":"stream"},{"name":"stderr","text":"Epoch 54/54: 100%|██████████| 391/391 [00:11<00:00, 35.12it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 54, Training Loss: 0.6455451769901969\n","output_type":"stream"},{"name":"stderr","text":"Testing: 100%|██████████| 79/79 [00:01<00:00, 53.05it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 77.83%\nNumber of Parameters: 134314\nMillions of MACs (FLOPs): 0.88M\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"## gpt\n\nimport torch\nimport torch\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\n\n\nclass LightCIFAR10Net(nn.Module):\n    def __init__(self, num_classes=10):\n        super(LightCIFAR10Net, self).__init__()\n        self.stem = nn.Sequential(\n            nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1, bias=False),\n            nn.BatchNorm2d(32),\n            nn.GELU()\n        )\n\n        # Efficient blocks with depthwise separable convolutions\n        self.block1 = self._make_block(32, 64, 1)\n        self.block2 = self._make_block(64, 128, 2)\n        self.block3 = self._make_block(128, 256, 2)\n        self.block4 = self._make_block(256, 512, 2)\n\n        # Classifier head\n        self.global_pool = nn.AdaptiveAvgPool2d((1, 1))\n        self.fc = nn.Sequential(\n            nn.Linear(512, 256),\n            nn.GELU(),\n            nn.Dropout(0.2),\n            nn.Linear(256, num_classes)\n        )\n\n    def _make_block(self, in_channels, out_channels, stride):\n        return nn.Sequential(\n            nn.Conv2d(in_channels, in_channels, kernel_size=3, stride=stride, padding=1, groups=in_channels, bias=False),\n            nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.GELU()\n        )\n\n    def forward(self, x):\n        x = self.stem(x)\n        x = self.block1(x)\n        x = self.block2(x)\n        x = self.block3(x)\n        x = self.block4(x)\n        x = self.global_pool(x)\n        x = torch.flatten(x, 1)\n        x = self.fc(x)\n        return x\n\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = LightCIFAR10Net().to(device)\n\n# Data transforms\ntrain_transform = transforms.Compose([\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomCrop(32, padding=4),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=(0.4914, 0.4822, 0.4465), std=(0.2023, 0.1994, 0.2010)),\n])\n\ntest_transform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize(mean=(0.4914, 0.4822, 0.4465), std=(0.2023, 0.1994, 0.2010)),\n])\n\n# Load CIFAR-10 Dataset\ntrain_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=train_transform)\ntest_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=test_transform)\n\ntrain_loader = DataLoader(dataset=train_dataset, batch_size=128, shuffle=True, num_workers=4)\ntest_loader = DataLoader(dataset=test_dataset, batch_size=128, shuffle=False, num_workers=4)\n\n# Loss, Optimizer, Scheduler\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=3e-4)\nscheduler = CosineAnnealingLR(optimizer, T_max=50)\n\n# Train function\ndef train(model, train_loader, criterion, optimizer):\n    model.train()\n    running_loss = 0.0\n    correct = 0\n    total = 0\n    for images, labels in tqdm(train_loader, desc=\"Training\"):\n        images, labels = images.to(device), labels.to(device)\n        optimizer.zero_grad()\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n        _, predicted = outputs.max(1)\n        total += labels.size(0)\n        correct += predicted.eq(labels).sum().item()\n\n    accuracy = 100. * correct / total\n    print(f\"Train Loss: {running_loss/len(train_loader):.4f}, Train Accuracy: {accuracy:.2f}%\")\n    return accuracy\n\n# Test function\ndef test(model, test_loader, criterion):\n    model.eval()\n    running_loss = 0.0\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for images, labels in tqdm(test_loader, desc=\"Testing\"):\n            images, labels = images.to(device), labels.to(device)\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n\n            running_loss += loss.item()\n            _, predicted = outputs.max(1)\n            total += labels.size(0)\n            correct += predicted.eq(labels).sum().item()\n\n    accuracy = 100. * correct / total\n    print(f\"Test Loss: {running_loss/len(test_loader):.4f}, Test Accuracy: {accuracy:.2f}%\")\n    return accuracy\n\n# Training loop\nnum_epochs = 54\nbest_acc = 0.0\nfor epoch in range(num_epochs):\n    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n    train_acc = train(model, train_loader, criterion, optimizer)\n    test_acc = test(model, test_loader, criterion)\n    scheduler.step()\n\n    if test_acc > best_acc:\n        best_acc = test_acc\n        torch.save(model.state_dict(), \"best_cifar10_modelFinal.pth\")\n        print(f\"Saved new best model with accuracy: {best_acc:.2f}%\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-01T06:45:03.033826Z","iopub.execute_input":"2024-12-01T06:45:03.034160Z","iopub.status.idle":"2024-12-01T06:54:30.633602Z","shell.execute_reply.started":"2024-12-01T06:45:03.034131Z","shell.execute_reply":"2024-12-01T06:54:30.632583Z"}},"outputs":[{"name":"stdout","text":"Files already downloaded and verified\nFiles already downloaded and verified\nEpoch 1/54\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 391/391 [00:09<00:00, 42.76it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.6288, Train Accuracy: 38.97%\n","output_type":"stream"},{"name":"stderr","text":"Testing: 100%|██████████| 79/79 [00:01<00:00, 63.35it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 1.4351, Test Accuracy: 48.50%\nSaved new best model with accuracy: 48.50%\nEpoch 2/54\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 391/391 [00:09<00:00, 41.01it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.2106, Train Accuracy: 56.44%\n","output_type":"stream"},{"name":"stderr","text":"Testing: 100%|██████████| 79/79 [00:01<00:00, 64.25it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 1.3207, Test Accuracy: 54.07%\nSaved new best model with accuracy: 54.07%\nEpoch 3/54\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 391/391 [00:09<00:00, 43.36it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.0373, Train Accuracy: 62.94%\n","output_type":"stream"},{"name":"stderr","text":"Testing: 100%|██████████| 79/79 [00:01<00:00, 62.45it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 1.0186, Test Accuracy: 63.95%\nSaved new best model with accuracy: 63.95%\nEpoch 4/54\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 391/391 [00:09<00:00, 42.87it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.9380, Train Accuracy: 66.67%\n","output_type":"stream"},{"name":"stderr","text":"Testing: 100%|██████████| 79/79 [00:01<00:00, 63.72it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 1.2287, Test Accuracy: 57.73%\nEpoch 5/54\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 391/391 [00:09<00:00, 41.31it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.8762, Train Accuracy: 68.76%\n","output_type":"stream"},{"name":"stderr","text":"Testing: 100%|██████████| 79/79 [00:01<00:00, 62.85it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 0.9889, Test Accuracy: 65.25%\nSaved new best model with accuracy: 65.25%\nEpoch 6/54\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 391/391 [00:09<00:00, 43.39it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.8205, Train Accuracy: 71.09%\n","output_type":"stream"},{"name":"stderr","text":"Testing: 100%|██████████| 79/79 [00:01<00:00, 64.12it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 0.8592, Test Accuracy: 70.07%\nSaved new best model with accuracy: 70.07%\nEpoch 7/54\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 391/391 [00:09<00:00, 43.22it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.7793, Train Accuracy: 72.52%\n","output_type":"stream"},{"name":"stderr","text":"Testing: 100%|██████████| 79/79 [00:01<00:00, 64.33it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 0.8386, Test Accuracy: 70.66%\nSaved new best model with accuracy: 70.66%\nEpoch 8/54\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 391/391 [00:09<00:00, 41.17it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.7300, Train Accuracy: 74.44%\n","output_type":"stream"},{"name":"stderr","text":"Testing: 100%|██████████| 79/79 [00:01<00:00, 64.89it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 0.7670, Test Accuracy: 73.17%\nSaved new best model with accuracy: 73.17%\nEpoch 9/54\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 391/391 [00:08<00:00, 43.50it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.6994, Train Accuracy: 75.40%\n","output_type":"stream"},{"name":"stderr","text":"Testing: 100%|██████████| 79/79 [00:01<00:00, 62.39it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 0.7473, Test Accuracy: 74.13%\nSaved new best model with accuracy: 74.13%\nEpoch 10/54\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 391/391 [00:08<00:00, 43.59it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.6684, Train Accuracy: 76.48%\n","output_type":"stream"},{"name":"stderr","text":"Testing: 100%|██████████| 79/79 [00:01<00:00, 62.80it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 0.7259, Test Accuracy: 74.41%\nSaved new best model with accuracy: 74.41%\nEpoch 11/54\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 391/391 [00:09<00:00, 41.34it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.6443, Train Accuracy: 77.64%\n","output_type":"stream"},{"name":"stderr","text":"Testing: 100%|██████████| 79/79 [00:01<00:00, 61.13it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 0.6959, Test Accuracy: 75.74%\nSaved new best model with accuracy: 75.74%\nEpoch 12/54\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 391/391 [00:09<00:00, 43.27it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.6278, Train Accuracy: 78.22%\n","output_type":"stream"},{"name":"stderr","text":"Testing: 100%|██████████| 79/79 [00:01<00:00, 62.51it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 0.6577, Test Accuracy: 77.41%\nSaved new best model with accuracy: 77.41%\nEpoch 13/54\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 391/391 [00:08<00:00, 44.03it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.6045, Train Accuracy: 78.90%\n","output_type":"stream"},{"name":"stderr","text":"Testing: 100%|██████████| 79/79 [00:01<00:00, 63.18it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 0.7227, Test Accuracy: 74.45%\nEpoch 14/54\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 391/391 [00:08<00:00, 43.49it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.5928, Train Accuracy: 79.39%\n","output_type":"stream"},{"name":"stderr","text":"Testing: 100%|██████████| 79/79 [00:01<00:00, 50.55it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 0.6361, Test Accuracy: 78.08%\nSaved new best model with accuracy: 78.08%\nEpoch 15/54\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 391/391 [00:08<00:00, 44.05it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.5814, Train Accuracy: 79.60%\n","output_type":"stream"},{"name":"stderr","text":"Testing: 100%|██████████| 79/79 [00:01<00:00, 62.80it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 0.6258, Test Accuracy: 77.73%\nEpoch 16/54\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 391/391 [00:09<00:00, 43.16it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.5667, Train Accuracy: 80.14%\n","output_type":"stream"},{"name":"stderr","text":"Testing: 100%|██████████| 79/79 [00:01<00:00, 63.00it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 0.6703, Test Accuracy: 76.91%\nEpoch 17/54\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 391/391 [00:08<00:00, 43.49it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.5571, Train Accuracy: 80.51%\n","output_type":"stream"},{"name":"stderr","text":"Testing: 100%|██████████| 79/79 [00:01<00:00, 50.49it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 0.6216, Test Accuracy: 78.81%\nSaved new best model with accuracy: 78.81%\nEpoch 18/54\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 391/391 [00:09<00:00, 42.28it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.5358, Train Accuracy: 81.41%\n","output_type":"stream"},{"name":"stderr","text":"Testing: 100%|██████████| 79/79 [00:01<00:00, 61.75it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 0.6004, Test Accuracy: 79.39%\nSaved new best model with accuracy: 79.39%\nEpoch 19/54\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 391/391 [00:08<00:00, 43.68it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.5264, Train Accuracy: 81.73%\n","output_type":"stream"},{"name":"stderr","text":"Testing: 100%|██████████| 79/79 [00:01<00:00, 62.73it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 0.6396, Test Accuracy: 77.53%\nEpoch 20/54\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 391/391 [00:08<00:00, 43.46it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.5141, Train Accuracy: 82.08%\n","output_type":"stream"},{"name":"stderr","text":"Testing: 100%|██████████| 79/79 [00:01<00:00, 63.42it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 0.6018, Test Accuracy: 79.29%\nEpoch 21/54\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 391/391 [00:09<00:00, 41.30it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.5057, Train Accuracy: 82.51%\n","output_type":"stream"},{"name":"stderr","text":"Testing: 100%|██████████| 79/79 [00:01<00:00, 63.09it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 0.5886, Test Accuracy: 79.53%\nSaved new best model with accuracy: 79.53%\nEpoch 22/54\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 391/391 [00:09<00:00, 43.43it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.4938, Train Accuracy: 82.61%\n","output_type":"stream"},{"name":"stderr","text":"Testing: 100%|██████████| 79/79 [00:01<00:00, 63.33it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 0.5288, Test Accuracy: 81.69%\nSaved new best model with accuracy: 81.69%\nEpoch 23/54\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 391/391 [00:09<00:00, 43.32it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.4814, Train Accuracy: 83.22%\n","output_type":"stream"},{"name":"stderr","text":"Testing: 100%|██████████| 79/79 [00:01<00:00, 62.23it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 0.5496, Test Accuracy: 81.47%\nEpoch 24/54\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 391/391 [00:09<00:00, 40.87it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.4707, Train Accuracy: 83.70%\n","output_type":"stream"},{"name":"stderr","text":"Testing: 100%|██████████| 79/79 [00:01<00:00, 62.61it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 0.5933, Test Accuracy: 79.39%\nEpoch 25/54\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 391/391 [00:09<00:00, 43.42it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.4640, Train Accuracy: 83.89%\n","output_type":"stream"},{"name":"stderr","text":"Testing: 100%|██████████| 79/79 [00:01<00:00, 64.70it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 0.5354, Test Accuracy: 81.52%\nEpoch 26/54\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 391/391 [00:08<00:00, 43.66it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.4481, Train Accuracy: 84.44%\n","output_type":"stream"},{"name":"stderr","text":"Testing: 100%|██████████| 79/79 [00:01<00:00, 64.71it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 0.5722, Test Accuracy: 80.71%\nEpoch 27/54\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 391/391 [00:09<00:00, 40.63it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.4350, Train Accuracy: 84.85%\n","output_type":"stream"},{"name":"stderr","text":"Testing: 100%|██████████| 79/79 [00:01<00:00, 62.88it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 0.5518, Test Accuracy: 81.20%\nEpoch 28/54\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 391/391 [00:08<00:00, 43.68it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.4258, Train Accuracy: 85.27%\n","output_type":"stream"},{"name":"stderr","text":"Testing: 100%|██████████| 79/79 [00:01<00:00, 63.22it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 0.5280, Test Accuracy: 82.37%\nSaved new best model with accuracy: 82.37%\nEpoch 29/54\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 391/391 [00:08<00:00, 43.55it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.4113, Train Accuracy: 85.89%\n","output_type":"stream"},{"name":"stderr","text":"Testing: 100%|██████████| 79/79 [00:01<00:00, 62.80it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 0.4759, Test Accuracy: 84.21%\nSaved new best model with accuracy: 84.21%\nEpoch 30/54\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 391/391 [00:09<00:00, 41.21it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.4013, Train Accuracy: 85.99%\n","output_type":"stream"},{"name":"stderr","text":"Testing: 100%|██████████| 79/79 [00:01<00:00, 64.16it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 0.5109, Test Accuracy: 82.27%\nEpoch 31/54\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 391/391 [00:09<00:00, 43.43it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.3944, Train Accuracy: 86.26%\n","output_type":"stream"},{"name":"stderr","text":"Testing: 100%|██████████| 79/79 [00:01<00:00, 64.37it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 0.5099, Test Accuracy: 82.90%\nEpoch 32/54\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 391/391 [00:09<00:00, 41.17it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.3798, Train Accuracy: 86.75%\n","output_type":"stream"},{"name":"stderr","text":"Testing: 100%|██████████| 79/79 [00:01<00:00, 63.52it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 0.4889, Test Accuracy: 83.25%\nEpoch 33/54\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 391/391 [00:09<00:00, 40.36it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.3684, Train Accuracy: 87.07%\n","output_type":"stream"},{"name":"stderr","text":"Testing: 100%|██████████| 79/79 [00:01<00:00, 63.26it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 0.4464, Test Accuracy: 85.01%\nSaved new best model with accuracy: 85.01%\nEpoch 34/54\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 391/391 [00:08<00:00, 43.45it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.3565, Train Accuracy: 87.56%\n","output_type":"stream"},{"name":"stderr","text":"Testing: 100%|██████████| 79/79 [00:01<00:00, 63.46it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 0.5016, Test Accuracy: 83.35%\nEpoch 35/54\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 391/391 [00:09<00:00, 43.02it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.3423, Train Accuracy: 88.14%\n","output_type":"stream"},{"name":"stderr","text":"Testing: 100%|██████████| 79/79 [00:01<00:00, 62.69it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 0.4557, Test Accuracy: 84.66%\nEpoch 36/54\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 391/391 [00:09<00:00, 41.31it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.3331, Train Accuracy: 88.35%\n","output_type":"stream"},{"name":"stderr","text":"Testing: 100%|██████████| 79/79 [00:01<00:00, 63.15it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 0.4657, Test Accuracy: 84.54%\nEpoch 37/54\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 391/391 [00:09<00:00, 42.77it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.3216, Train Accuracy: 88.72%\n","output_type":"stream"},{"name":"stderr","text":"Testing: 100%|██████████| 79/79 [00:01<00:00, 62.49it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 0.4313, Test Accuracy: 85.54%\nSaved new best model with accuracy: 85.54%\nEpoch 38/54\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 391/391 [00:09<00:00, 43.16it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.3086, Train Accuracy: 89.21%\n","output_type":"stream"},{"name":"stderr","text":"Testing: 100%|██████████| 79/79 [00:01<00:00, 61.98it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 0.4186, Test Accuracy: 85.78%\nSaved new best model with accuracy: 85.78%\nEpoch 39/54\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 391/391 [00:09<00:00, 41.48it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.2997, Train Accuracy: 89.59%\n","output_type":"stream"},{"name":"stderr","text":"Testing: 100%|██████████| 79/79 [00:01<00:00, 59.01it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 0.4185, Test Accuracy: 85.89%\nSaved new best model with accuracy: 85.89%\nEpoch 40/54\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 391/391 [00:09<00:00, 43.17it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.2854, Train Accuracy: 90.05%\n","output_type":"stream"},{"name":"stderr","text":"Testing: 100%|██████████| 79/79 [00:01<00:00, 62.01it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 0.3917, Test Accuracy: 86.77%\nSaved new best model with accuracy: 86.77%\nEpoch 41/54\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 391/391 [00:08<00:00, 43.44it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.2745, Train Accuracy: 90.37%\n","output_type":"stream"},{"name":"stderr","text":"Testing: 100%|██████████| 79/79 [00:01<00:00, 63.26it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 0.3955, Test Accuracy: 86.59%\nEpoch 42/54\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 391/391 [00:09<00:00, 41.24it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.2614, Train Accuracy: 90.92%\n","output_type":"stream"},{"name":"stderr","text":"Testing: 100%|██████████| 79/79 [00:01<00:00, 63.67it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 0.3771, Test Accuracy: 87.24%\nSaved new best model with accuracy: 87.24%\nEpoch 43/54\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 391/391 [00:08<00:00, 43.48it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.2519, Train Accuracy: 91.28%\n","output_type":"stream"},{"name":"stderr","text":"Testing: 100%|██████████| 79/79 [00:01<00:00, 62.39it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 0.3884, Test Accuracy: 87.28%\nSaved new best model with accuracy: 87.28%\nEpoch 44/54\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 391/391 [00:09<00:00, 43.28it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.2417, Train Accuracy: 91.74%\n","output_type":"stream"},{"name":"stderr","text":"Testing: 100%|██████████| 79/79 [00:01<00:00, 61.68it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 0.3862, Test Accuracy: 87.18%\nEpoch 45/54\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 391/391 [00:09<00:00, 41.04it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.2317, Train Accuracy: 91.99%\n","output_type":"stream"},{"name":"stderr","text":"Testing: 100%|██████████| 79/79 [00:01<00:00, 63.29it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 0.3660, Test Accuracy: 87.50%\nSaved new best model with accuracy: 87.50%\nEpoch 46/54\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 391/391 [00:09<00:00, 43.02it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.2309, Train Accuracy: 92.06%\n","output_type":"stream"},{"name":"stderr","text":"Testing: 100%|██████████| 79/79 [00:01<00:00, 63.06it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 0.3581, Test Accuracy: 87.87%\nSaved new best model with accuracy: 87.87%\nEpoch 47/54\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 391/391 [00:09<00:00, 42.78it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.2192, Train Accuracy: 92.42%\n","output_type":"stream"},{"name":"stderr","text":"Testing: 100%|██████████| 79/79 [00:01<00:00, 62.82it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 0.3628, Test Accuracy: 87.62%\nEpoch 48/54\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 391/391 [00:09<00:00, 41.12it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.2150, Train Accuracy: 92.72%\n","output_type":"stream"},{"name":"stderr","text":"Testing: 100%|██████████| 79/79 [00:01<00:00, 62.92it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 0.3564, Test Accuracy: 87.85%\nEpoch 49/54\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 391/391 [00:09<00:00, 43.21it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.2119, Train Accuracy: 92.80%\n","output_type":"stream"},{"name":"stderr","text":"Testing: 100%|██████████| 79/79 [00:01<00:00, 63.22it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 0.3563, Test Accuracy: 87.91%\nSaved new best model with accuracy: 87.91%\nEpoch 50/54\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 391/391 [00:09<00:00, 43.10it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.2111, Train Accuracy: 92.78%\n","output_type":"stream"},{"name":"stderr","text":"Testing: 100%|██████████| 79/79 [00:01<00:00, 62.87it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 0.3541, Test Accuracy: 87.81%\nEpoch 51/54\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 391/391 [00:09<00:00, 40.95it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.2112, Train Accuracy: 92.72%\n","output_type":"stream"},{"name":"stderr","text":"Testing: 100%|██████████| 79/79 [00:01<00:00, 62.45it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 0.3569, Test Accuracy: 87.95%\nSaved new best model with accuracy: 87.95%\nEpoch 52/54\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 391/391 [00:09<00:00, 42.54it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.2112, Train Accuracy: 92.83%\n","output_type":"stream"},{"name":"stderr","text":"Testing: 100%|██████████| 79/79 [00:01<00:00, 63.98it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 0.3551, Test Accuracy: 87.82%\nEpoch 53/54\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 391/391 [00:09<00:00, 42.97it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.2117, Train Accuracy: 92.81%\n","output_type":"stream"},{"name":"stderr","text":"Testing: 100%|██████████| 79/79 [00:01<00:00, 64.37it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 0.3571, Test Accuracy: 87.80%\nEpoch 54/54\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 391/391 [00:10<00:00, 38.65it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.2121, Train Accuracy: 92.71%\n","output_type":"stream"},{"name":"stderr","text":"Testing: 100%|██████████| 79/79 [00:01<00:00, 48.99it/s]","output_type":"stream"},{"name":"stdout","text":"Test Loss: 0.3616, Test Accuracy: 87.77%\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"print(len(train_dataset))\nprint(len(test_dataset))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-01T06:24:08.303309Z","iopub.execute_input":"2024-12-01T06:24:08.303667Z","iopub.status.idle":"2024-12-01T06:24:08.309229Z","shell.execute_reply.started":"2024-12-01T06:24:08.303632Z","shell.execute_reply":"2024-12-01T06:24:08.308337Z"}},"outputs":[{"name":"stdout","text":"50000\n10000\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"### TEST","metadata":{}},{"cell_type":"code","source":"# Test dataset setup (as before)\ntest_filenames = [os.path.join('/kaggle/working/test_images/test', f) for f in os.listdir('/kaggle/working/test_images/test')]\ntest_filenames.sort(key=lambda x: int(x.split('/')[-1].split('.')[0]))\n\nclass CIFAR10TestDataset(Dataset):\n    def __init__(self, filenames, transform):\n        self.filenames = filenames\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.filenames)\n\n    def __getitem__(self, idx):\n        img = Image.open(self.filenames[idx])\n        if self.transform:\n            img = self.transform(img)\n        return img\n\ntest_dataset = CIFAR10TestDataset(test_filenames, test_transform)\ntest_loader = DataLoader(test_dataset, batch_size=128, shuffle=False, num_workers=2)\n\n# Predict on the test data (same as before)\nmodel.load_state_dict(torch.load('best_model.pth'))\nmodel = model.cuda()\n\nresult = []\nwith torch.no_grad():\n    model.eval()\n    for images in tqdm(test_loader):\n        images = images.cuda()\n        outputs = model(images)\n        preds = torch.argmax(outputs, 1)\n        preds = preds.cpu().numpy()\n        result.extend([classes[i] for i in preds])\n\n# Save the predictions to CSV\nsample = pd.read_csv('/kaggle/input/cifar-10/sampleSubmission.csv')\nsample['label'] = result\nsample.to_csv('./submissionfile.csv', index=False)\n\nprint(\"Predictions saved to submissionfile.csv\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T11:11:58.468637Z","iopub.execute_input":"2024-11-30T11:11:58.469017Z","iopub.status.idle":"2024-11-30T11:13:30.000577Z","shell.execute_reply.started":"2024-11-30T11:11:58.468984Z","shell.execute_reply":"2024-11-30T11:13:29.999455Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_30/952181040.py:23: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load('best_model.pth'))\n100%|██████████| 2344/2344 [01:30<00:00, 25.93it/s]\n","output_type":"stream"},{"name":"stdout","text":"Predictions saved to submissionfile.csv\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"import shutil\n\nshutil.rmtree('/kaggle/working/train_images')\nshutil.rmtree('/kaggle/working/test_images')\n\n# Check if the file exists and delete it\nfile_path = '/kaggle/working/best_model.pth'\nif os.path.exists(file_path):\n    os.remove(file_path)\n    print(f\"{file_path} has been deleted.\")\nelse:\n    print(f\"{file_path} does not exist.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T11:35:45.317343Z","iopub.execute_input":"2024-11-30T11:35:45.317712Z","iopub.status.idle":"2024-11-30T11:35:45.475581Z","shell.execute_reply.started":"2024-11-30T11:35:45.317682Z","shell.execute_reply":"2024-11-30T11:35:45.474321Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[11], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mshutil\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mshutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrmtree\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/kaggle/working/train_images\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m shutil\u001b[38;5;241m.\u001b[39mrmtree(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/kaggle/working/test_images\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Check if the file exists and delete it\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/shutil.py:715\u001b[0m, in \u001b[0;36mrmtree\u001b[0;34m(path, ignore_errors, onerror)\u001b[0m\n\u001b[1;32m    713\u001b[0m     orig_st \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mlstat(path)\n\u001b[1;32m    714\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m--> 715\u001b[0m     \u001b[43monerror\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexc_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    716\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    717\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/shutil.py:713\u001b[0m, in \u001b[0;36mrmtree\u001b[0;34m(path, ignore_errors, onerror)\u001b[0m\n\u001b[1;32m    710\u001b[0m \u001b[38;5;66;03m# Note: To guard against symlink races, we use the standard\u001b[39;00m\n\u001b[1;32m    711\u001b[0m \u001b[38;5;66;03m# lstat()/open()/fstat() trick.\u001b[39;00m\n\u001b[1;32m    712\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 713\u001b[0m     orig_st \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    714\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m    715\u001b[0m     onerror(os\u001b[38;5;241m.\u001b[39mlstat, path, sys\u001b[38;5;241m.\u001b[39mexc_info())\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/working/train_images'"],"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '/kaggle/working/train_images'","output_type":"error"}],"execution_count":11},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### train-94, test-92","metadata":{}},{"cell_type":"code","source":"import os\nimport random\nimport numpy as np\nimport math\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision import datasets, transforms\nfrom torch.optim.lr_scheduler import LambdaLR\nfrom tqdm import tqdm\n\n# Custom CutOut augmentation\nclass CutOut(object):\n    def __init__(self, n_holes, length):\n        self.n_holes = n_holes\n        self.length = length\n\n    def __call__(self, img):\n        h, w = img.size(1), img.size(2)\n        mask = np.ones((h, w), np.float32)\n\n        for n in range(self.n_holes):\n            y = random.randint(0, h - 1)\n            x = random.randint(0, w - 1)\n\n            y1 = np.clip(y - self.length // 2, 0, h)\n            y2 = np.clip(y + self.length // 2, 0, h)\n            x1 = np.clip(x - self.length // 2, 0, w)\n            x2 = np.clip(x + self.length // 2, 0, w)\n\n            mask[y1:y2, x1:x2] = 0.\n\n        mask = torch.from_numpy(mask).expand_as(img)\n        img = img * mask\n        return img\n\n# Data transformations\ntrain_transform = transforms.Compose([\n    transforms.RandomHorizontalFlip(p=0.5),\n    transforms.RandomAffine(degrees=15, translate=(0.1, 0.1), scale=(0.9, 1.1)),  # Rotate, scale, shift\n    transforms.RandomCrop(32, padding=4),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=(0.4914, 0.4822, 0.4465), std=(0.2023, 0.1994, 0.2010)),\n    CutOut(n_holes=1, length=8),\n])\n\ntest_transform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize(mean=(0.4914, 0.4822, 0.4465), std=(0.2023, 0.1994, 0.2010)),\n])\n\n# Load CIFAR-10 dataset using torchvision\ntrain_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=train_transform)\ntest_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=test_transform)\n\ntrain_loader = DataLoader(dataset=train_dataset, batch_size=128, shuffle=True, num_workers=4)\ntest_loader = DataLoader(dataset=test_dataset, batch_size=128, shuffle=False, num_workers=4)\n\n# Define the Residual Block and ResNet classes (as defined in your original code)\nclass ResidualBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n        super(ResidualBlock, self).__init__()\n        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(out_channels)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(out_channels)\n        self.downsample = downsample\n\n    def forward(self, x):\n        residual = x\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n        out = self.conv2(out)\n        out = self.bn2(out)\n        if self.downsample:\n            residual = self.downsample(x)\n        out += residual\n        out = self.relu(out)\n        return out\n\nclass ResNet(nn.Module):\n    def __init__(self, block, layers, num_classes=10, width_mult=1.0):\n        super(ResNet, self).__init__()\n        self.in_channels = int(64 * width_mult)\n        self.conv1 = nn.Conv2d(3, self.in_channels, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(self.in_channels)\n        self.relu = nn.ReLU(inplace=True)\n        self.layer1 = self.make_layer(block, int(64 * width_mult), layers[0], stride=1)\n        self.layer2 = self.make_layer(block, int(128 * width_mult), layers[1], stride=2)\n        self.layer3 = self.make_layer(block, int(256 * width_mult), layers[2], stride=2)\n        self.layer4 = self.make_layer(block, int(512 * width_mult), layers[3], stride=2)\n        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n        self.fc = nn.Linear(int(512 * width_mult), num_classes)\n\n    def make_layer(self, block, out_channels, blocks, stride=1):\n        downsample = None\n        if stride != 1 or self.in_channels != out_channels:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(out_channels)\n            )\n        layers = []\n        layers.append(block(self.in_channels, out_channels, stride, downsample))\n        self.in_channels = out_channels\n        for _ in range(1, blocks):\n            layers.append(block(out_channels, out_channels))\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        x = self.avg_pool(x)\n        x = torch.flatten(x, 1)\n        x = self.fc(x)\n        return x\n\n# Initialize model, loss function, optimizer, and learning rate scheduler\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = ResNet(ResidualBlock, [2, 2, 2, 2], width_mult=0.5).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=3e-4)\nscheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=50)\n\n# Training and Testing Functions\ndef train(model, train_loader, criterion, optimizer, scheduler):\n    model.train()\n    running_loss = 0.0\n    correct = 0\n    total = 0\n    for images, labels in tqdm(train_loader):\n        images, labels = images.to(device), labels.to(device)\n        optimizer.zero_grad()\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item()\n        _, predicted = outputs.max(1)\n        total += labels.size(0)\n        correct += predicted.eq(labels).sum().item()\n    scheduler.step()\n    print(f\"Train Loss: {running_loss/len(train_loader):.4f}, Train Accuracy: {100 * correct/total:.2f}%\")\n\ndef test(model, test_loader, criterion):\n    model.eval()\n    running_loss = 0.0\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for images, labels in tqdm(test_loader):\n            images, labels = images.to(device), labels.to(device)\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            running_loss += loss.item()\n            _, predicted = outputs.max(1)\n            total += labels.size(0)\n            correct += predicted.eq(labels).sum().item()\n    print(f\"Test Loss: {running_loss/len(test_loader):.4f}, Test Accuracy: {100 * correct/total:.2f}%\")\n    return 100 * correct / total\n\n# Main training loop\nbest_acc = 0\nfor epoch in range(60):  # 50 epochs\n    print(f\"Epoch {epoch+1}/50\")\n    train(model, train_loader, criterion, optimizer, scheduler)\n    acc = test(model, test_loader, criterion)\n    if acc > best_acc:\n        best_acc = acc\n        torch.save(model.state_dict(), \"best_new_model60.pth\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### test-94","metadata":{}},{"cell_type":"code","source":"import os\nfrom tqdm import tqdm\nimport torch\nfrom torch.utils.data import DataLoader\nimport torchvision\nimport torchvision.transforms as transforms\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\nfrom PIL import Image\nimport random\nimport pandas as pd\n!pip install py7zr\nimport py7zr\nfrom io import BytesIO\nimport torch.optim.lr_scheduler as lr_scheduler\n\n# Define a basic residual block with dropout\nclass ResidualBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, stride=1):\n        super(ResidualBlock, self).__init__()\n        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(out_channels)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(out_channels)\n        self.dropout = nn.Dropout(p=0.3)  # Dropout to regularize\n\n        self.skip_connection = nn.Sequential()\n        if stride != 1 or in_channels != out_channels:\n            self.skip_connection = nn.Sequential(\n                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(out_channels)\n            )\n\n    def forward(self, x):\n        identity = self.skip_connection(x)\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.dropout(out)  # Apply dropout\n        out += identity\n        out = self.relu(out)\n        return out\n\n# Define ResNet-34 for CIFAR-10\nclass ResNet34(nn.Module):\n    def __init__(self, block, num_blocks, num_classes=10):\n        super(ResNet34, self).__init__()\n        self.in_channels = 64\n\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.relu = nn.ReLU(inplace=True)\n        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n        self.fc = nn.Linear(512, num_classes)\n\n    def _make_layer(self, block, out_channels, blocks, stride):\n        strides = [stride] + [1] * (blocks - 1)\n        layers = []\n        for stride in strides:\n            layers.append(block(self.in_channels, out_channels, stride))\n            self.in_channels = out_channels\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n        out = self.layer1(out)\n        out = self.layer2(out)\n        out = self.layer3(out)\n        out = self.layer4(out)\n        out = self.avg_pool(out)\n        out = torch.flatten(out, 1)\n        out = self.fc(out)\n        return out\n\n# Function to instantiate the ResNet-34 model\ndef get_resnet34():\n    return ResNet34(ResidualBlock, [3, 4, 6, 3])\n\n# Custom CutOut transformation\nclass CutOut(object):\n    def __init__(self, n_holes, length):\n        self.n_holes = n_holes\n        self.length = length\n\n    def __call__(self, img):\n        h, w = img.size(1), img.size(2)\n        mask = np.ones((h, w), np.float32)\n\n        for n in range(self.n_holes):\n            y = random.randint(0, h)\n            x = random.randint(0, w)\n\n            y1 = np.clip(y - self.length // 2, 0, h)\n            y2 = np.clip(y + self.length // 2, 0, h)\n            x1 = np.clip(x - self.length // 2, 0, w)\n            x2 = np.clip(x + self.length // 2, 0, w)\n\n            mask[y1:y2, x1:x2] = 0.\n\n        mask = torch.from_numpy(mask).expand_as(img)\n        img = img * mask\n        return img\n\n# Transformation function using PyTorch's transforms\ntransform_train = transforms.Compose([\n    transforms.RandomHorizontalFlip(p=0.5),  # Horizontal flip\n    transforms.RandomAffine(degrees=15, translate=(0.1, 0.1), scale=(0.9, 1.1)),  # Rotate, scale, and shift\n    transforms.RandomCrop(32, padding=4),  # Random crop with padding\n    transforms.ToTensor(),\n    transforms.Normalize(mean=(0.4914, 0.4822, 0.4465), std=(0.2023, 0.1994, 0.2010)),\n    CutOut(n_holes=1, length=8),  # Use CutOut after ToTensor for regularization\n])\n\ntransform_test = transforms.Compose([\n    transforms.Resize((32, 32)),  # Resize to 32x32\n    transforms.ToTensor(),\n    transforms.Normalize(mean=(0.4914, 0.4822, 0.4465), std=(0.2023, 0.1994, 0.2010)),\n])\n\n# Example of applying the transformations to the CIFAR-10 dataset\ntrain_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\ntest_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size=256, shuffle=True)\ntest_loader = torch.utils.data.DataLoader(test_dataset, batch_size=100, shuffle=False)\n\n# Define device (GPU/CPU)\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Create model, define loss function and optimizer\nmodel = get_resnet34().to(device)\ncriterion = nn.CrossEntropyLoss()\n\n# Use AdamW optimizer with weight decay\noptimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-4)\n# Calculate steps per epoch\nsteps_per_epoch = len(train_loader)  # Number of batches in the training set\n\n# Cosine annealing scheduler with warmup\n\n# Define the OneCycleLR scheduler\nscheduler = lr_scheduler.OneCycleLR(\n    optimizer, \n    max_lr=0.001,        # The maximum learning rate after warmup\n    steps_per_epoch=steps_per_epoch,  # Total steps in one epoch (train dataset size / batch size)\n    epochs=80,           # Total number of epochs\n    pct_start=0.3,       # Warmup period (30% of the total steps)\n    anneal_strategy='cos',  # Cosine annealing after warmup\n    div_factor=25.0      # Initial learning rate will be max_lr / div_factor\n)\n\n# Training loop\ndef train(epoch):\n    model.train()\n    running_loss = 0.0\n    for batch_idx, (inputs, targets) in enumerate(train_loader):\n        inputs, targets = inputs.to(device), targets.to(device)\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs, targets)\n        loss.backward()\n        optimizer.step()\n\n        # Update the learning rate at each batch step\n        scheduler.step()\n\n        running_loss += loss.item()\n        if batch_idx % 100 == 99:\n            print(f\"Epoch {epoch+1}, Batch {batch_idx+1}: Loss = {running_loss / 100:.4f}\")\n            running_loss = 0.0\n\ndef test(epoch):\n    model.eval()\n    test_loss = 0.0\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for batch_idx, (inputs, targets) in enumerate(test_loader):\n            inputs, targets = inputs.to(device), targets.to(device)\n            outputs = model(inputs)\n            loss = criterion(outputs, targets)\n            test_loss += loss.item()\n\n            _, predicted = outputs.max(1)\n            total += targets.size(0)\n            correct += predicted.eq(targets).sum().item()\n\n        print(f\"Epoch {epoch+1}: Test Loss = {test_loss / len(test_loader):.4f}, Accuracy = {100. * correct / total:.2f}%\")\n\n# Main training loop\nfor epoch in range(5):\n    train(epoch)\n    test(epoch)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T05:06:10.797691Z","iopub.execute_input":"2024-12-02T05:06:10.798131Z","iopub.status.idle":"2024-12-02T05:18:35.886219Z","shell.execute_reply.started":"2024-12-02T05:06:10.798091Z","shell.execute_reply":"2024-12-02T05:18:35.885030Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: py7zr in /opt/conda/lib/python3.10/site-packages (0.22.0)\nRequirement already satisfied: texttable in /opt/conda/lib/python3.10/site-packages (from py7zr) (1.7.0)\nRequirement already satisfied: pycryptodomex>=3.16.0 in /opt/conda/lib/python3.10/site-packages (from py7zr) (3.21.0)\nRequirement already satisfied: pyzstd>=0.15.9 in /opt/conda/lib/python3.10/site-packages (from py7zr) (0.16.2)\nRequirement already satisfied: pyppmd<1.2.0,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from py7zr) (1.1.0)\nRequirement already satisfied: pybcj<1.1.0,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from py7zr) (1.0.2)\nRequirement already satisfied: multivolumefile>=0.2.3 in /opt/conda/lib/python3.10/site-packages (from py7zr) (0.2.3)\nRequirement already satisfied: inflate64<1.1.0,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from py7zr) (1.0.0)\nRequirement already satisfied: brotli>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from py7zr) (1.1.0)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from py7zr) (5.9.3)\nFiles already downloaded and verified\nFiles already downloaded and verified\nEpoch 1, Batch 100: Loss = 2.1271\nEpoch 1: Test Loss = 3.4228, Accuracy = 19.50%\nEpoch 2, Batch 100: Loss = 1.7461\nEpoch 2: Test Loss = 1.8835, Accuracy = 36.52%\nEpoch 3, Batch 100: Loss = 1.5620\nEpoch 3: Test Loss = 1.7684, Accuracy = 43.58%\nEpoch 4, Batch 100: Loss = 1.4393\nEpoch 4: Test Loss = 1.6325, Accuracy = 48.06%\nEpoch 5, Batch 100: Loss = 1.3019\nEpoch 5: Test Loss = 1.1812, Accuracy = 60.49%\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"!pip install torchsummary\n!pip install fvcore","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T14:58:30.594972Z","iopub.execute_input":"2024-12-04T14:58:30.595879Z","iopub.status.idle":"2024-12-04T14:58:52.469241Z","shell.execute_reply.started":"2024-12-04T14:58:30.595832Z","shell.execute_reply":"2024-12-04T14:58:52.468088Z"}},"outputs":[{"name":"stdout","text":"Collecting torchsummary\n  Downloading torchsummary-1.5.1-py3-none-any.whl.metadata (296 bytes)\nDownloading torchsummary-1.5.1-py3-none-any.whl (2.8 kB)\nInstalling collected packages: torchsummary\nSuccessfully installed torchsummary-1.5.1\nCollecting fvcore\n  Downloading fvcore-0.1.5.post20221221.tar.gz (50 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.2/50.2 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from fvcore) (1.26.4)\nCollecting yacs>=0.1.6 (from fvcore)\n  Downloading yacs-0.1.8-py3-none-any.whl.metadata (639 bytes)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from fvcore) (6.0.2)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from fvcore) (4.66.4)\nRequirement already satisfied: termcolor>=1.1 in /opt/conda/lib/python3.10/site-packages (from fvcore) (2.4.0)\nRequirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from fvcore) (10.3.0)\nRequirement already satisfied: tabulate in /opt/conda/lib/python3.10/site-packages (from fvcore) (0.9.0)\nCollecting iopath>=0.1.7 (from fvcore)\n  Downloading iopath-0.1.10.tar.gz (42 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: typing_extensions in /opt/conda/lib/python3.10/site-packages (from iopath>=0.1.7->fvcore) (4.12.2)\nCollecting portalocker (from iopath>=0.1.7->fvcore)\n  Downloading portalocker-3.0.0-py3-none-any.whl.metadata (8.5 kB)\nDownloading yacs-0.1.8-py3-none-any.whl (14 kB)\nDownloading portalocker-3.0.0-py3-none-any.whl (19 kB)\nBuilding wheels for collected packages: fvcore, iopath\n  Building wheel for fvcore (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for fvcore: filename=fvcore-0.1.5.post20221221-py3-none-any.whl size=61400 sha256=c1d0e85651628ad569584bcd01bf47fc26828addf1faa2a1feaacc019eb21167\n  Stored in directory: /root/.cache/pip/wheels/01/c0/af/77c1cf53a1be9e42a52b48e5af2169d40ec2e89f7362489dd0\n  Building wheel for iopath (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for iopath: filename=iopath-0.1.10-py3-none-any.whl size=31532 sha256=1c432ae21e8282b8fc532ed09788a964f5b8e5358ffed6bbf62e3ae3659a9b55\n  Stored in directory: /root/.cache/pip/wheels/9a/a3/b6/ac0fcd1b4ed5cfeb3db92e6a0e476cfd48ed0df92b91080c1d\nSuccessfully built fvcore iopath\nInstalling collected packages: yacs, portalocker, iopath, fvcore\nSuccessfully installed fvcore-0.1.5.post20221221 iopath-0.1.10 portalocker-3.0.0 yacs-0.1.8\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"### FIRST SUBMISSION","metadata":{}},{"cell_type":"code","source":"import os\nfrom tqdm import tqdm\nimport torch\nfrom torch.utils.data import DataLoader\nimport torchvision\nimport torchvision.transforms as transforms\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\nfrom fvcore.nn import FlopCountAnalysis, flop_count_table\nfrom torchinfo import summary\nfrom PIL import Image\nimport random\n\n# Define a basic convolutional block with GELU activation\nclass ConvBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, stride=1):\n        super(ConvBlock, self).__init__()\n        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(out_channels)\n        self.act = nn.GELU()\n        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(out_channels)\n        self.dropout = nn.Dropout(p=0.3)\n\n        self.skip_connection = nn.Sequential()\n        if stride != 1 or in_channels != out_channels:\n            self.skip_connection = nn.Sequential(\n                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(out_channels)\n            )\n\n    def forward(self, x):\n        identity = self.skip_connection(x)\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.act(out)\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.dropout(out)\n        out += identity\n        out = self.act(out)\n        return out\n\n# Define the base architecture for CIFAR-10\nclass BaseArchitecture(nn.Module):\n    def __init__(self, block, num_blocks, num_classes=10):\n        super(BaseArchitecture, self).__init__()\n        self.in_channels = 32\n\n        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(32)\n        self.act = nn.GELU()\n        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n        self.fc = nn.Linear(256, num_classes)\n\n    def _make_layer(self, block, out_channels, blocks, stride):\n        strides = [stride] + [1] * (blocks - 1)\n        layers = []\n        for stride in strides:\n            layers.append(block(self.in_channels, out_channels, stride))\n            self.in_channels = out_channels\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.act(out)\n        out = self.layer1(out)\n        out = self.layer2(out)\n        out = self.layer3(out)\n        out = self.avg_pool(out)\n        out = torch.flatten(out, 1)\n        out = self.fc(out)\n        return out\n\ndef get_base_architecture():\n    return BaseArchitecture(ConvBlock, [2, 2, 2])\n\n# Training configurations\ntransform_train = transforms.Compose([\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomCrop(32, padding=4),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=(0.4914, 0.4822, 0.4465), std=(0.2023, 0.1994, 0.2010)),\n])\n\ntransform_test = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize(mean=(0.4914, 0.4822, 0.4465), std=(0.2023, 0.1994, 0.2010)),\n])\n\ntrain_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\ntest_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n\ntrain_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=2)\ntest_loader = DataLoader(test_dataset, batch_size=100, shuffle=False, num_workers=2)\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nmodel = get_base_architecture().to(device)\n\n# Calculate FLOPs and Params\ndummy_input = torch.randn(1, 3, 32, 32).to(device)\nflops = FlopCountAnalysis(model, dummy_input)\nprint(\"FLOPs and Parameters BEFORE Training:\")\nprint(flop_count_table(flops))\nprint(f\"Total Parameters: {sum(p.numel() for p in model.parameters())}\")\n\n# Loss, optimizer, scheduler\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=3e-4)\nscheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=50)\n\n# Training loop\ndef train(epoch):\n    model.train()\n    running_loss = 0.0\n    for inputs, targets in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{50}\"):\n        inputs, targets = inputs.to(device), targets.to(device)\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs, targets)\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item()\n    scheduler.step()\n    print(f\"Training Loss: {running_loss / len(train_loader):.4f}\")\n\n# Test loop\ndef test():\n    model.eval()\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for inputs, targets in test_loader:\n            inputs, targets = inputs.to(device), targets.to(device)\n            outputs = model(inputs)\n            _, predicted = outputs.max(1)\n            total += targets.size(0)\n            correct += predicted.eq(targets).sum().item()\n    print(f\"Test Accuracy: {100. * correct / total:.2f}%\")\n\n# Train and save model\nfor epoch in range(50):\n    train(epoch)\n    test()\n\nprint(\"FLOPs and Parameters AFTER Training:\")\nprint(flop_count_table(flops))\nprint(f\"Total Parameters AFTER Training: {sum(p.numel() for p in model.parameters())}\")\n\ntorch.save(model.state_dict(), \"cifar_last_model.pth\")\nprint(\"MODEL SAVED\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T06:25:19.099600Z","iopub.execute_input":"2024-12-02T06:25:19.100448Z","iopub.status.idle":"2024-12-02T06:54:39.810933Z","shell.execute_reply.started":"2024-12-02T06:25:19.100405Z","shell.execute_reply":"2024-12-02T06:54:39.809810Z"}},"outputs":[{"name":"stdout","text":"Files already downloaded and verified\nFiles already downloaded and verified\nFLOPs and Parameters BEFORE Training:\n| module                      | #parameters or shape   | #flops     |\n|:----------------------------|:-----------------------|:-----------|\n| model                       | 2.76M                  | 0.407G     |\n|  conv1                      |  0.864K                |  0.885M    |\n|   conv1.weight              |   (32, 3, 3, 3)        |            |\n|  bn1                        |  64                    |  0.164M    |\n|   bn1.weight                |   (32,)                |            |\n|   bn1.bias                  |   (32,)                |            |\n|  layer1                     |  0.132M                |  0.136G    |\n|   layer1.0                  |   57.728K              |   59.703M  |\n|    layer1.0.conv1           |    18.432K             |    18.874M |\n|    layer1.0.bn1             |    0.128K              |    0.328M  |\n|    layer1.0.conv2           |    36.864K             |    37.749M |\n|    layer1.0.bn2             |    0.128K              |    0.328M  |\n|    layer1.0.skip_connection |    2.176K              |    2.425M  |\n|   layer1.1                  |   73.984K              |   76.153M  |\n|    layer1.1.conv1           |    36.864K             |    37.749M |\n|    layer1.1.bn1             |    0.128K              |    0.328M  |\n|    layer1.1.conv2           |    36.864K             |    37.749M |\n|    layer1.1.bn2             |    0.128K              |    0.328M  |\n|  layer2                     |  0.526M                |  0.135G    |\n|   layer2.0                  |   0.23M                |   59.212M  |\n|    layer2.0.conv1           |    73.728K             |    18.874M |\n|    layer2.0.bn1             |    0.256K              |    0.164M  |\n|    layer2.0.conv2           |    0.147M              |    37.749M |\n|    layer2.0.bn2             |    0.256K              |    0.164M  |\n|    layer2.0.skip_connection |    8.448K              |    2.261M  |\n|   layer2.1                  |   0.295M               |   75.825M  |\n|    layer2.1.conv1           |    0.147M              |    37.749M |\n|    layer2.1.bn1             |    0.256K              |    0.164M  |\n|    layer2.1.conv2           |    0.147M              |    37.749M |\n|    layer2.1.bn2             |    0.256K              |    0.164M  |\n|  layer3                     |  2.1M                  |  0.135G    |\n|   layer3.0                  |   0.919M               |   58.966M  |\n|    layer3.0.conv1           |    0.295M              |    18.874M |\n|    layer3.0.bn1             |    0.512K              |    81.92K  |\n|    layer3.0.conv2           |    0.59M               |    37.749M |\n|    layer3.0.bn2             |    0.512K              |    81.92K  |\n|    layer3.0.skip_connection |    33.28K              |    2.179M  |\n|   layer3.1                  |   1.181M               |   75.661M  |\n|    layer3.1.conv1           |    0.59M               |    37.749M |\n|    layer3.1.bn1             |    0.512K              |    81.92K  |\n|    layer3.1.conv2           |    0.59M               |    37.749M |\n|    layer3.1.bn2             |    0.512K              |    81.92K  |\n|  fc                         |  2.57K                 |  2.56K     |\n|   fc.weight                 |   (10, 256)            |            |\n|   fc.bias                   |   (10,)                |            |\n|  avg_pool                   |                        |  16.384K   |\nTotal Parameters: 2760490\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/50: 100%|██████████| 391/391 [00:33<00:00, 11.53it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 1.6032\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 42.59%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/50: 100%|██████████| 391/391 [00:32<00:00, 12.01it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 1.0943\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 61.87%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/50: 100%|██████████| 391/391 [00:33<00:00, 11.80it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.8885\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 67.92%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/50: 100%|██████████| 391/391 [00:33<00:00, 11.78it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.7398\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 75.03%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/50: 100%|██████████| 391/391 [00:33<00:00, 11.83it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.6293\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 75.93%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6/50: 100%|██████████| 391/391 [00:32<00:00, 11.87it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.5667\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 81.12%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7/50: 100%|██████████| 391/391 [00:32<00:00, 11.89it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.5227\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 78.90%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8/50: 100%|██████████| 391/391 [00:32<00:00, 11.86it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.4844\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 80.80%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9/50: 100%|██████████| 391/391 [00:33<00:00, 11.83it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.4605\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 82.36%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10/50: 100%|██████████| 391/391 [00:33<00:00, 11.79it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.4329\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 83.49%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 11/50: 100%|██████████| 391/391 [00:32<00:00, 11.85it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.4150\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 82.35%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 12/50: 100%|██████████| 391/391 [00:32<00:00, 11.88it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.3949\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 85.76%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 13/50: 100%|██████████| 391/391 [00:32<00:00, 11.87it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.3783\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 85.90%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 14/50: 100%|██████████| 391/391 [00:33<00:00, 11.82it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.3661\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 86.86%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 15/50: 100%|██████████| 391/391 [00:33<00:00, 11.81it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.3498\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 85.00%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 16/50: 100%|██████████| 391/391 [00:33<00:00, 11.82it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.3339\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 85.69%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 17/50: 100%|██████████| 391/391 [00:33<00:00, 11.84it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.3217\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 86.51%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 18/50: 100%|██████████| 391/391 [00:33<00:00, 11.84it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.3124\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 87.55%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 19/50: 100%|██████████| 391/391 [00:32<00:00, 11.85it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.3010\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 87.61%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 20/50: 100%|██████████| 391/391 [00:33<00:00, 11.83it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.2866\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 86.63%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 21/50: 100%|██████████| 391/391 [00:32<00:00, 11.85it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.2736\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 88.27%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 22/50: 100%|██████████| 391/391 [00:33<00:00, 11.84it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.2683\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 88.21%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 23/50: 100%|██████████| 391/391 [00:33<00:00, 11.83it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.2531\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 88.07%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 24/50: 100%|██████████| 391/391 [00:33<00:00, 11.84it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.2414\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 89.96%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 25/50: 100%|██████████| 391/391 [00:33<00:00, 11.84it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.2287\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 87.84%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 26/50: 100%|██████████| 391/391 [00:33<00:00, 11.83it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.2204\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 88.87%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 27/50: 100%|██████████| 391/391 [00:33<00:00, 11.82it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.2098\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 89.17%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 28/50: 100%|██████████| 391/391 [00:33<00:00, 11.83it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.2001\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 89.28%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 29/50: 100%|██████████| 391/391 [00:33<00:00, 11.84it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.1837\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 88.89%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 30/50: 100%|██████████| 391/391 [00:33<00:00, 11.81it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.1730\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 90.40%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 31/50: 100%|██████████| 391/391 [00:33<00:00, 11.82it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.1635\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 89.05%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 32/50: 100%|██████████| 391/391 [00:32<00:00, 11.93it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.1511\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 91.35%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 33/50: 100%|██████████| 391/391 [00:32<00:00, 11.89it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.1407\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 91.31%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 34/50: 100%|██████████| 391/391 [00:32<00:00, 11.89it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.1270\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 91.12%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 35/50: 100%|██████████| 391/391 [00:32<00:00, 11.86it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.1193\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 90.87%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 36/50: 100%|██████████| 391/391 [00:32<00:00, 11.87it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.1060\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 91.38%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 37/50: 100%|██████████| 391/391 [00:33<00:00, 11.83it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.0929\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 92.46%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 38/50: 100%|██████████| 391/391 [00:33<00:00, 11.83it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.0804\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 92.10%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 39/50: 100%|██████████| 391/391 [00:32<00:00, 11.89it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.0732\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 91.95%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 40/50: 100%|██████████| 391/391 [00:32<00:00, 11.88it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.0622\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 92.48%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 41/50: 100%|██████████| 391/391 [00:32<00:00, 11.89it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.0533\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 92.17%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 42/50: 100%|██████████| 391/391 [00:32<00:00, 11.86it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.0465\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 92.51%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 43/50: 100%|██████████| 391/391 [00:32<00:00, 11.87it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.0420\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 92.67%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 44/50: 100%|██████████| 391/391 [00:32<00:00, 11.85it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.0382\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 93.06%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 45/50: 100%|██████████| 391/391 [00:33<00:00, 11.84it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.0329\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 92.99%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 46/50: 100%|██████████| 391/391 [00:33<00:00, 11.84it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.0314\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 93.08%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 47/50: 100%|██████████| 391/391 [00:33<00:00, 11.81it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.0284\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 93.09%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 48/50: 100%|██████████| 391/391 [00:33<00:00, 11.83it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.0282\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 93.14%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 49/50: 100%|██████████| 391/391 [00:32<00:00, 11.88it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.0277\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 93.22%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 50/50: 100%|██████████| 391/391 [00:32<00:00, 11.88it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.0267\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 93.12%\nFLOPs and Parameters AFTER Training:\n| module                      | #parameters or shape   | #flops     |\n|:----------------------------|:-----------------------|:-----------|\n| model                       | 2.76M                  | 0.407G     |\n|  conv1                      |  0.864K                |  0.885M    |\n|   conv1.weight              |   (32, 3, 3, 3)        |            |\n|  bn1                        |  64                    |  0.164M    |\n|   bn1.weight                |   (32,)                |            |\n|   bn1.bias                  |   (32,)                |            |\n|  layer1                     |  0.132M                |  0.136G    |\n|   layer1.0                  |   57.728K              |   59.703M  |\n|    layer1.0.conv1           |    18.432K             |    18.874M |\n|    layer1.0.bn1             |    0.128K              |    0.328M  |\n|    layer1.0.conv2           |    36.864K             |    37.749M |\n|    layer1.0.bn2             |    0.128K              |    0.328M  |\n|    layer1.0.skip_connection |    2.176K              |    2.425M  |\n|   layer1.1                  |   73.984K              |   76.153M  |\n|    layer1.1.conv1           |    36.864K             |    37.749M |\n|    layer1.1.bn1             |    0.128K              |    0.328M  |\n|    layer1.1.conv2           |    36.864K             |    37.749M |\n|    layer1.1.bn2             |    0.128K              |    0.328M  |\n|  layer2                     |  0.526M                |  0.135G    |\n|   layer2.0                  |   0.23M                |   59.212M  |\n|    layer2.0.conv1           |    73.728K             |    18.874M |\n|    layer2.0.bn1             |    0.256K              |    0.164M  |\n|    layer2.0.conv2           |    0.147M              |    37.749M |\n|    layer2.0.bn2             |    0.256K              |    0.164M  |\n|    layer2.0.skip_connection |    8.448K              |    2.261M  |\n|   layer2.1                  |   0.295M               |   75.825M  |\n|    layer2.1.conv1           |    0.147M              |    37.749M |\n|    layer2.1.bn1             |    0.256K              |    0.164M  |\n|    layer2.1.conv2           |    0.147M              |    37.749M |\n|    layer2.1.bn2             |    0.256K              |    0.164M  |\n|  layer3                     |  2.1M                  |  0.135G    |\n|   layer3.0                  |   0.919M               |   58.966M  |\n|    layer3.0.conv1           |    0.295M              |    18.874M |\n|    layer3.0.bn1             |    0.512K              |    81.92K  |\n|    layer3.0.conv2           |    0.59M               |    37.749M |\n|    layer3.0.bn2             |    0.512K              |    81.92K  |\n|    layer3.0.skip_connection |    33.28K              |    2.179M  |\n|   layer3.1                  |   1.181M               |   75.661M  |\n|    layer3.1.conv1           |    0.59M               |    37.749M |\n|    layer3.1.bn1             |    0.512K              |    81.92K  |\n|    layer3.1.conv2           |    0.59M               |    37.749M |\n|    layer3.1.bn2             |    0.512K              |    81.92K  |\n|  fc                         |  2.57K                 |  2.56K     |\n|   fc.weight                 |   (10, 256)            |            |\n|   fc.bias                   |   (10,)                |            |\n|  avg_pool                   |                        |  16.384K   |\nTotal Parameters AFTER Training: 2760490\nMODEL SAVED\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"!pip install gdown","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T14:58:52.471000Z","iopub.execute_input":"2024-12-04T14:58:52.471302Z","iopub.status.idle":"2024-12-04T14:59:00.770060Z","shell.execute_reply.started":"2024-12-04T14:58:52.471267Z","shell.execute_reply":"2024-12-04T14:59:00.769008Z"}},"outputs":[{"name":"stdout","text":"Collecting gdown\n  Downloading gdown-5.2.0-py3-none-any.whl.metadata (5.8 kB)\nRequirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.10/site-packages (from gdown) (4.12.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from gdown) (3.15.1)\nRequirement already satisfied: requests[socks] in /opt/conda/lib/python3.10/site-packages (from gdown) (2.32.3)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from gdown) (4.66.4)\nRequirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.10/site-packages (from beautifulsoup4->gdown) (2.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (2024.6.2)\nRequirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (1.7.1)\nDownloading gdown-5.2.0-py3-none-any.whl (18 kB)\nInstalling collected packages: gdown\nSuccessfully installed gdown-5.2.0\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import gdown\ngdown.download(f\"https://drive.google.com/uc?id=19y5_-tSLzIjhjvT86NdqL0OB0h2b7b-s\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T14:59:00.771421Z","iopub.execute_input":"2024-12-04T14:59:00.771706Z","iopub.status.idle":"2024-12-04T14:59:04.416818Z","shell.execute_reply.started":"2024-12-04T14:59:00.771679Z","shell.execute_reply":"2024-12-04T14:59:04.415929Z"}},"outputs":[{"name":"stderr","text":"Downloading...\nFrom: https://drive.google.com/uc?id=19y5_-tSLzIjhjvT86NdqL0OB0h2b7b-s\nTo: /kaggle/working/b13.PNG\n100%|██████████| 610k/610k [00:00<00:00, 104MB/s]\n","output_type":"stream"},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"'b13.PNG'"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"import torch\nfrom torchvision import transforms\nfrom PIL import Image\n\n# Load the trained model\nmodel = get_base_architecture()\nmodel.load_state_dict(torch.load(\"/kaggle/working/cifar_last_model.pth\"))\nmodel.eval()\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel.to(device)\n\n# Define the transform pipeline for single image\ntransform_test_single = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize(mean=(0.4914, 0.4822, 0.4465), std=(0.2023, 0.1994, 0.2010)),\n])\n\n# Class names for CIFAR-10\nclass_names = [\n    'airplane', 'automobile', 'bird', 'cat', 'deer',\n    'dog', 'frog', 'horse', 'ship', 'truck'\n]\n\n# Preprocess function\ndef preprocess_image(image_path):\n    image = Image.open(image_path).convert('RGB')\n    image = transform_test_single(image)\n    image = image.unsqueeze(0)\n    return image\n\n# Predict function\ndef predict(image_path, model, class_names):\n    image = preprocess_image(image_path).to(device)\n    with torch.no_grad():\n        outputs = model(image)\n        _, predicted_class = outputs.max(1)\n    return class_names[predicted_class.item()]\n\n# Test on a new image\nimage_path = \"/kaggle/working/b13.PNG\"  # Replace with your image path\npredicted_label = predict(image_path, model, class_names)\nprint(f\"Predicted Class: {predicted_label}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T07:08:19.071657Z","iopub.execute_input":"2024-12-02T07:08:19.072786Z","iopub.status.idle":"2024-12-02T07:08:19.241173Z","shell.execute_reply.started":"2024-12-02T07:08:19.072751Z","shell.execute_reply":"2024-12-02T07:08:19.240261Z"}},"outputs":[{"name":"stdout","text":"Predicted Class: cat\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_30/4119229442.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load(\"/kaggle/working/cifar_last_model.pth\"))\n","output_type":"stream"}],"execution_count":24},{"cell_type":"markdown","source":"## just test","metadata":{}},{"cell_type":"code","source":"# Define a basic residual block with GELU activation\nclass ConvBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, stride=1):\n        super(ConvBlock, self).__init__()\n        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(out_channels)\n        self.act = nn.GELU()\n        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(out_channels)\n        self.dropout = nn.Dropout(p=0.3)\n\n        self.skip_connection = nn.Sequential()\n        if stride != 1 or in_channels != out_channels:\n            self.skip_connection = nn.Sequential(\n                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(out_channels)\n            )\n\n    def forward(self, x):\n        identity = self.skip_connection(x)\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.act(out)\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.dropout(out)\n        out += identity\n        out = self.act(out)\n        return out\n\n# Define the base architecture for CIFAR-10\nclass BaseArchitecture(nn.Module):\n    def __init__(self, block, num_blocks, num_classes=10):\n        super(BaseArchitecture, self).__init__()\n        self.in_channels = 32\n\n        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(32)\n        self.act = nn.GELU()\n        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n        self.fc = nn.Linear(256, num_classes)\n\n    def _make_layer(self, block, out_channels, blocks, stride):\n        strides = [stride] + [1] * (blocks - 1)\n        layers = []\n        for stride in strides:\n            layers.append(block(self.in_channels, out_channels, stride))\n            self.in_channels = out_channels\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.act(out)\n        out = self.layer1(out)\n        out = self.layer2(out)\n        out = self.layer3(out)\n        out = self.avg_pool(out)\n        out = torch.flatten(out, 1)\n        out = self.fc(out)\n        return out","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T06:54:39.812983Z","iopub.execute_input":"2024-12-02T06:54:39.813294Z","iopub.status.idle":"2024-12-02T06:54:39.826285Z","shell.execute_reply.started":"2024-12-02T06:54:39.813263Z","shell.execute_reply":"2024-12-02T06:54:39.825468Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"def get_base_architecture():\n    return BaseArchitecture(ConvBlock, [2, 2, 2])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T06:54:39.827246Z","iopub.execute_input":"2024-12-02T06:54:39.827505Z","iopub.status.idle":"2024-12-02T06:54:39.838685Z","shell.execute_reply.started":"2024-12-02T06:54:39.827480Z","shell.execute_reply":"2024-12-02T06:54:39.837934Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"# Training configurations\ntransform_train = transforms.Compose([\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomCrop(32, padding=4),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=(0.4914, 0.4822, 0.4465), std=(0.2023, 0.1994, 0.2010)),\n])\n\ntransform_test = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize(mean=(0.4914, 0.4822, 0.4465), std=(0.2023, 0.1994, 0.2010)),\n])\n\ntrain_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\ntest_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n\ntrain_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=2)\ntest_loader = DataLoader(test_dataset, batch_size=100, shuffle=False, num_workers=2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T06:54:39.840085Z","iopub.execute_input":"2024-12-02T06:54:39.840330Z","iopub.status.idle":"2024-12-02T06:54:41.409964Z","shell.execute_reply.started":"2024-12-02T06:54:39.840306Z","shell.execute_reply":"2024-12-02T06:54:41.409116Z"}},"outputs":[{"name":"stdout","text":"Files already downloaded and verified\nFiles already downloaded and verified\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nmodel = get_base_architecture().to(device)\n\n# Calculate FLOPs and Params\ndummy_input = torch.randn(1, 3, 32, 32).to(device)\nflops = FlopCountAnalysis(model, dummy_input)\nprint(\"FLOPs and Parameters BEFORE Training:\")\nprint(flop_count_table(flops))\nprint(f\"Total Parameters: {sum(p.numel() for p in model.parameters())}\")\n\n# Loss, optimizer, scheduler\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=3e-4)\nscheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=50)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T06:54:41.410919Z","iopub.execute_input":"2024-12-02T06:54:41.411158Z","iopub.status.idle":"2024-12-02T06:54:41.539701Z","shell.execute_reply.started":"2024-12-02T06:54:41.411134Z","shell.execute_reply":"2024-12-02T06:54:41.538908Z"}},"outputs":[{"name":"stdout","text":"FLOPs and Parameters BEFORE Training:\n| module                      | #parameters or shape   | #flops     |\n|:----------------------------|:-----------------------|:-----------|\n| model                       | 2.76M                  | 0.407G     |\n|  conv1                      |  0.864K                |  0.885M    |\n|   conv1.weight              |   (32, 3, 3, 3)        |            |\n|  bn1                        |  64                    |  0.164M    |\n|   bn1.weight                |   (32,)                |            |\n|   bn1.bias                  |   (32,)                |            |\n|  layer1                     |  0.132M                |  0.136G    |\n|   layer1.0                  |   57.728K              |   59.703M  |\n|    layer1.0.conv1           |    18.432K             |    18.874M |\n|    layer1.0.bn1             |    0.128K              |    0.328M  |\n|    layer1.0.conv2           |    36.864K             |    37.749M |\n|    layer1.0.bn2             |    0.128K              |    0.328M  |\n|    layer1.0.skip_connection |    2.176K              |    2.425M  |\n|   layer1.1                  |   73.984K              |   76.153M  |\n|    layer1.1.conv1           |    36.864K             |    37.749M |\n|    layer1.1.bn1             |    0.128K              |    0.328M  |\n|    layer1.1.conv2           |    36.864K             |    37.749M |\n|    layer1.1.bn2             |    0.128K              |    0.328M  |\n|  layer2                     |  0.526M                |  0.135G    |\n|   layer2.0                  |   0.23M                |   59.212M  |\n|    layer2.0.conv1           |    73.728K             |    18.874M |\n|    layer2.0.bn1             |    0.256K              |    0.164M  |\n|    layer2.0.conv2           |    0.147M              |    37.749M |\n|    layer2.0.bn2             |    0.256K              |    0.164M  |\n|    layer2.0.skip_connection |    8.448K              |    2.261M  |\n|   layer2.1                  |   0.295M               |   75.825M  |\n|    layer2.1.conv1           |    0.147M              |    37.749M |\n|    layer2.1.bn1             |    0.256K              |    0.164M  |\n|    layer2.1.conv2           |    0.147M              |    37.749M |\n|    layer2.1.bn2             |    0.256K              |    0.164M  |\n|  layer3                     |  2.1M                  |  0.135G    |\n|   layer3.0                  |   0.919M               |   58.966M  |\n|    layer3.0.conv1           |    0.295M              |    18.874M |\n|    layer3.0.bn1             |    0.512K              |    81.92K  |\n|    layer3.0.conv2           |    0.59M               |    37.749M |\n|    layer3.0.bn2             |    0.512K              |    81.92K  |\n|    layer3.0.skip_connection |    33.28K              |    2.179M  |\n|   layer3.1                  |   1.181M               |   75.661M  |\n|    layer3.1.conv1           |    0.59M               |    37.749M |\n|    layer3.1.bn1             |    0.512K              |    81.92K  |\n|    layer3.1.conv2           |    0.59M               |    37.749M |\n|    layer3.1.bn2             |    0.512K              |    81.92K  |\n|  fc                         |  2.57K                 |  2.56K     |\n|   fc.weight                 |   (10, 256)            |            |\n|   fc.bias                   |   (10,)                |            |\n|  avg_pool                   |                        |  16.384K   |\nTotal Parameters: 2760490\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"# Training loop\ndef train(epoch):\n    model.train()\n    running_loss = 0.0\n    for inputs, targets in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{50}\"):\n        inputs, targets = inputs.to(device), targets.to(device)\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs, targets)\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item()\n    scheduler.step()\n    print(f\"Training Loss: {running_loss / len(train_loader):.4f}\")\n\n# Test loop\ndef test():\n    model.eval()\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for inputs, targets in test_loader:\n            inputs, targets = inputs.to(device), targets.to(device)\n            outputs = model(inputs)\n            _, predicted = outputs.max(1)\n            total += targets.size(0)\n            correct += predicted.eq(targets).sum().item()\n    print(f\"Test Accuracy: {100. * correct / total:.2f}%\")\n\n# Train and save model\nfor epoch in range(1):\n    train(epoch)\n    test()\n\ntorch.save(model.state_dict(), \"just_test.pth\")\nprint(\"MODEL SAVED\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T06:55:50.895321Z","iopub.execute_input":"2024-12-02T06:55:50.896030Z","iopub.status.idle":"2024-12-02T06:56:27.373464Z","shell.execute_reply.started":"2024-12-02T06:55:50.895995Z","shell.execute_reply":"2024-12-02T06:56:27.372370Z"}},"outputs":[{"name":"stderr","text":"Epoch 1/50: 100%|██████████| 391/391 [00:34<00:00, 11.42it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 1.6605\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 49.42%\nMODEL SAVED\n","output_type":"stream"}],"execution_count":16},{"cell_type":"markdown","source":"### Update 93 accuracy model. reduce parameters","metadata":{}},{"cell_type":"code","source":"import os\nfrom tqdm import tqdm\nimport torch \nfrom torch.utils.data import DataLoader\nimport torchvision\nimport torchvision.transforms as transforms\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\nfrom fvcore.nn import FlopCountAnalysis, flop_count_table\nfrom torchinfo import summary\nfrom PIL import Image\nimport random\n\n# Define a basic convolutional block with GELU activation\nclass ConvBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, stride=1):\n        super(ConvBlock, self).__init__()\n        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(out_channels)\n        self.act = nn.GELU()\n        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(out_channels)\n        self.dropout = nn.Dropout(p=0.2)\n\n        self.skip_connection = nn.Sequential()\n        if stride != 1 or in_channels != out_channels:\n            self.skip_connection = nn.Sequential(\n                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(out_channels)\n            )\n\n    def forward(self, x):\n        identity = self.skip_connection(x)\n        out = self.conv1(x)\n        out = self.bn1(out)  # Batch Normalization\n        out = self.act(out)\n        out = self.conv2(out)\n        out = self.bn2(out)  # Batch Normalization\n        out = self.dropout(out)\n        out += identity\n        out = self.act(out)\n        return out\n\n# Define the base architecture for CIFAR-10\nclass BaseArchitecture(nn.Module):\n    def __init__(self, block, num_blocks, num_classes=10):\n        super(BaseArchitecture, self).__init__()\n        self.in_channels = 32\n\n        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(32)\n        self.act = nn.GELU()\n        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n        self.fc = nn.Linear(256, num_classes)\n\n    def _make_layer(self, block, out_channels, blocks, stride):\n        strides = [stride] + [1] * (blocks - 1)\n        layers = []\n        for stride in strides:\n            layers.append(block(self.in_channels, out_channels, stride))\n            self.in_channels = out_channels\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.act(out)\n        out = self.layer1(out)\n        out = self.layer2(out)\n        out = self.layer3(out)\n        out = self.avg_pool(out)\n        out = torch.flatten(out, 1)\n        out = self.fc(out)\n        return out\n\ndef get_base_architecture():\n    return BaseArchitecture(ConvBlock, [2, 2, 2])\n\n# Training configurations\ntransform_train = transforms.Compose([\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomCrop(32, padding=4),\n    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),  # Color jitter\n    transforms.RandomRotation(10),  # Random rotation\n    transforms.ToTensor(),\n    transforms.Normalize(mean=(0.4914, 0.4822, 0.4465), std=(0.2023, 0.1994, 0.2010)),\n])\n\ntransform_test = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize(mean=(0.4914, 0.4822, 0.4465), std=(0.2023, 0.1994, 0.2010)),\n])\n\ntrain_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\ntest_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n\ntrain_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=2)\ntest_loader = DataLoader(test_dataset, batch_size=100, shuffle=False, num_workers=2)\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nmodel = get_base_architecture().to(device)\n\n# Calculate FLOPs and Params\ndummy_input = torch.randn(1, 3, 32, 32).to(device)\nflops = FlopCountAnalysis(model, dummy_input)\nprint(\"FLOPs and Parameters BEFORE Training:\")\nprint(flop_count_table(flops))\nprint(f\"Total Parameters: {sum(p.numel() for p in model.parameters())}\")\n\n# Loss, optimizer, scheduler\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=3e-4)\nscheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=50)\n\n# Training loop\ndef train(epoch):\n    model.train()\n    running_loss = 0.0\n    for inputs, targets in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{50}\"):\n        inputs, targets = inputs.to(device), targets.to(device)\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs, targets)\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item()\n    scheduler.step()\n    print(f\"Training Loss: {running_loss / len(train_loader):.4f}\")\n\n# Test loop\ndef test():\n    model.eval()\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for inputs, targets in test_loader:\n            inputs, targets = inputs.to(device), targets.to(device)\n            outputs = model(inputs)\n            _, predicted = outputs.max(1)\n            total += targets.size(0)\n            correct += predicted.eq(targets).sum().item()\n    print(f\"Test Accuracy: {100. * correct / total:.2f}%\")\n\n# Train and save model\nfor epoch in range(65):\n    train(epoch)\n    test()\n\nprint(\"FLOPs and Parameters AFTER Training:\")\nprint(flop_count_table(flops))\nprint(f\"Total Parameters AFTER Training: {sum(p.numel() for p in model.parameters())}\")\n\ntorch.save(model.state_dict(), \"cifar_umode_traintransofom.pth\")\nprint(\"MODEL SAVED\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T08:00:52.291472Z","iopub.execute_input":"2024-12-03T08:00:52.292385Z","iopub.status.idle":"2024-12-03T08:36:00.737488Z","shell.execute_reply.started":"2024-12-03T08:00:52.292335Z","shell.execute_reply":"2024-12-03T08:36:00.736449Z"}},"outputs":[{"name":"stdout","text":"Files already downloaded and verified\nFiles already downloaded and verified\nFLOPs and Parameters BEFORE Training:\n| module                      | #parameters or shape   | #flops     |\n|:----------------------------|:-----------------------|:-----------|\n| model                       | 2.76M                  | 0.407G     |\n|  conv1                      |  0.864K                |  0.885M    |\n|   conv1.weight              |   (32, 3, 3, 3)        |            |\n|  bn1                        |  64                    |  0.164M    |\n|   bn1.weight                |   (32,)                |            |\n|   bn1.bias                  |   (32,)                |            |\n|  layer1                     |  0.132M                |  0.136G    |\n|   layer1.0                  |   57.728K              |   59.703M  |\n|    layer1.0.conv1           |    18.432K             |    18.874M |\n|    layer1.0.bn1             |    0.128K              |    0.328M  |\n|    layer1.0.conv2           |    36.864K             |    37.749M |\n|    layer1.0.bn2             |    0.128K              |    0.328M  |\n|    layer1.0.skip_connection |    2.176K              |    2.425M  |\n|   layer1.1                  |   73.984K              |   76.153M  |\n|    layer1.1.conv1           |    36.864K             |    37.749M |\n|    layer1.1.bn1             |    0.128K              |    0.328M  |\n|    layer1.1.conv2           |    36.864K             |    37.749M |\n|    layer1.1.bn2             |    0.128K              |    0.328M  |\n|  layer2                     |  0.526M                |  0.135G    |\n|   layer2.0                  |   0.23M                |   59.212M  |\n|    layer2.0.conv1           |    73.728K             |    18.874M |\n|    layer2.0.bn1             |    0.256K              |    0.164M  |\n|    layer2.0.conv2           |    0.147M              |    37.749M |\n|    layer2.0.bn2             |    0.256K              |    0.164M  |\n|    layer2.0.skip_connection |    8.448K              |    2.261M  |\n|   layer2.1                  |   0.295M               |   75.825M  |\n|    layer2.1.conv1           |    0.147M              |    37.749M |\n|    layer2.1.bn1             |    0.256K              |    0.164M  |\n|    layer2.1.conv2           |    0.147M              |    37.749M |\n|    layer2.1.bn2             |    0.256K              |    0.164M  |\n|  layer3                     |  2.1M                  |  0.135G    |\n|   layer3.0                  |   0.919M               |   58.966M  |\n|    layer3.0.conv1           |    0.295M              |    18.874M |\n|    layer3.0.bn1             |    0.512K              |    81.92K  |\n|    layer3.0.conv2           |    0.59M               |    37.749M |\n|    layer3.0.bn2             |    0.512K              |    81.92K  |\n|    layer3.0.skip_connection |    33.28K              |    2.179M  |\n|   layer3.1                  |   1.181M               |   75.661M  |\n|    layer3.1.conv1           |    0.59M               |    37.749M |\n|    layer3.1.bn1             |    0.512K              |    81.92K  |\n|    layer3.1.conv2           |    0.59M               |    37.749M |\n|    layer3.1.bn2             |    0.512K              |    81.92K  |\n|  fc                         |  2.57K                 |  2.56K     |\n|   fc.weight                 |   (10, 256)            |            |\n|   fc.bias                   |   (10,)                |            |\n|  avg_pool                   |                        |  16.384K   |\nTotal Parameters: 2760490\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/50: 100%|██████████| 391/391 [00:32<00:00, 12.01it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 1.6937\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 51.23%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/50: 100%|██████████| 391/391 [00:32<00:00, 11.93it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 1.2072\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 57.63%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/50: 100%|██████████| 391/391 [00:32<00:00, 11.99it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 1.0116\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 65.53%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/50: 100%|██████████| 391/391 [00:33<00:00, 11.83it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.8551\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 72.00%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/50: 100%|██████████| 391/391 [00:32<00:00, 11.96it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.7422\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 75.34%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6/50: 100%|██████████| 391/391 [00:32<00:00, 11.87it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.6785\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 75.19%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7/50: 100%|██████████| 391/391 [00:32<00:00, 11.93it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.6315\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 73.06%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8/50: 100%|██████████| 391/391 [00:32<00:00, 11.86it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.5960\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 82.04%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9/50: 100%|██████████| 391/391 [00:33<00:00, 11.77it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.5651\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 82.33%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10/50: 100%|██████████| 391/391 [00:33<00:00, 11.79it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.5391\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 81.36%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 11/50: 100%|██████████| 391/391 [00:32<00:00, 11.93it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.5131\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 83.79%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 12/50: 100%|██████████| 391/391 [00:32<00:00, 11.85it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.4950\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 83.72%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 13/50: 100%|██████████| 391/391 [00:32<00:00, 11.87it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.4747\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 84.44%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 14/50: 100%|██████████| 391/391 [00:32<00:00, 11.95it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.4577\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 84.83%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 15/50: 100%|██████████| 391/391 [00:33<00:00, 11.83it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.4417\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 86.41%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 16/50: 100%|██████████| 391/391 [00:32<00:00, 11.91it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.4291\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 86.24%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 17/50: 100%|██████████| 391/391 [00:32<00:00, 11.94it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.4152\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 83.89%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 18/50: 100%|██████████| 391/391 [00:33<00:00, 11.81it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.3963\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 87.33%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 19/50: 100%|██████████| 391/391 [00:33<00:00, 11.82it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.3873\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 85.94%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 20/50: 100%|██████████| 391/391 [00:32<00:00, 11.86it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.3721\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 87.23%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 21/50: 100%|██████████| 391/391 [00:32<00:00, 11.87it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.3682\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 85.55%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 22/50: 100%|██████████| 391/391 [00:32<00:00, 11.92it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.3505\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 88.08%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 23/50: 100%|██████████| 391/391 [00:33<00:00, 11.73it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.3382\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 87.61%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 24/50: 100%|██████████| 391/391 [00:32<00:00, 11.91it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.3252\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 89.19%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 25/50: 100%|██████████| 391/391 [00:33<00:00, 11.85it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.3154\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 89.21%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 26/50: 100%|██████████| 391/391 [00:32<00:00, 11.91it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.2998\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 89.67%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 27/50: 100%|██████████| 391/391 [00:33<00:00, 11.84it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.2864\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 89.07%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 28/50: 100%|██████████| 391/391 [00:32<00:00, 11.87it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.2754\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 90.70%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 29/50: 100%|██████████| 391/391 [00:33<00:00, 11.81it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.2661\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 90.52%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 30/50: 100%|██████████| 391/391 [00:32<00:00, 11.94it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.2508\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 91.00%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 31/50: 100%|██████████| 391/391 [00:32<00:00, 11.90it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.2351\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 90.71%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 32/50: 100%|██████████| 391/391 [00:33<00:00, 11.80it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.2245\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 91.17%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 33/50: 100%|██████████| 391/391 [00:32<00:00, 11.88it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.2115\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 90.70%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 34/50: 100%|██████████| 391/391 [00:32<00:00, 11.95it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.2012\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 91.62%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 35/50: 100%|██████████| 391/391 [00:33<00:00, 11.76it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.1837\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 91.08%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 36/50: 100%|██████████| 391/391 [00:32<00:00, 11.88it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.1732\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 91.21%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 37/50: 100%|██████████| 391/391 [00:32<00:00, 11.87it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.1594\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 92.02%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 38/50: 100%|██████████| 391/391 [00:32<00:00, 11.90it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.1485\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 92.01%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 39/50: 100%|██████████| 391/391 [00:33<00:00, 11.84it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.1354\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 92.27%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 40/50: 100%|██████████| 391/391 [00:33<00:00, 11.82it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.1216\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 92.48%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 41/50: 100%|██████████| 391/391 [00:33<00:00, 11.80it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.1113\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 92.96%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 42/50: 100%|██████████| 391/391 [00:32<00:00, 11.90it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.0969\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 92.92%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 43/50: 100%|██████████| 391/391 [00:32<00:00, 11.86it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.0938\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 93.32%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 44/50: 100%|██████████| 391/391 [00:32<00:00, 11.90it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.0844\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 93.30%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 45/50: 100%|██████████| 391/391 [00:33<00:00, 11.85it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.0784\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 93.46%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 46/50: 100%|██████████| 391/391 [00:33<00:00, 11.81it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.0724\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 93.40%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 47/50: 100%|██████████| 391/391 [00:33<00:00, 11.82it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.0663\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 93.48%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 48/50: 100%|██████████| 391/391 [00:33<00:00, 11.79it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.0663\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 93.59%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 49/50: 100%|██████████| 391/391 [00:32<00:00, 11.88it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.0655\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 93.66%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 50/50: 100%|██████████| 391/391 [00:33<00:00, 11.77it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.0628\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 93.57%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 51/50: 100%|██████████| 391/391 [00:33<00:00, 11.82it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.0640\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 93.62%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 52/50: 100%|██████████| 391/391 [00:32<00:00, 11.87it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.0646\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 93.59%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 53/50: 100%|██████████| 391/391 [00:33<00:00, 11.83it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.0634\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 93.77%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 54/50: 100%|██████████| 391/391 [00:33<00:00, 11.84it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.0641\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 93.56%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 55/50: 100%|██████████| 391/391 [00:32<00:00, 11.88it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.0649\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 93.49%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 56/50: 100%|██████████| 391/391 [00:32<00:00, 11.88it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.0664\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 93.40%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 57/50: 100%|██████████| 391/391 [00:32<00:00, 11.87it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.0701\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 93.48%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 58/50: 100%|██████████| 391/391 [00:33<00:00, 11.84it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.0719\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 93.22%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 59/50: 100%|██████████| 391/391 [00:32<00:00, 11.86it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.0771\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 92.90%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 60/50: 100%|██████████| 391/391 [00:32<00:00, 11.93it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.0885\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 92.76%\nFLOPs and Parameters AFTER Training:\n| module                      | #parameters or shape   | #flops     |\n|:----------------------------|:-----------------------|:-----------|\n| model                       | 2.76M                  | 0.407G     |\n|  conv1                      |  0.864K                |  0.885M    |\n|   conv1.weight              |   (32, 3, 3, 3)        |            |\n|  bn1                        |  64                    |  0.164M    |\n|   bn1.weight                |   (32,)                |            |\n|   bn1.bias                  |   (32,)                |            |\n|  layer1                     |  0.132M                |  0.136G    |\n|   layer1.0                  |   57.728K              |   59.703M  |\n|    layer1.0.conv1           |    18.432K             |    18.874M |\n|    layer1.0.bn1             |    0.128K              |    0.328M  |\n|    layer1.0.conv2           |    36.864K             |    37.749M |\n|    layer1.0.bn2             |    0.128K              |    0.328M  |\n|    layer1.0.skip_connection |    2.176K              |    2.425M  |\n|   layer1.1                  |   73.984K              |   76.153M  |\n|    layer1.1.conv1           |    36.864K             |    37.749M |\n|    layer1.1.bn1             |    0.128K              |    0.328M  |\n|    layer1.1.conv2           |    36.864K             |    37.749M |\n|    layer1.1.bn2             |    0.128K              |    0.328M  |\n|  layer2                     |  0.526M                |  0.135G    |\n|   layer2.0                  |   0.23M                |   59.212M  |\n|    layer2.0.conv1           |    73.728K             |    18.874M |\n|    layer2.0.bn1             |    0.256K              |    0.164M  |\n|    layer2.0.conv2           |    0.147M              |    37.749M |\n|    layer2.0.bn2             |    0.256K              |    0.164M  |\n|    layer2.0.skip_connection |    8.448K              |    2.261M  |\n|   layer2.1                  |   0.295M               |   75.825M  |\n|    layer2.1.conv1           |    0.147M              |    37.749M |\n|    layer2.1.bn1             |    0.256K              |    0.164M  |\n|    layer2.1.conv2           |    0.147M              |    37.749M |\n|    layer2.1.bn2             |    0.256K              |    0.164M  |\n|  layer3                     |  2.1M                  |  0.135G    |\n|   layer3.0                  |   0.919M               |   58.966M  |\n|    layer3.0.conv1           |    0.295M              |    18.874M |\n|    layer3.0.bn1             |    0.512K              |    81.92K  |\n|    layer3.0.conv2           |    0.59M               |    37.749M |\n|    layer3.0.bn2             |    0.512K              |    81.92K  |\n|    layer3.0.skip_connection |    33.28K              |    2.179M  |\n|   layer3.1                  |   1.181M               |   75.661M  |\n|    layer3.1.conv1           |    0.59M               |    37.749M |\n|    layer3.1.bn1             |    0.512K              |    81.92K  |\n|    layer3.1.conv2           |    0.59M               |    37.749M |\n|    layer3.1.bn2             |    0.512K              |    81.92K  |\n|  fc                         |  2.57K                 |  2.56K     |\n|   fc.weight                 |   (10, 256)            |            |\n|   fc.bias                   |   (10,)                |            |\n|  avg_pool                   |                        |  16.384K   |\nTotal Parameters AFTER Training: 2760490\nMODEL SAVED\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"## GPT\n\nimport os\nfrom tqdm import tqdm\nimport torch\nfrom torch.utils.data import DataLoader\nimport torchvision\nimport torchvision.transforms as transforms\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\nfrom fvcore.nn import FlopCountAnalysis, flop_count_table\nfrom torchinfo import summary\nfrom PIL import Image\nimport random\n\n# Define a basic convolutional block with GELU activation\nclass ConvBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, stride=1, reduction=16):\n        super(ConvBlock, self).__init__()\n        # Depthwise convolution\n        self.dw_conv = nn.Conv2d(in_channels, in_channels, kernel_size=3, stride=stride, padding=1, groups=in_channels, bias=False)\n        self.bn1 = nn.BatchNorm2d(in_channels)\n        # Pointwise convolution\n        self.pw_conv = nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(out_channels)\n        self.act = nn.GELU()\n        \n        # Squeeze-and-Excitation block\n        self.se = nn.Sequential(\n            nn.AdaptiveAvgPool2d(1),\n            nn.Conv2d(out_channels, out_channels // reduction, kernel_size=1),\n            nn.GELU(),\n            nn.Conv2d(out_channels // reduction, out_channels, kernel_size=1),\n            nn.Sigmoid()\n        )\n\n        # Residual connection\n        self.skip_connection = nn.Sequential()\n        if stride != 1 or in_channels != out_channels:\n            self.skip_connection = nn.Sequential(\n                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(out_channels)\n            )\n\n    def forward(self, x):\n        identity = self.skip_connection(x)\n        out = self.dw_conv(x)\n        out = self.bn1(out)\n        out = self.act(out)\n        out = self.pw_conv(out)\n        out = self.bn2(out)\n        out = self.act(out)\n        \n        # Apply SE Attention\n        se_weight = self.se(out)\n        out = out * se_weight\n        \n        out += identity  # Residual connection\n        out = self.act(out)\n        return out\n\n# Define the updated BaseArchitecture with increased depth\nclass BaseArchitecture(nn.Module):\n    def __init__(self, block, num_blocks, num_classes=10):\n        super(BaseArchitecture, self).__init__()\n        self.in_channels = 32\n\n        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(32)\n        self.act = nn.GELU()\n        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n        self.fc = nn.Linear(256, num_classes)\n\n    def _make_layer(self, block, out_channels, blocks, stride):\n        strides = [stride] + [1] * (blocks - 1)\n        layers = []\n        for stride in strides:\n            layers.append(block(self.in_channels, out_channels, stride))\n            self.in_channels = out_channels\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.act(out)\n        out = self.layer1(out)\n        out = self.layer2(out)\n        out = self.layer3(out)\n        out = self.avg_pool(out)\n        out = torch.flatten(out, 1)\n        out = self.fc(out)\n        return out\n\ndef get_updated_architecture():\n    return BaseArchitecture(ConvBlock, [3, 3, 3])  # Increased depth\n\n# Training configurations\n# transform_train = transforms.Compose([\n#     transforms.RandomHorizontalFlip(),\n#     transforms.RandomCrop(32, padding=4),\n#     transforms.ToTensor(),\n#     transforms.Normalize(mean=(0.4914, 0.4822, 0.4465), std=(0.2023, 0.1994, 0.2010)),\n# ])\n\ntransform_train = transforms.Compose([\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomCrop(32, padding=4),\n    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),  # Color jitter\n    transforms.RandomRotation(10),  # Random rotation\n    transforms.ToTensor(),\n    transforms.Normalize(mean=(0.4914, 0.4822, 0.4465), std=(0.2023, 0.1994, 0.2010)),\n])\n\ntransform_test = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize(mean=(0.4914, 0.4822, 0.4465), std=(0.2023, 0.1994, 0.2010)),\n])\n\ntrain_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\ntest_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n\ntrain_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=2)\ntest_loader = DataLoader(test_dataset, batch_size=100, shuffle=False, num_workers=2)\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nmodel = get_updated_architecture().to(device)\n\n\n# Calculate FLOPs and Params\ndummy_input = torch.randn(1, 3, 32, 32).to(device)\nflops = FlopCountAnalysis(model, dummy_input)\nprint(\"FLOPs and Parameters AFTER Enhancements:\")\nprint(flop_count_table(flops))\nprint(f\"Total Parameters: {sum(p.numel() for p in model.parameters())}\")\n\n# Loss, optimizer, scheduler\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=3e-4)\nscheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=50)\n\n# Training loop\ndef train(epoch):\n    model.train()\n    running_loss = 0.0\n    for inputs, targets in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{50}\"):\n        inputs, targets = inputs.to(device), targets.to(device)\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs, targets)\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item()\n    scheduler.step()\n    print(f\"Training Loss: {running_loss / len(train_loader):.4f}\")\n\n# Test loop\ndef test():\n    model.eval()\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for inputs, targets in test_loader:\n            inputs, targets = inputs.to(device), targets.to(device)\n            outputs = model(inputs)\n            _, predicted = outputs.max(1)\n            total += targets.size(0)\n            correct += predicted.eq(targets).sum().item()\n    print(f\"Test Accuracy: {100. * correct / total:.2f}%\")\n\n# Train and save model\nfor epoch in range(70):\n    train(epoch)\n    test()\n\nprint(\"FLOPs and Parameters AFTER Training:\")\nprint(flop_count_table(flops))\nprint(f\"Total Parameters AFTER Training: {sum(p.numel() for p in model.parameters())}\")\n\ntorch.save(model.state_dict(), \"cifar10_gpt_train.pth\")\nprint(\"MODEL SAVED\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T08:47:10.514031Z","iopub.execute_input":"2024-12-03T08:47:10.514413Z","iopub.status.idle":"2024-12-03T09:22:50.556682Z","shell.execute_reply.started":"2024-12-03T08:47:10.514374Z","shell.execute_reply":"2024-12-03T09:22:50.555433Z"}},"outputs":[{"name":"stdout","text":"Files already downloaded and verified\nFiles already downloaded and verified\nFLOPs and Parameters AFTER Enhancements:\n| module                      | #parameters or shape   | #flops     |\n|:----------------------------|:-----------------------|:-----------|\n| model                       | 0.311M                 | 45.5M      |\n|  conv1                      |  0.864K                |  0.885M    |\n|   conv1.weight              |   (32, 3, 3, 3)        |            |\n|  bn1                        |  64                    |  0.164M    |\n|   bn1.weight                |   (32,)                |            |\n|   bn1.bias                  |   (32,)                |            |\n|  layer1                     |  16.3K                 |  16.386M   |\n|   layer1.0                  |   5.284K               |   5.374M   |\n|    layer1.0.dw_conv         |    0.288K              |    0.295M  |\n|    layer1.0.bn1             |    64                  |    0.164M  |\n|    layer1.0.pw_conv         |    2.048K              |    2.097M  |\n|    layer1.0.bn2             |    0.128K              |    0.328M  |\n|    layer1.0.se              |    0.58K               |    66.048K |\n|    layer1.0.skip_connection |    2.176K              |    2.425M  |\n|   layer1.1                  |   5.508K               |   5.506M   |\n|    layer1.1.dw_conv         |    0.576K              |    0.59M   |\n|    layer1.1.bn1             |    0.128K              |    0.328M  |\n|    layer1.1.pw_conv         |    4.096K              |    4.194M  |\n|    layer1.1.bn2             |    0.128K              |    0.328M  |\n|    layer1.1.se              |    0.58K               |    66.048K |\n|   layer1.2                  |   5.508K               |   5.506M   |\n|    layer1.2.dw_conv         |    0.576K              |    0.59M   |\n|    layer1.2.bn1             |    0.128K              |    0.328M  |\n|    layer1.2.pw_conv         |    4.096K              |    4.194M  |\n|    layer1.2.bn2             |    0.128K              |    0.328M  |\n|    layer1.2.se              |    0.58K               |    66.048K |\n|  layer2                     |  60.248K               |  14.49M    |\n|   layer2.0                  |   19.784K              |   4.786M   |\n|    layer2.0.dw_conv         |    0.576K              |    0.147M  |\n|    layer2.0.bn1             |    0.128K              |    81.92K  |\n|    layer2.0.pw_conv         |    8.192K              |    2.097M  |\n|    layer2.0.bn2             |    0.256K              |    0.164M  |\n|    layer2.0.se              |    2.184K              |    34.816K |\n|    layer2.0.skip_connection |    8.448K              |    2.261M  |\n|   layer2.1                  |   20.232K              |   4.852M   |\n|    layer2.1.dw_conv         |    1.152K              |    0.295M  |\n|    layer2.1.bn1             |    0.256K              |    0.164M  |\n|    layer2.1.pw_conv         |    16.384K             |    4.194M  |\n|    layer2.1.bn2             |    0.256K              |    0.164M  |\n|    layer2.1.se              |    2.184K              |    34.816K |\n|   layer2.2                  |   20.232K              |   4.852M   |\n|    layer2.2.dw_conv         |    1.152K              |    0.295M  |\n|    layer2.2.bn1             |    0.256K              |    0.164M  |\n|    layer2.2.pw_conv         |    16.384K             |    4.194M  |\n|    layer2.2.bn2             |    0.256K              |    0.164M  |\n|    layer2.2.se              |    2.184K              |    34.816K |\n|  layer3                     |  0.231M                |  13.558M   |\n|   layer3.0                  |   76.432K              |   4.497M   |\n|    layer3.0.dw_conv         |    1.152K              |    73.728K |\n|    layer3.0.bn1             |    0.256K              |    40.96K  |\n|    layer3.0.pw_conv         |    32.768K             |    2.097M  |\n|    layer3.0.bn2             |    0.512K              |    81.92K  |\n|    layer3.0.se              |    8.464K              |    24.576K |\n|    layer3.0.skip_connection |    33.28K              |    2.179M  |\n|   layer3.1                  |   77.328K              |   4.53M    |\n|    layer3.1.dw_conv         |    2.304K              |    0.147M  |\n|    layer3.1.bn1             |    0.512K              |    81.92K  |\n|    layer3.1.pw_conv         |    65.536K             |    4.194M  |\n|    layer3.1.bn2             |    0.512K              |    81.92K  |\n|    layer3.1.se              |    8.464K              |    24.576K |\n|   layer3.2                  |   77.328K              |   4.53M    |\n|    layer3.2.dw_conv         |    2.304K              |    0.147M  |\n|    layer3.2.bn1             |    0.512K              |    81.92K  |\n|    layer3.2.pw_conv         |    65.536K             |    4.194M  |\n|    layer3.2.bn2             |    0.512K              |    81.92K  |\n|    layer3.2.se              |    8.464K              |    24.576K |\n|  fc                         |  2.57K                 |  2.56K     |\n|   fc.weight                 |   (10, 256)            |            |\n|   fc.bias                   |   (10,)                |            |\n|  avg_pool                   |                        |  16.384K   |\nTotal Parameters: 311134\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/50: 100%|██████████| 391/391 [00:29<00:00, 13.22it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 1.5472\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 57.63%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/50: 100%|██████████| 391/391 [00:29<00:00, 13.21it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 1.0450\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 69.49%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/50: 100%|██████████| 391/391 [00:30<00:00, 13.01it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.8457\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 72.26%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/50: 100%|██████████| 391/391 [00:29<00:00, 13.24it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.7397\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 76.11%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/50: 100%|██████████| 391/391 [00:29<00:00, 13.05it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.6739\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 78.47%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6/50: 100%|██████████| 391/391 [00:30<00:00, 13.02it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.6326\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 79.67%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7/50: 100%|██████████| 391/391 [00:29<00:00, 13.16it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.5993\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 80.97%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8/50: 100%|██████████| 391/391 [00:29<00:00, 13.18it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.5713\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 78.76%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9/50: 100%|██████████| 391/391 [00:29<00:00, 13.10it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.5466\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 82.32%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10/50: 100%|██████████| 391/391 [00:29<00:00, 13.15it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.5270\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 77.41%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 11/50: 100%|██████████| 391/391 [00:29<00:00, 13.26it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.5145\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 82.33%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 12/50: 100%|██████████| 391/391 [00:29<00:00, 13.27it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.4948\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 83.28%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 13/50: 100%|██████████| 391/391 [00:29<00:00, 13.36it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.4849\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 83.53%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 14/50: 100%|██████████| 391/391 [00:29<00:00, 13.45it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.4731\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 84.09%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 15/50: 100%|██████████| 391/391 [00:29<00:00, 13.14it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.4551\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 83.80%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 16/50: 100%|██████████| 391/391 [00:29<00:00, 13.24it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.4477\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 83.26%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 17/50: 100%|██████████| 391/391 [00:30<00:00, 13.00it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.4389\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 84.08%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 18/50: 100%|██████████| 391/391 [00:29<00:00, 13.29it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.4249\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 83.74%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 19/50: 100%|██████████| 391/391 [00:30<00:00, 12.97it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.4111\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 85.39%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 20/50: 100%|██████████| 391/391 [00:30<00:00, 12.98it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.4061\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 86.29%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 21/50: 100%|██████████| 391/391 [00:29<00:00, 13.23it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.3908\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 86.35%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 22/50: 100%|██████████| 391/391 [00:29<00:00, 13.16it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.3848\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 87.89%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 23/50: 100%|██████████| 391/391 [00:29<00:00, 13.29it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.3779\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 85.91%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 24/50: 100%|██████████| 391/391 [00:29<00:00, 13.27it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.3655\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 87.10%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 25/50: 100%|██████████| 391/391 [00:29<00:00, 13.24it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.3509\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 86.26%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 26/50: 100%|██████████| 391/391 [00:29<00:00, 13.24it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.3450\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 86.18%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 27/50: 100%|██████████| 391/391 [00:29<00:00, 13.20it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.3317\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 87.90%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 28/50: 100%|██████████| 391/391 [00:29<00:00, 13.12it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.3235\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 86.30%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 29/50: 100%|██████████| 391/391 [00:30<00:00, 13.00it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.3090\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 87.34%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 30/50: 100%|██████████| 391/391 [00:30<00:00, 13.01it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.2973\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 89.12%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 31/50: 100%|██████████| 391/391 [00:30<00:00, 12.86it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.2860\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 89.06%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 32/50: 100%|██████████| 391/391 [00:30<00:00, 12.93it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.2775\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 89.28%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 33/50: 100%|██████████| 391/391 [00:29<00:00, 13.07it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.2639\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 89.08%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 34/50: 100%|██████████| 391/391 [00:30<00:00, 12.81it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.2514\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 89.71%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 35/50: 100%|██████████| 391/391 [00:30<00:00, 12.76it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.2423\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 89.37%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 36/50: 100%|██████████| 391/391 [00:30<00:00, 12.89it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.2282\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 90.47%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 37/50: 100%|██████████| 391/391 [00:30<00:00, 12.94it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.2165\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 90.88%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 38/50: 100%|██████████| 391/391 [00:30<00:00, 12.97it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.2034\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 91.16%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 39/50: 100%|██████████| 391/391 [00:30<00:00, 12.90it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.1930\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 90.77%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 40/50: 100%|██████████| 391/391 [00:30<00:00, 13.00it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.1784\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 90.89%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 41/50: 100%|██████████| 391/391 [00:29<00:00, 13.04it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.1722\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 91.53%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 42/50: 100%|██████████| 391/391 [00:30<00:00, 13.02it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.1591\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 91.63%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 43/50: 100%|██████████| 391/391 [00:30<00:00, 12.64it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.1476\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 91.94%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 44/50: 100%|██████████| 391/391 [00:30<00:00, 12.78it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.1403\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 92.10%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 45/50: 100%|██████████| 391/391 [00:31<00:00, 12.41it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.1294\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 91.95%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 46/50: 100%|██████████| 391/391 [00:30<00:00, 12.81it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.1254\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 91.93%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 47/50: 100%|██████████| 391/391 [00:30<00:00, 12.88it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.1211\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 92.22%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 48/50: 100%|██████████| 391/391 [00:30<00:00, 12.75it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.1171\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 92.12%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 49/50: 100%|██████████| 391/391 [00:29<00:00, 13.07it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.1140\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 92.17%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 50/50: 100%|██████████| 391/391 [00:30<00:00, 13.02it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.1123\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 92.28%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 51/50: 100%|██████████| 391/391 [00:29<00:00, 13.09it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.1134\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 92.29%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 52/50: 100%|██████████| 391/391 [00:29<00:00, 13.06it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.1131\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 92.30%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 53/50: 100%|██████████| 391/391 [00:30<00:00, 13.01it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.1125\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 92.23%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 54/50: 100%|██████████| 391/391 [00:30<00:00, 13.01it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.1152\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 92.08%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 55/50: 100%|██████████| 391/391 [00:30<00:00, 13.03it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.1155\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 92.20%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 56/50: 100%|██████████| 391/391 [00:30<00:00, 12.87it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.1158\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 91.95%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 57/50: 100%|██████████| 391/391 [00:30<00:00, 12.96it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.1203\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 92.10%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 58/50: 100%|██████████| 391/391 [00:30<00:00, 12.87it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.1258\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 91.97%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 59/50: 100%|██████████| 391/391 [00:29<00:00, 13.06it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.1299\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 91.74%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 60/50: 100%|██████████| 391/391 [00:30<00:00, 12.66it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.1440\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 91.55%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 61/50: 100%|██████████| 391/391 [00:30<00:00, 12.69it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.1552\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 90.81%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 62/50: 100%|██████████| 391/391 [00:30<00:00, 12.94it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.1691\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 90.56%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 63/50: 100%|██████████| 391/391 [00:29<00:00, 13.18it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.1900\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 90.14%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 64/50: 100%|██████████| 391/391 [00:29<00:00, 13.15it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.2045\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 90.67%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 65/50: 100%|██████████| 391/391 [00:30<00:00, 13.03it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.2232\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 89.46%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 66/50: 100%|██████████| 391/391 [00:29<00:00, 13.17it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.2327\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 88.52%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 67/50:  91%|█████████ | 354/391 [00:27<00:02, 12.82it/s]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[16], line 177\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;66;03m# Train and save model\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m70\u001b[39m):\n\u001b[0;32m--> 177\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    178\u001b[0m     test()\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFLOPs and Parameters AFTER Training:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","Cell \u001b[0;32mIn[16], line 151\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m    149\u001b[0m running_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m inputs, targets \u001b[38;5;129;01min\u001b[39;00m tqdm(train_loader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;241m50\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 151\u001b[0m     inputs, targets \u001b[38;5;241m=\u001b[39m \u001b[43minputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m, targets\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    152\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m    153\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(inputs)\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":16},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## DLA","metadata":{}},{"cell_type":"code","source":"import math\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Bottleneck(nn.Module):\n    def __init__(self, in_planes, growth_rate):\n        super(Bottleneck, self).__init__()\n        self.bn1 = nn.BatchNorm2d(in_planes)\n        self.conv1 = nn.Conv2d(in_planes, 4*growth_rate, kernel_size=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(4*growth_rate)\n        self.conv2 = nn.Conv2d(4*growth_rate, growth_rate, kernel_size=3, padding=1, bias=False)\n\n    def forward(self, x):\n        out = self.conv1(F.relu(self.bn1(x)))\n        out = self.conv2(F.relu(self.bn2(out)))\n        out = torch.cat([out,x], 1)\n        return out\n\n\nclass Transition(nn.Module):\n    def __init__(self, in_planes, out_planes):\n        super(Transition, self).__init__()\n        self.bn = nn.BatchNorm2d(in_planes)\n        self.conv = nn.Conv2d(in_planes, out_planes, kernel_size=1, bias=False)\n\n    def forward(self, x):\n        out = self.conv(F.relu(self.bn(x)))\n        out = F.avg_pool2d(out, 2)\n        return out\n\nclass LightConvBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, stride=1):\n        super(LightConvBlock, self).__init__()\n        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(out_channels)\n        self.act = nn.GELU()\n        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(out_channels)\n        self.dropout = nn.Dropout(p=0.2)\n\n        self.skip_connection = nn.Sequential()\n        if stride != 1 or in_channels != out_channels:\n            self.skip_connection = nn.Sequential(\n                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(out_channels)\n            )\n\n    def forward(self, x):\n        identity = self.skip_connection(x)\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.act(out)\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.dropout(out)\n        out += identity\n        out = self.act(out)\n        return out\n\nclass DenseNet(nn.Module):\n    def __init__(self, block, nblocks, growth_rate=12, reduction=0.5, num_classes=10):\n        super(DenseNet, self).__init__()\n        self.growth_rate = growth_rate\n        self.in_channels = 16  # Reduced initial channels\n\n        self.conv1 = nn.Conv2d(3, self.in_channels, kernel_size=3, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(self.in_channels)\n        self.act = nn.ReLU()\n\n\n        # Crucial fix: correct input channels for dense1!\n        self.dense1 = self._make_dense_layers(block, self.in_channels, nblocks[0])\n        self.in_channels += nblocks[0] * self.growth_rate\n\n\n        self.trans1 = Transition(self.in_channels, int(self.in_channels * reduction))\n        self.in_channels = int(self.in_channels * reduction)\n\n\n        self.dense2 = self._make_dense_layers(block, self.in_channels, nblocks[1])  # Corrected number of blocks\n        self.in_channels += nblocks[1] * self.growth_rate\n        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n        self.fc = nn.Linear(self.in_channels, num_classes)\n\n\n    def _make_dense_layers(self, block, in_planes, nblock):\n        layers = []\n        for i in range(nblock):\n            layers.append(block(in_planes, self.growth_rate))\n            in_planes += self.growth_rate\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.act(out)\n        out = self.dense1(out)\n        out = self.trans1(out)\n        out = self.dense2(out)\n        out = self.avg_pool(self.act(out))\n        out = out.view(out.size(0), -1)\n        out = self.fc(out)\n        return out\n\nclass ExtremelyLightDenseNet(nn.Module):\n    def __init__(self, block, nblocks, growth_rate=16, reduction=0.5, num_classes=10):\n        super(ExtremelyLightDenseNet, self).__init__()\n        self.growth_rate = growth_rate\n        self.in_channels = 16 # Use 16 for initial channels\n\n        self.conv1 = nn.Conv2d(3, self.in_channels, kernel_size=3, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(self.in_channels)\n        self.act = nn.ReLU()\n\n        # Crucial fix: Correct input channels for dense1!\n        self.dense1 = self._make_dense_layers(block, self.in_channels, nblocks[0])\n        self.in_channels += nblocks[0] * self.growth_rate\n\n\n        self.trans1 = Transition(self.in_channels, int(self.in_channels * reduction))\n        self.in_channels = int(self.in_channels * reduction)\n\n        self.dense2 = self._make_dense_layers(block, self.in_channels, nblocks[1])\n        self.in_channels += nblocks[1] * self.growth_rate\n        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n        self.fc = nn.Linear(self.in_channels, num_classes)\n\n\n    def _make_dense_layers(self, block, in_planes, nblock):\n        layers = []\n        for i in range(nblock):\n            layers.append(block(in_planes, self.growth_rate))\n            in_planes += self.growth_rate\n        return nn.Sequential(*layers)\n\n    \n    def forward(self, x):\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.act(out)\n        out = self.dense1(out)\n        out = self.trans1(out)\n        out = self.dense2(out)\n        out = self.avg_pool(self.act(out))\n        out = out.view(out.size(0), -1)\n        out = self.fc(out)\n        return out\n\n\ndef DenseNet121():\n    return DenseNet(Bottleneck, [6,12,24,16], growth_rate=32)\n\ndef DenseNet169():\n    return DenseNet(Bottleneck, [6,12,32,32], growth_rate=32)\n\ndef DenseNet201():\n    return DenseNet(Bottleneck, [6,12,48,32], growth_rate=32)\n\ndef DenseNet161():\n    return DenseNet(Bottleneck, [6,12,36,24], growth_rate=48)\n\ndef densenet_cifar():\n    return DenseNet(Bottleneck, [6,12,24,16], growth_rate=12)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T06:20:31.687121Z","iopub.execute_input":"2024-12-04T06:20:31.687654Z","iopub.status.idle":"2024-12-04T06:20:31.718612Z","shell.execute_reply.started":"2024-12-04T06:20:31.687602Z","shell.execute_reply":"2024-12-04T06:20:31.717671Z"}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, in_planes, planes, stride=1):\n        super(BasicBlock, self).__init__()\n        self.conv1 = nn.Conv2d(\n            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n                               stride=1, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n\n        self.shortcut = nn.Sequential()\n        if stride != 1 or in_planes != self.expansion*planes:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_planes, self.expansion*planes,\n                          kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(self.expansion*planes)\n            )\n\n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = self.bn2(self.conv2(out))\n        out += self.shortcut(x)\n        out = F.relu(out)\n        return out\n\n\nclass Root(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size=1):\n        super(Root, self).__init__()\n        self.conv = nn.Conv2d(\n            in_channels, out_channels, kernel_size,\n            stride=1, padding=(kernel_size - 1) // 2, bias=False)\n        self.bn = nn.BatchNorm2d(out_channels)\n\n    def forward(self, xs):\n        x = torch.cat(xs, 1)\n        out = F.relu(self.bn(self.conv(x)))\n        return out\n\n\nclass Tree(nn.Module):\n    def __init__(self, block, in_channels, out_channels, level=1, stride=1):\n        super(Tree, self).__init__()\n        self.root = Root(2*out_channels, out_channels)\n        if level == 1:\n            self.left_tree = block(in_channels, out_channels, stride=stride)\n            self.right_tree = block(out_channels, out_channels, stride=1)\n        else:\n            self.left_tree = Tree(block, in_channels,\n                                  out_channels, level=level-1, stride=stride)\n            self.right_tree = Tree(block, out_channels,\n                                   out_channels, level=level-1, stride=1)\n\n    def forward(self, x):\n        out1 = self.left_tree(x)\n        out2 = self.right_tree(out1)\n        out = self.root([out1, out2])\n        return out\n\n\nclass SimpleDLA(nn.Module):\n    def __init__(self, block=BasicBlock, num_classes=10):\n        super(SimpleDLA, self).__init__()\n        self.base = nn.Sequential(\n            nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False),\n            nn.BatchNorm2d(16),\n            nn.ReLU(True)\n        )\n\n        self.layer1 = nn.Sequential(  # No need to repeat these\n            nn.Conv2d(16, 16, kernel_size=3, stride=1, padding=1, bias=False),\n            nn.BatchNorm2d(16),\n            nn.ReLU(True)\n        )\n\n        self.layer2 = nn.Sequential(\n            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1, bias=False),\n            nn.BatchNorm2d(32),\n            nn.ReLU(True)\n        )\n\n\n        # CRITICAL: Correct the Tree usage and define a working _forward_\n        self.layer3 = Tree(block, 32, 32, level=1, stride=1)\n        self.layer4 = Tree(block, 32, 64, level=2, stride=2)\n        self.layer5 = Tree(block, 64, 128, level=2, stride=2)\n        self.layer6 = Tree(block, 128, 256, level=1, stride=1)\n        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n        self.fc = nn.Linear(256, num_classes)\n\n    def forward(self, x):\n        x = self.base(x)\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        x = self.layer5(x)\n        x = self.layer6(x)\n        x = self.avg_pool(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-05T00:36:34.314000Z","iopub.execute_input":"2024-12-05T00:36:34.314598Z","iopub.status.idle":"2024-12-05T00:36:34.331485Z","shell.execute_reply.started":"2024-12-05T00:36:34.314564Z","shell.execute_reply":"2024-12-05T00:36:34.330413Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"import os\nimport sys\nimport time\nimport math\nimport shutil\nimport torch.nn as nn\nimport torch.nn.init as init\n\ndef get_mean_and_std(dataset):\n    '''Compute the mean and std value of dataset.'''\n    dataloader = torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=True, num_workers=2)\n    mean = torch.zeros(3)\n    std = torch.zeros(3)\n    print('==> Computing mean and std..')\n    for inputs, targets in dataloader:\n        for i in range(3):\n            mean[i] += inputs[:,i,:,:].mean()\n            std[i] += inputs[:,i,:,:].std()\n    mean.div_(len(dataset))\n    std.div_(len(dataset))\n    return mean, std\n\ndef init_params(net):\n    '''Init layer parameters.'''\n    for m in net.modules():\n        if isinstance(m, nn.Conv2d):\n            init.kaiming_normal(m.weight, mode='fan_out')\n            if m.bias is not None:\n                init.constant(m.bias, 0)\n        elif isinstance(m, nn.BatchNorm2d):\n            init.constant(m.weight, 1)\n            init.constant(m.bias, 0)\n        elif isinstance(m, nn.Linear):\n            init.normal(m.weight, std=1e-3)\n            if m.bias is not None:\n                init.constant(m.bias, 0)\n\n# Get terminal width using shutil\nterm_width, _ = shutil.get_terminal_size()\nTOTAL_BAR_LENGTH = 65.\nlast_time = time.time()\nbegin_time = last_time\n\ndef progress_bar(current, total, msg=None):\n    global last_time, begin_time\n    if current == 0:\n        begin_time = time.time()  # Reset for new bar.\n\n    cur_len = int(TOTAL_BAR_LENGTH*current/total)\n    rest_len = int(TOTAL_BAR_LENGTH - cur_len) - 1\n\n    sys.stdout.write(' [')\n    for i in range(cur_len):\n        sys.stdout.write('=')\n    sys.stdout.write('>')\n    for i in range(rest_len):\n        sys.stdout.write('.')\n    sys.stdout.write(']')\n\n    cur_time = time.time()\n    step_time = cur_time - last_time\n    last_time = cur_time\n    tot_time = cur_time - begin_time\n\n    L = []\n    L.append('  Step: %s' % format_time(step_time))\n    L.append(' | Tot: %s' % format_time(tot_time))\n    if msg:\n        L.append(' | ' + msg)\n\n    msg = ''.join(L)\n    sys.stdout.write(msg)\n    for i in range(term_width-int(TOTAL_BAR_LENGTH)-len(msg)-3):\n        sys.stdout.write(' ')\n\n    # Go back to the center of the bar.\n    for i in range(term_width-int(TOTAL_BAR_LENGTH/2)+2):\n        sys.stdout.write('\\b')\n    sys.stdout.write(' %d/%d ' % (current+1, total))\n\n    if current < total-1:\n        sys.stdout.write('\\r')\n    else:\n        sys.stdout.write('\\n')\n    sys.stdout.flush()\n\ndef format_time(seconds):\n    days = int(seconds / 3600/24)\n    seconds = seconds - days*3600*24\n    hours = int(seconds / 3600)\n    seconds = seconds - hours*3600\n    minutes = int(seconds / 60)\n    seconds = seconds - minutes*60\n    secondsf = int(seconds)\n    seconds = seconds - secondsf\n    millis = int(seconds*1000)\n\n    f = ''\n    i = 1\n    if days > 0:\n        f += str(days) + 'D'\n        i += 1\n    if hours > 0 and i <= 2:\n        f += str(hours) + 'h'\n        i += 1\n    if minutes > 0 and i <= 2:\n        f += str(minutes) + 'm'\n        i += 1\n    if secondsf > 0 and i <= 2:\n        f += str(secondsf) + 's'\n        i += 1\n    if millis > 0 and i <= 2:\n        f += str(millis) + 'ms'\n        i += 1\n    if f == '':\n        f = '0ms'\n    return f","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-05T00:36:35.293044Z","iopub.execute_input":"2024-12-05T00:36:35.293390Z","iopub.status.idle":"2024-12-05T00:36:35.309424Z","shell.execute_reply.started":"2024-12-05T00:36:35.293349Z","shell.execute_reply":"2024-12-05T00:36:35.308350Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport torch.backends.cudnn as cudnn\n\nimport torchvision\nimport torchvision.transforms as transforms\n\nimport os\nimport argparse\n\n# from models import *\n# from utils import progress_bar\n\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nbest_acc = 0  # best test accuracy\nstart_epoch = 0  # start from epoch 0 or last checkpoint epoch\n\n# Data\nprint('==> Preparing data..')\ntransform_train = transforms.Compose([\n    transforms.RandomCrop(32, padding=4),\n    transforms.RandomHorizontalFlip(),\n    transforms.ToTensor(),\n    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n])\n\ntransform_test = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n])\n\ntrainset = torchvision.datasets.CIFAR10(\n    root='./data', train=True, download=True, transform=transform_train)\ntrainloader = torch.utils.data.DataLoader(\n    trainset, batch_size=128, shuffle=True, num_workers=2)\n\ntestset = torchvision.datasets.CIFAR10(\n    root='./data', train=False, download=True, transform=transform_test)\ntestloader = torch.utils.data.DataLoader(\n    testset, batch_size=100, shuffle=False, num_workers=2)\n\nclasses = ('plane', 'car', 'bird', 'cat', 'deer',\n           'dog', 'frog', 'horse', 'ship', 'truck')\n\n# Model\nprint('==> Building model..')\nnet = SimpleDLA()\nprint(f\"Total Parameters: {sum(p.numel() for p in net.parameters())}\")\n# net = DenseNet121()\n# net = ExtremelyLightDenseNet(LightConvBlock, [2, 4]).to(device)\nnet = net.to(device)\nif device == 'cuda':\n    net = torch.nn.DataParallel(net)\n    cudnn.benchmark = True\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(net.parameters(), lr=0.01,\n                      momentum=0.9, weight_decay=5e-4)\nscheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)\n\n\n# Training\ndef train(epoch):\n    print('\\nEpoch: %d' % epoch)\n    net.train()\n    train_loss = 0\n    correct = 0\n    total = 0\n    for batch_idx, (inputs, targets) in enumerate(trainloader):\n        inputs, targets = inputs.to(device), targets.to(device)\n        optimizer.zero_grad()\n        outputs = net(inputs)\n        loss = criterion(outputs, targets)\n        loss.backward()\n        optimizer.step()\n\n        train_loss += loss.item()\n        _, predicted = outputs.max(1)\n        total += targets.size(0)\n        correct += predicted.eq(targets).sum().item()\n\n        progress_bar(batch_idx, len(trainloader), 'Loss: %.3f |Train Acc: %.3f%% (%d/%d)'\n                     % (train_loss/(batch_idx+1), 100.*correct/total, correct, total))\n\n\ndef test(epoch):\n    global best_acc\n    net.eval()\n    test_loss = 0\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for batch_idx, (inputs, targets) in enumerate(testloader):\n            inputs, targets = inputs.to(device), targets.to(device)\n            outputs = net(inputs)\n            loss = criterion(outputs, targets)\n\n            test_loss += loss.item()\n            _, predicted = outputs.max(1)\n            total += targets.size(0)\n            correct += predicted.eq(targets).sum().item()\n\n            progress_bar(batch_idx, len(testloader), 'Loss: %.3f |Test Acc: %.3f%% (%d/%d)'\n                         % (test_loss/(batch_idx+1), 100.*correct/total, correct, total))\n\n    # Save checkpoint.\n    acc = 100.*correct/total\n    if acc > best_acc:\n        print('Saving..')\n        state = {\n            'net': net.state_dict(),\n            'acc': acc,\n            'epoch': epoch,\n        }\n        if not os.path.isdir('/kaggle/working/checkpoint2'):\n            os.mkdir('/kaggle/working/checkpoint2')\n        torch.save(state, '/kaggle/working/checkpoint2/ckpt1.pth')\n        best_acc = acc\n\n\n# for epoch in range(start_epoch, start_epoch+1):\nfor epoch in range(1):\n    train(epoch)\n    test(epoch)\n    scheduler.step()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T10:37:42.533770Z","iopub.execute_input":"2024-12-04T10:37:42.534465Z","iopub.status.idle":"2024-12-04T10:39:42.755559Z","shell.execute_reply.started":"2024-12-04T10:37:42.534430Z","shell.execute_reply":"2024-12-04T10:39:42.754459Z"}},"outputs":[{"name":"stdout","text":"==> Preparing data..\nFiles already downloaded and verified\nFiles already downloaded and verified\n==> Building model..\nTotal Parameters: 3800698\n\nEpoch: 0\n [================================================================>]  Step: 721ms | Tot: 1m40s | Loss: 1.610 | Acc: 39.852% (19926/5000 391/391 1  \n [================================================================>]  Step: 171ms | Tot: 17s518ms | Loss: 1.311 | Acc: 53.110% (5311/1000 100/100 \nSaving..\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, in_planes, planes, stride=1):\n        super(BasicBlock, self).__init__()\n        self.conv1 = nn.Conv2d(\n            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n                               stride=1, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n\n        self.shortcut = nn.Sequential()\n        if stride != 1 or in_planes != self.expansion*planes:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_planes, self.expansion*planes,\n                          kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(self.expansion*planes)\n            )\n\n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = self.bn2(self.conv2(out))\n        out += self.shortcut(x)\n        out = F.relu(out)\n        return out\n\n\nclass Root(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size=1):\n        super(Root, self).__init__()\n        self.conv = nn.Conv2d(\n            in_channels, out_channels, kernel_size,\n            stride=1, padding=(kernel_size - 1) // 2, bias=False)\n        self.bn = nn.BatchNorm2d(out_channels)\n\n    def forward(self, xs):\n        x = torch.cat(xs, 1)\n        out = F.relu(self.bn(self.conv(x)))\n        return out\n\n\nclass Tree(nn.Module):\n    def __init__(self, block, in_channels, out_channels, level=1, stride=1):\n        super(Tree, self).__init__()\n        self.root = Root(2*out_channels, out_channels)\n        if level == 1:\n            self.left_tree = block(in_channels, out_channels, stride=stride)\n            self.right_tree = block(out_channels, out_channels, stride=1)\n        else:\n            self.left_tree = Tree(block, in_channels,\n                                  out_channels, level=level-1, stride=stride)\n            self.right_tree = Tree(block, out_channels,\n                                   out_channels, level=level-1, stride=1)\n\n    def forward(self, x):\n        out1 = self.left_tree(x)\n        out2 = self.right_tree(out1)\n        out = self.root([out1, out2])\n        return out\n\n\nclass SimpleDLA(nn.Module):\n    def __init__(self, block=BasicBlock, num_classes=10):\n        super(SimpleDLA, self).__init__()\n        self.base = nn.Sequential(\n            nn.Conv2d(3, 8, kernel_size=3, stride=1, padding=1, bias=False),\n            nn.BatchNorm2d(8),\n            nn.ReLU(True)\n        )\n\n        self.layer1 = nn.Sequential(\n            nn.Conv2d(8, 8, kernel_size=3, stride=1, padding=1, bias=False),\n            nn.BatchNorm2d(8),\n            nn.ReLU(True)\n        )\n\n        self.layer2 = nn.Sequential(\n            nn.Conv2d(8, 16, kernel_size=3, stride=1, padding=1, bias=False),\n            nn.BatchNorm2d(16),\n            nn.ReLU(True)\n        )\n\n        self.layer3 = Tree(block, 16, 16, level=1, stride=1)\n        self.layer4 = Tree(block, 16, 32, level=2, stride=2)\n        self.layer5 = Tree(block, 32, 64, level=2, stride=2)\n        self.layer6 = Tree(block, 64, 128, level=1, stride=1)\n        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n        self.fc = nn.Linear(128, num_classes)\n\n    def forward(self, x):\n        x = self.base(x)\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        x = self.layer5(x)\n        x = self.layer6(x)\n        x = self.avg_pool(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n        return x\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport torch.backends.cudnn as cudnn\n\nimport torchvision\nimport torchvision.transforms as transforms\n\nimport os\nimport argparse\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nbest_acc = 0  # best test accuracy\nstart_epoch = 0  # start from epoch 0 or last checkpoint epoch\n\n# Data\nprint('==> Preparing data..')\ntransform_train = transforms.Compose([\n    transforms.RandomCrop(32, padding=4),\n    transforms.RandomHorizontalFlip(),\n    transforms.ToTensor(),\n    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n])\n\ntransform_test = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n])\n\ntrainset = torchvision.datasets.CIFAR10(\n    root='./data', train=True, download=True, transform=transform_train)\ntrainloader = torch.utils.data.DataLoader(\n    trainset, batch_size=128, shuffle=True, num_workers=2)\n\ntestset = torchvision.datasets.CIFAR10(\n    root='./data', train=False, download=True, transform=transform_test)\ntestloader = torch.utils.data.DataLoader(\n    testset, batch_size=100, shuffle=False, num_workers=2)\n\nclasses = ('plane', 'car', 'bird', 'cat', 'deer',\n           'dog', 'frog', 'horse', 'ship', 'truck')\n\n# Model\nprint('==> Building model..')\nnet = SimpleDLA()\nprint(f\"Total Parameters: {sum(p.numel() for p in net.parameters())}\")\nnet = net.to(device)\nif device == 'cuda':\n    net = torch.nn.DataParallel(net)\n    cudnn.benchmark = True\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(net.parameters(), lr=0.01,\n                      momentum=0.9, weight_decay=5e-4)\nscheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)\n\n\n# Training\ndef train(epoch):\n    print('\\nEpoch: %d' % epoch)\n    net.train()\n    train_loss = 0\n    correct = 0\n    total = 0\n    for batch_idx, (inputs, targets) in enumerate(trainloader):\n        inputs, targets = inputs.to(device), targets.to(device)\n        optimizer.zero_grad()\n        outputs = net(inputs)\n        loss = criterion(outputs, targets)\n        loss.backward()\n        optimizer.step()\n\n        train_loss += loss.item()\n        _, predicted = outputs.max(1)\n        total += targets.size(0)\n        correct += predicted.eq(targets).sum().item()\n\n        progress_bar(batch_idx, len(trainloader), 'Loss: %.3f | Train Acc: %.3f%% (%d/%d)'\n                     % (train_loss/(batch_idx+1), 100.*correct/total, correct, total))\n\n\ndef test(epoch):\n    global best_acc\n    net.eval()\n    test_loss = 0\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for batch_idx, (inputs, targets) in enumerate(testloader):\n            inputs, targets = inputs.to(device), targets.to(device)\n            outputs = net(inputs)\n            loss = criterion(outputs, targets)\n\n            test_loss += loss.item()\n            _, predicted = outputs.max(1)\n            total += targets.size(0)\n            correct += predicted.eq(targets).sum().item()\n\n            progress_bar(batch_idx, len(testloader), 'Loss: %.3f | Test Acc: %.3f%% (%d/%d)'\n                         % (test_loss/(batch_idx+1), 100.*correct/total, correct, total))\n\n    # Save checkpoint.\n    acc = 100.*correct/total\n    if acc > best_acc:\n        print('Saving..')\n        state = {\n            'net': net.state_dict(),\n            'acc': acc,\n            'epoch': epoch,\n        }\n        if not os.path.isdir('/kaggle/working/checkpoint2'):\n            os.mkdir('/kaggle/working/checkpoint2')\n        torch.save(state, '/kaggle/working/checkpoint2/ckpt_new_today.pth')\n        best_acc = acc\n\n\nfor epoch in range(130):\n    train(epoch)\n    test(epoch)\n    scheduler.step()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-05T00:37:43.691878Z","iopub.execute_input":"2024-12-05T00:37:43.692564Z","iopub.status.idle":"2024-12-05T03:18:25.471580Z","shell.execute_reply.started":"2024-12-05T00:37:43.692530Z","shell.execute_reply":"2024-12-05T03:18:25.470468Z"}},"outputs":[{"name":"stdout","text":"==> Preparing data..\nFiles already downloaded and verified\nFiles already downloaded and verified\n==> Building model..\nTotal Parameters: 952962\n\nEpoch: 0\n [================================================================>]  Step: 373ms | Tot: 1m2s | Loss: 1.714 | Train Acc: 35.596% (17798/5000 391/391 91  \n [================================================================>]  Step: 130ms | Tot: 11s613ms | Loss: 1.472 | Test Acc: 45.820% (4582/1000 100/100 \nSaving..\n\nEpoch: 1\n [================================================================>]  Step: 180ms | Tot: 1m2s | Loss: 1.243 | Train Acc: 55.066% (27533/5000 391/391 91  \n [================================================================>]  Step: 101ms | Tot: 11s374ms | Loss: 1.321 | Test Acc: 54.360% (5436/1000 100/100 \nSaving..\n\nEpoch: 2\n [================================================================>]  Step: 134ms | Tot: 1m2s | Loss: 1.011 | Train Acc: 63.750% (31875/5000 391/391 91  \n [================================================================>]  Step: 105ms | Tot: 11s320ms | Loss: 0.926 | Test Acc: 67.590% (6759/1000 100/100 \nSaving..\n\nEpoch: 3\n [================================================================>]  Step: 124ms | Tot: 1m2s | Loss: 0.872 | Train Acc: 69.146% (34573/5000 391/391 91  \n [================================================================>]  Step: 100ms | Tot: 11s575ms | Loss: 0.891 | Test Acc: 69.930% (6993/1000 100/100 \nSaving..\n\nEpoch: 4\n [================================================================>]  Step: 153ms | Tot: 1m2s | Loss: 0.770 | Train Acc: 73.176% (36588/5000 391/391 91  \n [================================================================>]  Step: 107ms | Tot: 11s182ms | Loss: 0.753 | Test Acc: 74.030% (7403/1000 100/100 \nSaving..\n\nEpoch: 5\n [================================================================>]  Step: 133ms | Tot: 1m2s | Loss: 0.700 | Train Acc: 75.720% (37860/5000 391/391 91  \n [================================================================>]  Step: 99ms | Tot: 11s99ms | Loss: 0.759 | Test Acc: 75.030% (7503/1000 100/100 \nSaving..\n\nEpoch: 6\n [================================================================>]  Step: 130ms | Tot: 1m2s | Loss: 0.632 | Train Acc: 77.986% (38993/5000 391/391 91  \n [================================================================>]  Step: 105ms | Tot: 11s718ms | Loss: 0.664 | Test Acc: 77.180% (7718/1000 100/100 \nSaving..\n\nEpoch: 7\n [================================================================>]  Step: 182ms | Tot: 1m2s | Loss: 0.594 | Train Acc: 79.284% (39642/5000 391/391 91  \n [================================================================>]  Step: 110ms | Tot: 11s296ms | Loss: 0.660 | Test Acc: 77.680% (7768/1000 100/100 \nSaving..\n\nEpoch: 8\n [================================================================>]  Step: 140ms | Tot: 1m2s | Loss: 0.559 | Train Acc: 80.746% (40373/5000 391/391 91  \n [================================================================>]  Step: 106ms | Tot: 11s343ms | Loss: 0.584 | Test Acc: 80.280% (8028/1000 100/100 \nSaving..\n\nEpoch: 9\n [================================================================>]  Step: 138ms | Tot: 1m2s | Loss: 0.523 | Train Acc: 81.920% (40960/5000 391/391 91  \n [================================================================>]  Step: 109ms | Tot: 11s570ms | Loss: 0.712 | Test Acc: 76.740% (7674/1000 100/100 \n\nEpoch: 10\n [================================================================>]  Step: 169ms | Tot: 1m2s | Loss: 0.500 | Train Acc: 82.846% (41423/5000 391/391 91  \n [================================================================>]  Step: 106ms | Tot: 11s412ms | Loss: 0.652 | Test Acc: 78.450% (7845/1000 100/100 \n\nEpoch: 11\n [================================================================>]  Step: 141ms | Tot: 1m2s | Loss: 0.475 | Train Acc: 83.518% (41759/5000 391/391 91  \n [================================================================>]  Step: 103ms | Tot: 11s341ms | Loss: 0.618 | Test Acc: 79.010% (7901/1000 100/100 \n\nEpoch: 12\n [================================================================>]  Step: 138ms | Tot: 1m2s | Loss: 0.464 | Train Acc: 83.826% (41913/5000 391/391 91  \n [================================================================>]  Step: 104ms | Tot: 11s658ms | Loss: 0.641 | Test Acc: 78.650% (7865/1000 100/100 \n\nEpoch: 13\n [================================================================>]  Step: 154ms | Tot: 1m2s | Loss: 0.442 | Train Acc: 84.638% (42319/5000 391/391 91  \n [================================================================>]  Step: 106ms | Tot: 11s263ms | Loss: 0.525 | Test Acc: 82.330% (8233/1000 100/100 \nSaving..\n\nEpoch: 14\n [================================================================>]  Step: 126ms | Tot: 1m2s | Loss: 0.433 | Train Acc: 84.994% (42497/5000 391/391 91  \n [================================================================>]  Step: 103ms | Tot: 11s237ms | Loss: 0.510 | Test Acc: 82.870% (8287/1000 100/100 \nSaving..\n\nEpoch: 15\n [================================================================>]  Step: 125ms | Tot: 1m2s | Loss: 0.411 | Train Acc: 85.992% (42996/5000 391/391 91  \n [================================================================>]  Step: 140ms | Tot: 11s570ms | Loss: 0.469 | Test Acc: 84.210% (8421/1000 100/100 \nSaving..\n\nEpoch: 16\n [================================================================>]  Step: 126ms | Tot: 1m2s | Loss: 0.400 | Train Acc: 86.254% (43127/5000 391/391 91  \n [================================================================>]  Step: 102ms | Tot: 11s277ms | Loss: 0.566 | Test Acc: 81.750% (8175/1000 100/100 \n\nEpoch: 17\n [================================================================>]  Step: 133ms | Tot: 1m2s | Loss: 0.387 | Train Acc: 86.746% (43373/5000 391/391 91  \n [================================================================>]  Step: 110ms | Tot: 11s196ms | Loss: 0.558 | Test Acc: 81.100% (8110/1000 100/100 \n\nEpoch: 18\n [================================================================>]  Step: 136ms | Tot: 1m2s | Loss: 0.382 | Train Acc: 86.876% (43438/5000 391/391 91  \n [================================================================>]  Step: 102ms | Tot: 11s561ms | Loss: 0.544 | Test Acc: 82.010% (8201/1000 100/100 \n\nEpoch: 19\n [================================================================>]  Step: 134ms | Tot: 1m2s | Loss: 0.364 | Train Acc: 87.414% (43707/5000 391/391 91  \n [================================================================>]  Step: 100ms | Tot: 11s204ms | Loss: 0.455 | Test Acc: 84.810% (8481/1000 100/100 \nSaving..\n\nEpoch: 20\n [================================================================>]  Step: 128ms | Tot: 1m2s | Loss: 0.358 | Train Acc: 87.506% (43753/5000 391/391 91  \n [================================================================>]  Step: 107ms | Tot: 11s245ms | Loss: 0.476 | Test Acc: 84.470% (8447/1000 100/100 \n\nEpoch: 21\n [================================================================>]  Step: 136ms | Tot: 1m2s | Loss: 0.349 | Train Acc: 87.888% (43944/5000 391/391 91  \n [================================================================>]  Step: 106ms | Tot: 11s624ms | Loss: 0.452 | Test Acc: 84.870% (8487/1000 100/100 \nSaving..\n\nEpoch: 22\n [================================================================>]  Step: 156ms | Tot: 1m2s | Loss: 0.341 | Train Acc: 88.162% (44081/5000 391/391 91  \n [================================================================>]  Step: 103ms | Tot: 11s292ms | Loss: 0.468 | Test Acc: 84.560% (8456/1000 100/100 \n\nEpoch: 23\n [================================================================>]  Step: 123ms | Tot: 1m2s | Loss: 0.328 | Train Acc: 88.628% (44314/5000 391/391 91  \n [================================================================>]  Step: 105ms | Tot: 11s343ms | Loss: 0.446 | Test Acc: 85.620% (8562/1000 100/100 \nSaving..\n\nEpoch: 24\n [================================================================>]  Step: 144ms | Tot: 1m2s | Loss: 0.326 | Train Acc: 88.674% (44337/5000 391/391 91  \n [================================================================>]  Step: 102ms | Tot: 11s542ms | Loss: 0.453 | Test Acc: 85.080% (8508/1000 100/100 \n\nEpoch: 25\n [================================================================>]  Step: 157ms | Tot: 1m2s | Loss: 0.318 | Train Acc: 88.888% (44444/5000 391/391 91  \n [================================================================>]  Step: 112ms | Tot: 11s275ms | Loss: 0.450 | Test Acc: 85.270% (8527/1000 100/100 \n\nEpoch: 26\n [================================================================>]  Step: 124ms | Tot: 1m2s | Loss: 0.309 | Train Acc: 89.320% (44660/5000 391/391 91  \n [================================================================>]  Step: 109ms | Tot: 11s381ms | Loss: 0.463 | Test Acc: 84.900% (8490/1000 100/100 \n\nEpoch: 27\n [================================================================>]  Step: 130ms | Tot: 1m2s | Loss: 0.304 | Train Acc: 89.376% (44688/5000 391/391 91  \n [================================================================>]  Step: 107ms | Tot: 11s558ms | Loss: 0.432 | Test Acc: 85.870% (8587/1000 100/100 \nSaving..\n\nEpoch: 28\n [================================================================>]  Step: 138ms | Tot: 1m2s | Loss: 0.300 | Train Acc: 89.728% (44864/5000 391/391 91  \n [================================================================>]  Step: 106ms | Tot: 11s277ms | Loss: 0.442 | Test Acc: 85.620% (8562/1000 100/100 \n\nEpoch: 29\n [================================================================>]  Step: 130ms | Tot: 1m2s | Loss: 0.292 | Train Acc: 89.940% (44970/5000 391/391 91  \n [================================================================>]  Step: 101ms | Tot: 11s237ms | Loss: 0.440 | Test Acc: 85.660% (8566/1000 100/100 \n\nEpoch: 30\n [================================================================>]  Step: 147ms | Tot: 1m2s | Loss: 0.288 | Train Acc: 90.010% (45005/5000 391/391 91  \n [================================================================>]  Step: 106ms | Tot: 11s632ms | Loss: 0.469 | Test Acc: 84.640% (8464/1000 100/100 \n\nEpoch: 31\n [================================================================>]  Step: 126ms | Tot: 1m2s | Loss: 0.283 | Train Acc: 90.154% (45077/5000 391/391 91  \n [================================================================>]  Step: 109ms | Tot: 11s366ms | Loss: 0.433 | Test Acc: 85.790% (8579/1000 100/100 \n\nEpoch: 32\n [================================================================>]  Step: 137ms | Tot: 1m2s | Loss: 0.273 | Train Acc: 90.752% (45376/5000 391/391 91  \n [================================================================>]  Step: 106ms | Tot: 11s378ms | Loss: 0.412 | Test Acc: 86.560% (8656/1000 100/100 \nSaving..\n\nEpoch: 33\n [================================================================>]  Step: 134ms | Tot: 1m2s | Loss: 0.274 | Train Acc: 90.510% (45255/5000 391/391 91  \n [================================================================>]  Step: 104ms | Tot: 11s455ms | Loss: 0.453 | Test Acc: 85.600% (8560/1000 100/100 \n\nEpoch: 34\n [================================================================>]  Step: 129ms | Tot: 1m2s | Loss: 0.269 | Train Acc: 90.706% (45353/5000 391/391 91  \n [================================================================>]  Step: 112ms | Tot: 11s243ms | Loss: 0.526 | Test Acc: 83.340% (8334/1000 100/100 \n\nEpoch: 35\n [================================================================>]  Step: 148ms | Tot: 1m2s | Loss: 0.266 | Train Acc: 90.788% (45394/5000 391/391 91  \n [================================================================>]  Step: 106ms | Tot: 11s348ms | Loss: 0.426 | Test Acc: 86.200% (8620/1000 100/100 \n\nEpoch: 36\n [================================================================>]  Step: 137ms | Tot: 1m2s | Loss: 0.258 | Train Acc: 91.094% (45547/5000 391/391 91  \n [================================================================>]  Step: 103ms | Tot: 11s677ms | Loss: 0.390 | Test Acc: 87.520% (8752/1000 100/100 \nSaving..\n\nEpoch: 37\n [================================================================>]  Step: 144ms | Tot: 1m2s | Loss: 0.253 | Train Acc: 91.296% (45648/5000 391/391 91  \n [================================================================>]  Step: 107ms | Tot: 11s204ms | Loss: 0.435 | Test Acc: 85.830% (8583/1000 100/100 \n\nEpoch: 38\n [================================================================>]  Step: 132ms | Tot: 1m2s | Loss: 0.255 | Train Acc: 91.114% (45557/5000 391/391 91  \n [================================================================>]  Step: 100ms | Tot: 11s221ms | Loss: 0.422 | Test Acc: 86.310% (8631/1000 100/100 \n\nEpoch: 39\n [================================================================>]  Step: 134ms | Tot: 1m2s | Loss: 0.245 | Train Acc: 91.534% (45767/5000 391/391 91  \n [================================================================>]  Step: 100ms | Tot: 11s429ms | Loss: 0.402 | Test Acc: 87.310% (8731/1000 100/100 \n\nEpoch: 40\n [================================================================>]  Step: 140ms | Tot: 1m2s | Loss: 0.243 | Train Acc: 91.520% (45760/5000 391/391 91  \n [================================================================>]  Step: 105ms | Tot: 11s232ms | Loss: 0.531 | Test Acc: 83.910% (8391/1000 100/100 \n\nEpoch: 41\n [================================================================>]  Step: 138ms | Tot: 1m1s | Loss: 0.238 | Train Acc: 91.612% (45806/5000 391/391 91  \n [================================================================>]  Step: 104ms | Tot: 11s192ms | Loss: 0.452 | Test Acc: 85.160% (8516/1000 100/100 \n\nEpoch: 42\n [================================================================>]  Step: 157ms | Tot: 1m2s | Loss: 0.237 | Train Acc: 91.872% (45936/5000 391/391 91  \n [================================================================>]  Step: 106ms | Tot: 11s537ms | Loss: 0.384 | Test Acc: 87.720% (8772/1000 100/100 \nSaving..\n\nEpoch: 43\n [================================================================>]  Step: 154ms | Tot: 1m1s | Loss: 0.236 | Train Acc: 91.822% (45911/5000 391/391 91  \n [================================================================>]  Step: 108ms | Tot: 11s243ms | Loss: 0.376 | Test Acc: 87.830% (8783/1000 100/100 \nSaving..\n\nEpoch: 44\n [================================================================>]  Step: 131ms | Tot: 1m2s | Loss: 0.227 | Train Acc: 92.080% (46040/5000 391/391 91  \n [================================================================>]  Step: 113ms | Tot: 11s202ms | Loss: 0.378 | Test Acc: 87.890% (8789/1000 100/100 \nSaving..\n\nEpoch: 45\n [================================================================>]  Step: 130ms | Tot: 1m2s | Loss: 0.224 | Train Acc: 92.326% (46163/5000 391/391 91  \n [================================================================>]  Step: 130ms | Tot: 11s482ms | Loss: 0.377 | Test Acc: 87.690% (8769/1000 100/100 \n\nEpoch: 46\n [================================================================>]  Step: 126ms | Tot: 1m1s | Loss: 0.226 | Train Acc: 92.160% (46080/5000 391/391 91  \n [================================================================>]  Step: 100ms | Tot: 11s275ms | Loss: 0.393 | Test Acc: 87.620% (8762/1000 100/100 \n\nEpoch: 47\n [================================================================>]  Step: 128ms | Tot: 1m1s | Loss: 0.218 | Train Acc: 92.360% (46180/5000 391/391 91  \n [================================================================>]  Step: 104ms | Tot: 11s261ms | Loss: 0.403 | Test Acc: 87.280% (8728/1000 100/100 \n\nEpoch: 48\n [================================================================>]  Step: 132ms | Tot: 1m1s | Loss: 0.214 | Train Acc: 92.540% (46270/5000 391/391 91  ============================================>....................]  Step: 201ms | Tot: 42s956ms | Loss: 0.212 | Train Acc: 92.633% (32014/3456 270/391 \n [================================================================>]  Step: 116ms | Tot: 11s239ms | Loss: 0.385 | Test Acc: 88.120% (8812/1000 100/100 \nSaving..\n\nEpoch: 49\n [================================================================>]  Step: 134ms | Tot: 1m2s | Loss: 0.213 | Train Acc: 92.686% (46343/5000 391/391 91  \n [================================================================>]  Step: 117ms | Tot: 11s607ms | Loss: 0.390 | Test Acc: 87.760% (8776/1000 100/100 \n\nEpoch: 50\n [================================================================>]  Step: 135ms | Tot: 1m2s | Loss: 0.209 | Train Acc: 92.726% (46363/5000 391/391 91  \n [================================================================>]  Step: 104ms | Tot: 11s153ms | Loss: 0.368 | Test Acc: 88.280% (8828/1000 100/100 \nSaving..\n\nEpoch: 51\n [================================================================>]  Step: 128ms | Tot: 1m2s | Loss: 0.207 | Train Acc: 92.742% (46371/5000 391/391 91  \n [================================================================>]  Step: 105ms | Tot: 11s94ms | Loss: 0.411 | Test Acc: 87.030% (8703/1000 100/100 \n\nEpoch: 52\n [================================================================>]  Step: 136ms | Tot: 1m1s | Loss: 0.206 | Train Acc: 92.866% (46433/5000 391/391 91  \n [================================================================>]  Step: 102ms | Tot: 11s563ms | Loss: 0.366 | Test Acc: 88.310% (8831/1000 100/100 \nSaving..\n\nEpoch: 53\n [================================================================>]  Step: 134ms | Tot: 1m2s | Loss: 0.202 | Train Acc: 92.928% (46464/5000 391/391 91  \n [================================================================>]  Step: 107ms | Tot: 11s343ms | Loss: 0.365 | Test Acc: 88.310% (8831/1000 100/100 \n\nEpoch: 54\n [================================================================>]  Step: 128ms | Tot: 1m2s | Loss: 0.195 | Train Acc: 93.266% (46633/5000 391/391 91  \n [================================================================>]  Step: 116ms | Tot: 11s242ms | Loss: 0.375 | Test Acc: 88.160% (8816/1000 100/100 \n\nEpoch: 55\n [================================================================>]  Step: 137ms | Tot: 1m1s | Loss: 0.191 | Train Acc: 93.394% (46697/5000 391/391 91  \n [================================================================>]  Step: 105ms | Tot: 11s399ms | Loss: 0.423 | Test Acc: 86.990% (8699/1000 100/100 \n\nEpoch: 56\n [================================================================>]  Step: 128ms | Tot: 1m2s | Loss: 0.196 | Train Acc: 93.338% (46669/5000 391/391 91  \n [================================================================>]  Step: 102ms | Tot: 11s274ms | Loss: 0.411 | Test Acc: 86.990% (8699/1000 100/100 \n\nEpoch: 57\n [================================================================>]  Step: 147ms | Tot: 1m1s | Loss: 0.190 | Train Acc: 93.362% (46681/5000 391/391 91  \n [================================================================>]  Step: 99ms | Tot: 11s493ms | Loss: 0.380 | Test Acc: 87.970% (8797/1000 100/100 \n\nEpoch: 58\n [================================================================>]  Step: 126ms | Tot: 1m1s | Loss: 0.189 | Train Acc: 93.418% (46709/5000 391/391 91  \n [================================================================>]  Step: 107ms | Tot: 11s517ms | Loss: 0.388 | Test Acc: 87.900% (8790/1000 100/100 \n\nEpoch: 59\n [================================================================>]  Step: 132ms | Tot: 1m2s | Loss: 0.188 | Train Acc: 93.440% (46720/5000 391/391 91  \n [================================================================>]  Step: 103ms | Tot: 11s247ms | Loss: 0.387 | Test Acc: 88.080% (8808/1000 100/100 \n\nEpoch: 60\n [================================================================>]  Step: 136ms | Tot: 1m2s | Loss: 0.187 | Train Acc: 93.444% (46722/5000 391/391 91  \n [================================================================>]  Step: 101ms | Tot: 11s307ms | Loss: 0.396 | Test Acc: 87.490% (8749/1000 100/100 \n\nEpoch: 61\n [================================================================>]  Step: 132ms | Tot: 1m1s | Loss: 0.180 | Train Acc: 93.730% (46865/5000 391/391 91  \n [================================================================>]  Step: 104ms | Tot: 11s510ms | Loss: 0.352 | Test Acc: 88.760% (8876/1000 100/100 \nSaving..\n\nEpoch: 62\n [================================================================>]  Step: 141ms | Tot: 1m1s | Loss: 0.182 | Train Acc: 93.706% (46853/5000 391/391 91  \n [================================================================>]  Step: 107ms | Tot: 11s201ms | Loss: 0.353 | Test Acc: 88.640% (8864/1000 100/100 \n\nEpoch: 63\n [================================================================>]  Step: 137ms | Tot: 1m2s | Loss: 0.173 | Train Acc: 93.948% (46974/5000 391/391 91  \n [================================================================>]  Step: 103ms | Tot: 11s149ms | Loss: 0.383 | Test Acc: 88.250% (8825/1000 100/100 \n\nEpoch: 64\n [================================================================>]  Step: 127ms | Tot: 1m2s | Loss: 0.176 | Train Acc: 93.938% (46969/5000 391/391 91  \n [================================================================>]  Step: 108ms | Tot: 11s536ms | Loss: 0.398 | Test Acc: 88.150% (8815/1000 100/100 \n\nEpoch: 65\n [================================================================>]  Step: 131ms | Tot: 1m1s | Loss: 0.172 | Train Acc: 94.148% (47074/5000 391/391 91  \n [================================================================>]  Step: 102ms | Tot: 11s293ms | Loss: 0.470 | Test Acc: 85.880% (8588/1000 100/100 \n\nEpoch: 66\n [================================================================>]  Step: 133ms | Tot: 1m2s | Loss: 0.173 | Train Acc: 93.986% (46993/5000 391/391 91  \n [================================================================>]  Step: 109ms | Tot: 11s169ms | Loss: 0.363 | Test Acc: 88.690% (8869/1000 100/100 \n\nEpoch: 67\n [================================================================>]  Step: 127ms | Tot: 1m1s | Loss: 0.168 | Train Acc: 94.206% (47103/5000 391/391 91  \n [================================================================>]  Step: 105ms | Tot: 11s373ms | Loss: 0.389 | Test Acc: 88.090% (8809/1000 100/100 \n\nEpoch: 68\n [================================================================>]  Step: 146ms | Tot: 1m1s | Loss: 0.168 | Train Acc: 94.222% (47111/5000 391/391 91  \n [================================================================>]  Step: 109ms | Tot: 11s159ms | Loss: 0.400 | Test Acc: 88.210% (8821/1000 100/100 \n\nEpoch: 69\n [================================================================>]  Step: 127ms | Tot: 1m1s | Loss: 0.161 | Train Acc: 94.420% (47210/5000 391/391 91  \n [================================================================>]  Step: 106ms | Tot: 11s126ms | Loss: 0.413 | Test Acc: 87.700% (8770/1000 100/100 \n\nEpoch: 70\n [================================================================>]  Step: 134ms | Tot: 1m2s | Loss: 0.162 | Train Acc: 94.298% (47149/5000 391/391 91  \n [================================================================>]  Step: 102ms | Tot: 11s90ms | Loss: 0.380 | Test Acc: 88.510% (8851/1000 100/100 \n\nEpoch: 71\n [================================================================>]  Step: 131ms | Tot: 1m2s | Loss: 0.158 | Train Acc: 94.564% (47282/5000 391/391 91  \n [================================================================>]  Step: 103ms | Tot: 11s383ms | Loss: 0.383 | Test Acc: 88.550% (8855/1000 100/100 \n\nEpoch: 72\n [================================================================>]  Step: 138ms | Tot: 1m2s | Loss: 0.159 | Train Acc: 94.444% (47222/5000 391/391 91  \n [================================================================>]  Step: 103ms | Tot: 11s95ms | Loss: 0.390 | Test Acc: 88.280% (8828/1000 100/100 \n\nEpoch: 73\n [================================================================>]  Step: 130ms | Tot: 1m2s | Loss: 0.158 | Train Acc: 94.464% (47232/5000 391/391 91  \n [================================================================>]  Step: 102ms | Tot: 11s59ms | Loss: 0.424 | Test Acc: 87.200% (8720/1000 100/100 \n\nEpoch: 74\n [================================================================>]  Step: 141ms | Tot: 1m1s | Loss: 0.151 | Train Acc: 94.776% (47388/5000 391/391 91  \n [================================================================>]  Step: 104ms | Tot: 11s483ms | Loss: 0.411 | Test Acc: 87.950% (8795/1000 100/100 \n\nEpoch: 75\n [================================================================>]  Step: 144ms | Tot: 1m2s | Loss: 0.150 | Train Acc: 94.776% (47388/5000 391/391 91  \n [================================================================>]  Step: 105ms | Tot: 11s146ms | Loss: 0.388 | Test Acc: 88.470% (8847/1000 100/100 \n\nEpoch: 76\n [================================================================>]  Step: 132ms | Tot: 1m2s | Loss: 0.147 | Train Acc: 94.874% (47437/5000 391/391 91  \n [================================================================>]  Step: 107ms | Tot: 11s189ms | Loss: 0.379 | Test Acc: 88.700% (8870/1000 100/100 \n\nEpoch: 77\n [================================================================>]  Step: 138ms | Tot: 1m2s | Loss: 0.148 | Train Acc: 94.944% (47472/5000 391/391 91  \n [================================================================>]  Step: 108ms | Tot: 11s483ms | Loss: 0.374 | Test Acc: 88.680% (8868/1000 100/100 \n\nEpoch: 78\n [================================================================>]  Step: 135ms | Tot: 1m2s | Loss: 0.143 | Train Acc: 95.094% (47547/5000 391/391 91  \n [================================================================>]  Step: 106ms | Tot: 11s67ms | Loss: 0.383 | Test Acc: 88.530% (8853/1000 100/100 \n\nEpoch: 79\n [================================================================>]  Step: 135ms | Tot: 1m2s | Loss: 0.148 | Train Acc: 94.924% (47462/5000 391/391 91  \n [================================================================>]  Step: 102ms | Tot: 11s37ms | Loss: 0.398 | Test Acc: 88.440% (8844/1000 100/100 \n\nEpoch: 80\n [================================================================>]  Step: 133ms | Tot: 1m2s | Loss: 0.138 | Train Acc: 95.246% (47623/5000 391/391 91  \n [================================================================>]  Step: 102ms | Tot: 11s433ms | Loss: 0.439 | Test Acc: 87.140% (8714/1000 100/100 \n\nEpoch: 81\n [================================================================>]  Step: 140ms | Tot: 1m2s | Loss: 0.142 | Train Acc: 95.012% (47506/5000 391/391 91  \n [================================================================>]  Step: 101ms | Tot: 11s64ms | Loss: 0.389 | Test Acc: 88.730% (8873/1000 100/100 \n\nEpoch: 82\n [================================================================>]  Step: 135ms | Tot: 1m2s | Loss: 0.140 | Train Acc: 95.110% (47555/5000 391/391 91  \n [================================================================>]  Step: 108ms | Tot: 11s37ms | Loss: 0.356 | Test Acc: 89.250% (8925/1000 100/100 \nSaving..\n\nEpoch: 83\n [================================================================>]  Step: 141ms | Tot: 1m2s | Loss: 0.131 | Train Acc: 95.486% (47743/5000 391/391 91  \n [================================================================>]  Step: 100ms | Tot: 11s430ms | Loss: 0.392 | Test Acc: 88.630% (8863/1000 100/100 \n\nEpoch: 84\n [================================================================>]  Step: 125ms | Tot: 1m2s | Loss: 0.134 | Train Acc: 95.250% (47625/5000 391/391 91  \n [================================================================>]  Step: 104ms | Tot: 11s134ms | Loss: 0.369 | Test Acc: 88.880% (8888/1000 100/100 \n\nEpoch: 85\n [================================================================>]  Step: 127ms | Tot: 1m1s | Loss: 0.131 | Train Acc: 95.534% (47767/5000 391/391 91  \n [================================================================>]  Step: 100ms | Tot: 11s24ms | Loss: 0.400 | Test Acc: 88.530% (8853/1000 100/100 \n\nEpoch: 86\n [================================================================>]  Step: 141ms | Tot: 1m1s | Loss: 0.128 | Train Acc: 95.618% (47809/5000 391/391 91  \n [================================================================>]  Step: 104ms | Tot: 11s368ms | Loss: 0.380 | Test Acc: 88.800% (8880/1000 100/100 \n\nEpoch: 87\n [================================================================>]  Step: 138ms | Tot: 1m2s | Loss: 0.122 | Train Acc: 95.784% (47892/5000 391/391 91  \n [================================================================>]  Step: 106ms | Tot: 11s38ms | Loss: 0.360 | Test Acc: 89.440% (8944/1000 100/100 \nSaving..\n\nEpoch: 88\n [================================================================>]  Step: 131ms | Tot: 1m2s | Loss: 0.130 | Train Acc: 95.490% (47745/5000 391/391 91  \n [================================================================>]  Step: 99ms | Tot: 10s979ms | Loss: 0.356 | Test Acc: 89.640% (8964/1000 100/100 \nSaving..\n\nEpoch: 89\n [================================================================>]  Step: 126ms | Tot: 1m2s | Loss: 0.122 | Train Acc: 95.752% (47876/5000 391/391 91  \n [================================================================>]  Step: 102ms | Tot: 11s468ms | Loss: 0.401 | Test Acc: 88.550% (8855/1000 100/100 \n\nEpoch: 90\n [================================================================>]  Step: 157ms | Tot: 1m1s | Loss: 0.121 | Train Acc: 95.830% (47915/5000 391/391 91  \n [================================================================>]  Step: 111ms | Tot: 11s95ms | Loss: 0.376 | Test Acc: 88.950% (8895/1000 100/100 \n\nEpoch: 91\n [================================================================>]  Step: 141ms | Tot: 1m2s | Loss: 0.118 | Train Acc: 95.882% (47941/5000 391/391 91  \n [================================================================>]  Step: 113ms | Tot: 11s107ms | Loss: 0.436 | Test Acc: 87.670% (8767/1000 100/100 \n\nEpoch: 92\n [================================================================>]  Step: 128ms | Tot: 1m1s | Loss: 0.120 | Train Acc: 95.862% (47931/5000 391/391 91  \n [================================================================>]  Step: 126ms | Tot: 11s284ms | Loss: 0.371 | Test Acc: 89.260% (8926/1000 100/100 \n\nEpoch: 93\n [================================================================>]  Step: 131ms | Tot: 1m1s | Loss: 0.112 | Train Acc: 96.052% (48026/5000 391/391 91  \n [================================================================>]  Step: 106ms | Tot: 11s551ms | Loss: 0.392 | Test Acc: 88.790% (8879/1000 100/100 \n\nEpoch: 94\n [================================================================>]  Step: 134ms | Tot: 1m2s | Loss: 0.113 | Train Acc: 96.064% (48032/5000 391/391 91  \n [================================================================>]  Step: 100ms | Tot: 11s102ms | Loss: 0.374 | Test Acc: 89.500% (8950/1000 100/100 \n\nEpoch: 95\n [================================================================>]  Step: 132ms | Tot: 1m2s | Loss: 0.110 | Train Acc: 96.214% (48107/5000 391/391 91  \n [================================================================>]  Step: 105ms | Tot: 11s27ms | Loss: 0.405 | Test Acc: 88.930% (8893/1000 100/100 \n\nEpoch: 96\n [================================================================>]  Step: 124ms | Tot: 1m2s | Loss: 0.107 | Train Acc: 96.254% (48127/5000 391/391 91  \n [================================================================>]  Step: 101ms | Tot: 11s386ms | Loss: 0.367 | Test Acc: 89.460% (8946/1000 100/100 \n\nEpoch: 97\n [================================================================>]  Step: 133ms | Tot: 1m2s | Loss: 0.105 | Train Acc: 96.440% (48220/5000 391/391 91  \n [================================================================>]  Step: 112ms | Tot: 10s893ms | Loss: 0.400 | Test Acc: 88.280% (8828/1000 100/100 \n\nEpoch: 98\n [================================================================>]  Step: 147ms | Tot: 1m2s | Loss: 0.101 | Train Acc: 96.516% (48258/5000 391/391 91  \n [================================================================>]  Step: 101ms | Tot: 10s932ms | Loss: 0.379 | Test Acc: 89.470% (8947/1000 100/100 \n\nEpoch: 99\n [================================================================>]  Step: 140ms | Tot: 1m1s | Loss: 0.102 | Train Acc: 96.492% (48246/5000 391/391 91  \n [================================================================>]  Step: 102ms | Tot: 11s290ms | Loss: 0.373 | Test Acc: 89.570% (8957/1000 100/100 \n\nEpoch: 100\n [================================================================>]  Step: 136ms | Tot: 1m1s | Loss: 0.099 | Train Acc: 96.646% (48323/5000 391/391 91  \n [================================================================>]  Step: 108ms | Tot: 11s16ms | Loss: 0.384 | Test Acc: 88.910% (8891/1000 100/100 \n\nEpoch: 101\n [================================================================>]  Step: 139ms | Tot: 1m1s | Loss: 0.102 | Train Acc: 96.460% (48230/5000 391/391 91  \n [================================================================>]  Step: 103ms | Tot: 11s66ms | Loss: 0.404 | Test Acc: 88.880% (8888/1000 100/100 \n\nEpoch: 102\n [================================================================>]  Step: 141ms | Tot: 1m1s | Loss: 0.095 | Train Acc: 96.730% (48365/5000 391/391 91  \n [================================================================>]  Step: 119ms | Tot: 11s274ms | Loss: 0.437 | Test Acc: 88.060% (8806/1000 100/100 \n\nEpoch: 103\n [================================================================>]  Step: 129ms | Tot: 1m1s | Loss: 0.095 | Train Acc: 96.764% (48382/5000 391/391 91  \n [================================================================>]  Step: 102ms | Tot: 10s936ms | Loss: 0.374 | Test Acc: 89.660% (8966/1000 100/100 \nSaving..\n\nEpoch: 104\n [================================================================>]  Step: 128ms | Tot: 1m2s | Loss: 0.096 | Train Acc: 96.594% (48297/5000 391/391 91  \n [================================================================>]  Step: 102ms | Tot: 10s984ms | Loss: 0.377 | Test Acc: 89.910% (8991/1000 100/100 \nSaving..\n\nEpoch: 105\n [================================================================>]  Step: 126ms | Tot: 1m2s | Loss: 0.085 | Train Acc: 97.068% (48534/5000 391/391 91  \n [================================================================>]  Step: 107ms | Tot: 11s366ms | Loss: 0.392 | Test Acc: 89.320% (8932/1000 100/100 \n\nEpoch: 106\n [================================================================>]  Step: 129ms | Tot: 1m1s | Loss: 0.091 | Train Acc: 96.870% (48435/5000 391/391 91  \n [================================================================>]  Step: 103ms | Tot: 11s66ms | Loss: 0.386 | Test Acc: 89.540% (8954/1000 100/100 \n\nEpoch: 107\n [================================================================>]  Step: 126ms | Tot: 1m2s | Loss: 0.087 | Train Acc: 96.974% (48487/5000 391/391 91  \n [================================================================>]  Step: 109ms | Tot: 11s61ms | Loss: 0.387 | Test Acc: 89.520% (8952/1000 100/100 \n\nEpoch: 108\n [================================================================>]  Step: 133ms | Tot: 1m2s | Loss: 0.085 | Train Acc: 97.146% (48573/5000 391/391 91  \n [================================================================>]  Step: 104ms | Tot: 11s426ms | Loss: 0.416 | Test Acc: 88.730% (8873/1000 100/100 \n\nEpoch: 109\n [================================================================>]  Step: 155ms | Tot: 1m1s | Loss: 0.086 | Train Acc: 97.036% (48518/5000 391/391 91  \n [================================================================>]  Step: 107ms | Tot: 10s973ms | Loss: 0.414 | Test Acc: 88.760% (8876/1000 100/100 \n\nEpoch: 110\n [================================================================>]  Step: 136ms | Tot: 1m2s | Loss: 0.077 | Train Acc: 97.396% (48698/5000 391/391 91  \n [================================================================>]  Step: 103ms | Tot: 11s203ms | Loss: 0.359 | Test Acc: 90.380% (9038/1000 100/100 \nSaving..\n\nEpoch: 111\n [================================================================>]  Step: 146ms | Tot: 1m1s | Loss: 0.078 | Train Acc: 97.396% (48698/5000 391/391 91  \n [================================================================>]  Step: 138ms | Tot: 11s139ms | Loss: 0.388 | Test Acc: 89.540% (8954/1000 100/100 \n\nEpoch: 112\n [================================================================>]  Step: 131ms | Tot: 1m1s | Loss: 0.078 | Train Acc: 97.408% (48704/5000 391/391 91  \n [================================================================>]  Step: 111ms | Tot: 11s465ms | Loss: 0.371 | Test Acc: 90.010% (9001/1000 100/100 =============================================================>.]  Step: 100ms | Tot: 11s353ms | Loss: 0.370 | Test Acc: 90.030% (8913/990 99/100 \n\nEpoch: 113\n [================================================================>]  Step: 130ms | Tot: 1m1s | Loss: 0.071 | Train Acc: 97.574% (48787/5000 391/391 91  \n [================================================================>]  Step: 102ms | Tot: 11s99ms | Loss: 0.366 | Test Acc: 89.900% (8990/1000 100/100 \n\nEpoch: 114\n [================================================================>]  Step: 131ms | Tot: 1m1s | Loss: 0.073 | Train Acc: 97.432% (48716/5000 391/391 91  \n [================================================================>]  Step: 100ms | Tot: 10s988ms | Loss: 0.414 | Test Acc: 89.360% (8936/1000 100/100 \n\nEpoch: 115\n [================================================================>]  Step: 132ms | Tot: 1m2s | Loss: 0.069 | Train Acc: 97.664% (48832/5000 391/391 91  \n [================================================================>]  Step: 105ms | Tot: 11s365ms | Loss: 0.352 | Test Acc: 90.510% (9051/1000 100/100 \nSaving..\n\nEpoch: 116\n [================================================================>]  Step: 138ms | Tot: 1m1s | Loss: 0.069 | Train Acc: 97.662% (48831/5000 391/391 91  \n [================================================================>]  Step: 103ms | Tot: 11s71ms | Loss: 0.378 | Test Acc: 90.140% (9014/1000 100/100 \n\nEpoch: 117\n [================================================================>]  Step: 131ms | Tot: 1m2s | Loss: 0.065 | Train Acc: 97.792% (48896/5000 391/391 91  \n [================================================================>]  Step: 110ms | Tot: 11s102ms | Loss: 0.383 | Test Acc: 90.150% (9015/1000 100/100 \n\nEpoch: 118\n [================================================================>]  Step: 143ms | Tot: 1m1s | Loss: 0.063 | Train Acc: 97.916% (48958/5000 391/391 91  \n [================================================================>]  Step: 103ms | Tot: 11s187ms | Loss: 0.398 | Test Acc: 89.360% (8936/1000 100/100 \n\nEpoch: 119\n [================================================================>]  Step: 124ms | Tot: 1m1s | Loss: 0.063 | Train Acc: 97.864% (48932/5000 391/391 91  \n [================================================================>]  Step: 102ms | Tot: 10s960ms | Loss: 0.374 | Test Acc: 90.090% (9009/1000 100/100 \n\nEpoch: 120\n [================================================================>]  Step: 134ms | Tot: 1m1s | Loss: 0.060 | Train Acc: 97.946% (48973/5000 391/391 91  \n [================================================================>]  Step: 103ms | Tot: 11s85ms | Loss: 0.417 | Test Acc: 89.280% (8928/1000 100/100 \n\nEpoch: 121\n [================================================================>]  Step: 140ms | Tot: 1m2s | Loss: 0.060 | Train Acc: 97.948% (48974/5000 391/391 91  \n [================================================================>]  Step: 100ms | Tot: 11s278ms | Loss: 0.376 | Test Acc: 90.540% (9054/1000 100/100 \nSaving..\n\nEpoch: 122\n [================================================================>]  Step: 131ms | Tot: 1m2s | Loss: 0.055 | Train Acc: 98.160% (49080/5000 391/391 91  \n [================================================================>]  Step: 106ms | Tot: 11s5ms | Loss: 0.377 | Test Acc: 90.460% (9046/1000 100/100 \n\nEpoch: 123\n [================================================================>]  Step: 135ms | Tot: 1m1s | Loss: 0.053 | Train Acc: 98.282% (49141/5000 391/391 91  \n [================================================================>]  Step: 104ms | Tot: 10s928ms | Loss: 0.362 | Test Acc: 90.670% (9067/1000 100/100 \nSaving..\n\nEpoch: 124\n [================================================================>]  Step: 130ms | Tot: 1m2s | Loss: 0.050 | Train Acc: 98.352% (49176/5000 391/391 91  \n [================================================================>]  Step: 100ms | Tot: 11s455ms | Loss: 0.380 | Test Acc: 90.490% (9049/1000 100/100 \n\nEpoch: 125\n [================================================================>]  Step: 177ms | Tot: 1m1s | Loss: 0.052 | Train Acc: 98.252% (49126/5000 391/391 91  \n [================================================================>]  Step: 102ms | Tot: 11s226ms | Loss: 0.385 | Test Acc: 90.000% (9000/1000 100/100 \n\nEpoch: 126\n [================================================================>]  Step: 130ms | Tot: 1m2s | Loss: 0.053 | Train Acc: 98.260% (49130/5000 391/391 91  \n [================================================================>]  Step: 102ms | Tot: 11s24ms | Loss: 0.385 | Test Acc: 90.320% (9032/1000 100/100 \n\nEpoch: 127\n [================================================================>]  Step: 135ms | Tot: 1m1s | Loss: 0.050 | Train Acc: 98.288% (49144/5000 391/391 91  \n [================================================================>]  Step: 108ms | Tot: 10s945ms | Loss: 0.401 | Test Acc: 90.060% (9006/1000 100/100 \n\nEpoch: 128\n [================================================================>]  Step: 121ms | Tot: 1m1s | Loss: 0.045 | Train Acc: 98.494% (49247/5000 391/391 91  \n [================================================================>]  Step: 113ms | Tot: 11s205ms | Loss: 0.380 | Test Acc: 90.750% (9075/1000 100/100 \nSaving..\n\nEpoch: 129\n [================================================================>]  Step: 145ms | Tot: 1m1s | Loss: 0.044 | Train Acc: 98.522% (49261/5000 391/391 91  \n [================================================================>]  Step: 106ms | Tot: 11s83ms | Loss: 0.372 | Test Acc: 90.780% (9078/1000 100/100 \nSaving..\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 96","metadata":{}},{"cell_type":"code","source":"import os\nimport sys\nimport uuid\nfrom math import ceil\n\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nimport torchvision\nimport torchvision.transforms as T\n\ntorch.backends.cudnn.benchmark = True\n\nhyp = {\n    'opt': {\n        'train_epochs': 37.0,\n        'batch_size': 1024,\n        'lr': 9.0,               # learning rate per 1024 examples\n        'momentum': 0.85,\n        'weight_decay': 0.012,   # weight decay per 1024 examples (decoupled from learning rate)\n        'bias_scaler': 64.0,     # scales up learning rate (but not weight decay) for BatchNorm biases\n        'label_smoothing': 0.2,\n        'whiten_bias_epochs': 3, # how many epochs to train the whitening layer bias before freezing\n    },\n    'aug': {\n        'flip': True,\n        'translate': 4,\n        'cutout': 12,\n    },\n    'net': {\n        'widths': {\n            'block1': 128,\n            'block2': 384,\n            'block3': 512,\n        },\n        'scaling_factor': 1/9,\n        'tta_level': 2,         # the level of test-time augmentation: 0=none, 1=mirror, 2=mirror+translate\n    },\n}\n\n#############################################\n#                DataLoader                 #\n#############################################\n\nCIFAR_MEAN = torch.tensor((0.4914, 0.4822, 0.4465))\nCIFAR_STD = torch.tensor((0.2470, 0.2435, 0.2616))\n\ndef batch_flip_lr(inputs):\n    flip_mask = (torch.rand(len(inputs), device=inputs.device) < 0.5).view(-1, 1, 1, 1)\n    return torch.where(flip_mask, inputs.flip(-1), inputs)\n\ndef batch_crop(images, crop_size):\n    r = (images.size(-1) - crop_size)//2\n    shifts = torch.randint(-r, r+1, size=(len(images), 2), device=images.device)\n    images_out = torch.empty((len(images), 3, crop_size, crop_size), device=images.device, dtype=images.dtype)\n    # The two cropping methods in this if-else produce equivalent results, but the second is faster for r > 2.\n    if r <= 2:\n        for sy in range(-r, r+1):\n            for sx in range(-r, r+1):\n                mask = (shifts[:, 0] == sy) & (shifts[:, 1] == sx)\n                images_out[mask] = images[mask, :, r+sy:r+sy+crop_size, r+sx:r+sx+crop_size]\n    else:\n        images_tmp = torch.empty((len(images), 3, crop_size, crop_size+2*r), device=images.device, dtype=images.dtype)\n        for s in range(-r, r+1):\n            mask = (shifts[:, 0] == s)\n            images_tmp[mask] = images[mask, :, r+s:r+s+crop_size, :]\n        for s in range(-r, r+1):\n            mask = (shifts[:, 1] == s)\n            images_out[mask] = images_tmp[mask, :, :, r+s:r+s+crop_size]\n    return images_out\n\ndef make_random_square_masks(inputs, size):\n    is_even = int(size % 2 == 0)\n    n,c,h,w = inputs.shape\n\n    # seed top-left corners of squares to cutout boxes from, in one dimension each\n    corner_y = torch.randint(0, h-size+1, size=(n,), device=inputs.device)\n    corner_x = torch.randint(0, w-size+1, size=(n,), device=inputs.device)\n\n    # measure distance, using the center as a reference point\n    corner_y_dists = torch.arange(h, device=inputs.device).view(1, 1, h, 1) - corner_y.view(-1, 1, 1, 1)\n    corner_x_dists = torch.arange(w, device=inputs.device).view(1, 1, 1, w) - corner_x.view(-1, 1, 1, 1)\n\n    mask_y = (corner_y_dists >= 0) * (corner_y_dists < size)\n    mask_x = (corner_x_dists >= 0) * (corner_x_dists < size)\n\n    final_mask = mask_y * mask_x\n\n    return final_mask\n\ndef batch_cutout(inputs, size):\n    cutout_masks = make_random_square_masks(inputs, size)\n    return inputs.masked_fill(cutout_masks, 0)\n\nclass CifarLoader:\n\n    def __init__(self, path, train=True, batch_size=500, aug=None, drop_last=None, shuffle=None, gpu=0):\n        data_path = os.path.join(path, 'train.pt' if train else 'test.pt')\n        if not os.path.exists(data_path):\n            dset = torchvision.datasets.CIFAR10(path, download=True, train=train)\n            images = torch.tensor(dset.data)\n            labels = torch.tensor(dset.targets)\n            torch.save({'images': images, 'labels': labels, 'classes': dset.classes}, data_path)\n\n        data = torch.load(data_path, map_location=torch.device(gpu))\n        self.images, self.labels, self.classes = data['images'], data['labels'], data['classes']\n        # It's faster to load+process uint8 data than to load preprocessed fp16 data\n        self.images = (self.images.half() / 255).permute(0, 3, 1, 2).to(memory_format=torch.channels_last)\n\n        self.normalize = T.Normalize(CIFAR_MEAN, CIFAR_STD)\n        self.proc_images = {} # Saved results of image processing to be done on the first epoch\n        self.epoch = 0\n\n        self.aug = aug or {}\n        for k in self.aug.keys():\n            assert k in ['flip', 'translate', 'cutout'], 'Unrecognized key: %s' % k\n\n        self.batch_size = batch_size\n        self.drop_last = train if drop_last is None else drop_last\n        self.shuffle = train if shuffle is None else shuffle\n\n    def __len__(self):\n        return len(self.images)//self.batch_size if self.drop_last else ceil(len(self.images)/self.batch_size)\n\n    def __iter__(self):\n\n        if self.epoch == 0:\n            images = self.proc_images['norm'] = self.normalize(self.images)\n            # Pre-flip images in order to do every-other epoch flipping scheme\n            if self.aug.get('flip', False):\n                images = self.proc_images['flip'] = batch_flip_lr(images)\n            # Pre-pad images to save time when doing random translation\n            pad = self.aug.get('translate', 0)\n            if pad > 0:\n                self.proc_images['pad'] = F.pad(images, (pad,)*4, 'reflect')\n\n        if self.aug.get('translate', 0) > 0:\n            images = batch_crop(self.proc_images['pad'], self.images.shape[-2])\n        elif self.aug.get('flip', False):\n            images = self.proc_images['flip']\n        else:\n            images = self.proc_images['norm']\n        # Flip all images together every other epoch. This increases diversity relative to random flipping\n        if self.aug.get('flip', False):\n            if self.epoch % 2 == 1:\n                images = images.flip(-1)\n        if self.aug.get('cutout', 0) > 0:\n            images = batch_cutout(images, self.aug['cutout'])\n\n        self.epoch += 1\n\n        indices = (torch.randperm if self.shuffle else torch.arange)(len(images), device=images.device)\n        for i in range(len(self)):\n            idxs = indices[i*self.batch_size:(i+1)*self.batch_size]\n            yield (images[idxs], self.labels[idxs])\n\n#############################################\n#            Network Components             #\n#############################################\n\nclass Flatten(nn.Module):\n    def forward(self, x):\n        return x.view(x.size(0), -1)\n\nclass Mul(nn.Module):\n    def __init__(self, scale):\n        super().__init__()\n        self.scale = scale\n    def forward(self, x):\n        return x * self.scale\n\nclass BatchNorm(nn.BatchNorm2d):\n    def __init__(self, num_features, eps=1e-12,\n                 weight=False, bias=True):\n        super().__init__(num_features, eps=eps)\n        self.weight.requires_grad = weight\n        self.bias.requires_grad = bias\n        # Note that PyTorch already initializes the weights to one and bias to zero\n\nclass Conv(nn.Conv2d):\n    def __init__(self, in_channels, out_channels, kernel_size=3, padding='same', bias=False):\n        super().__init__(in_channels, out_channels, kernel_size=kernel_size, padding=padding, bias=bias)\n\n    def reset_parameters(self):\n        super().reset_parameters()\n        if self.bias is not None:\n            self.bias.data.zero_()\n        w = self.weight.data\n        torch.nn.init.dirac_(w[:w.size(1)])\n\nclass ConvGroup(nn.Module):\n    def __init__(self, channels_in, channels_out):\n        super().__init__()\n        self.conv1 = Conv(channels_in,  channels_out)\n        self.pool = nn.MaxPool2d(2)\n        self.norm1 = BatchNorm(channels_out)\n        self.conv2 = Conv(channels_out, channels_out)\n        self.norm2 = BatchNorm(channels_out)\n        self.conv3 = Conv(channels_out, channels_out)\n        self.norm3 = BatchNorm(channels_out)\n        self.activ = nn.GELU()\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.pool(x)\n        x = self.norm1(x)\n        x = self.activ(x)\n        x0 = x\n        x = self.conv2(x)\n        x = self.norm2(x)\n        x = self.activ(x)\n        x = self.conv3(x)\n        x = self.norm3(x)\n        x = x + x0\n        x = self.activ(x)\n        return x\n\n#############################################\n#            Network Definition             #\n#############################################\n\ndef make_net():\n    widths = hyp['net']['widths']\n    whiten_kernel_size = 2\n    whiten_width = 2 * 3 * whiten_kernel_size**2\n    net = nn.Sequential(\n        Conv(3, whiten_width, whiten_kernel_size, padding=0, bias=True),\n        nn.GELU(),\n        ConvGroup(whiten_width,     widths['block1']),\n        ConvGroup(widths['block1'], widths['block2']),\n        ConvGroup(widths['block2'], widths['block3']),\n        nn.MaxPool2d(3),\n        Flatten(),\n        nn.Linear(widths['block3'], 10, bias=False),\n        Mul(hyp['net']['scaling_factor']),\n    )\n    net[0].weight.requires_grad = False\n    net = net.half().cuda()\n    net = net.to(memory_format=torch.channels_last)\n    for mod in net.modules():\n        if isinstance(mod, BatchNorm):\n            mod.float()\n    return net\n\n#############################################\n#       Whitening Conv Initialization       #\n#############################################\n\ndef get_patches(x, patch_shape):\n    c, (h, w) = x.shape[1], patch_shape\n    return x.unfold(2,h,1).unfold(3,w,1).transpose(1,3).reshape(-1,c,h,w).float()\n\ndef get_whitening_parameters(patches):\n    n,c,h,w = patches.shape\n    patches_flat = patches.view(n, -1)\n    est_patch_covariance = (patches_flat.T @ patches_flat) / n\n    eigenvalues, eigenvectors = torch.linalg.eigh(est_patch_covariance, UPLO='U')\n    return eigenvalues.flip(0).view(-1, 1, 1, 1), eigenvectors.T.reshape(c*h*w,c,h,w).flip(0)\n\ndef init_whitening_conv(layer, train_set, eps=5e-4):\n    patches = get_patches(train_set, patch_shape=layer.weight.data.shape[2:])\n    eigenvalues, eigenvectors = get_whitening_parameters(patches)\n    eigenvectors_scaled = eigenvectors / torch.sqrt(eigenvalues + eps)\n    layer.weight.data[:] = torch.cat((eigenvectors_scaled, -eigenvectors_scaled))\n\n############################################\n#                Lookahead                 #\n############################################\n\nclass LookaheadState:\n    def __init__(self, net):\n        self.net_ema = {k: v.clone() for k, v in net.state_dict().items()}\n\n    def update(self, net, decay):\n        for ema_param, net_param in zip(self.net_ema.values(), net.state_dict().values()):\n            if net_param.dtype in (torch.half, torch.float):\n                ema_param.lerp_(net_param, 1-decay)\n                net_param.copy_(ema_param)\n\n############################################\n#                 Logging                  #\n############################################\n\ndef print_columns(columns_list, is_head=False, is_final_entry=False):\n    print_string = ''\n    for col in columns_list:\n        print_string += '|  %s  ' % col\n    print_string += '|'\n    if is_head:\n        print('-'*len(print_string))\n    print(print_string)\n    if is_head or is_final_entry:\n        print('-'*len(print_string))\n\nlogging_columns_list = ['run   ', 'epoch', 'train_loss', 'train_acc', 'val_acc', 'tta_val_acc', 'total_time_seconds']\ndef print_training_details(variables, is_final_entry):\n    formatted = []\n    for col in logging_columns_list:\n        var = variables.get(col.strip(), None)\n        if type(var) in (int, str):\n            res = str(var)\n        elif type(var) is float:\n            res = '{:0.4f}'.format(var)\n        else:\n            assert var is None\n            res = ''\n        formatted.append(res.rjust(len(col)))\n    print_columns(formatted, is_final_entry=is_final_entry)\n\n############################################\n#               Evaluation                 #\n############################################\n\ndef infer(model, loader, tta_level=0):\n\n    # Test-time augmentation strategy (for tta_level=2):\n    # 1. Flip/mirror the image left-to-right (50% of the time).\n    # 2. Translate the image by one pixel either up-and-left or down-and-right (50% of the time,\n    #    i.e. both happen 25% of the time).\n    #\n    # This creates 6 views per image (left/right times the two translations and no-translation),\n    # which we evaluate and then weight according to the given probabilities.\n\n    def infer_basic(inputs, net):\n        return net(inputs).clone()\n\n    def infer_mirror(inputs, net):\n        return 0.5 * net(inputs) + 0.5 * net(inputs.flip(-1))\n\n    def infer_mirror_translate(inputs, net):\n        logits = infer_mirror(inputs, net)\n        pad = 1\n        padded_inputs = F.pad(inputs, (pad,)*4, 'reflect')\n        inputs_translate_list = [\n            padded_inputs[:, :, 0:32, 0:32],\n            padded_inputs[:, :, 2:34, 2:34],\n        ]\n        logits_translate_list = [infer_mirror(inputs_translate, net)\n                                 for inputs_translate in inputs_translate_list]\n        logits_translate = torch.stack(logits_translate_list).mean(0)\n        return 0.5 * logits + 0.5 * logits_translate\n\n    model.eval()\n    test_images = loader.normalize(loader.images)\n    infer_fn = [infer_basic, infer_mirror, infer_mirror_translate][tta_level]\n    with torch.no_grad():\n        return torch.cat([infer_fn(inputs, model) for inputs in test_images.split(2000)])\n\ndef evaluate(model, loader, tta_level=0):\n    logits = infer(model, loader, tta_level)\n    return (logits.argmax(1) == loader.labels).float().mean().item()\n\n############################################\n#                Training                  #\n############################################\n\ndef main(run):\n\n    batch_size = hyp['opt']['batch_size']\n    epochs = hyp['opt']['train_epochs']\n    momentum = hyp['opt']['momentum']\n    # Assuming gradients are constant in time, for Nesterov momentum, the below ratio is how much\n    # larger the default steps will be than the underlying per-example gradients. We divide the\n    # learning rate by this ratio in order to ensure steps are the same scale as gradients, regardless\n    # of the choice of momentum.\n    kilostep_scale = 1024 * (1 + 1 / (1 - momentum))\n    lr = hyp['opt']['lr'] / kilostep_scale # un-decoupled learning rate for PyTorch SGD\n    wd = hyp['opt']['weight_decay'] * batch_size / kilostep_scale\n    lr_biases = lr * hyp['opt']['bias_scaler']\n\n    loss_fn = nn.CrossEntropyLoss(label_smoothing=hyp['opt']['label_smoothing'], reduction='none')\n    test_loader = CifarLoader('cifar10', train=False, batch_size=2000)\n    train_loader = CifarLoader('cifar10', train=True, batch_size=batch_size, aug=hyp['aug'])\n    if run == 'warmup':\n        # The only purpose of the first run is to warmup, so we can use dummy data\n        train_loader.labels = torch.randint(0, 10, size=(len(train_loader.labels),), device=train_loader.labels.device)\n    total_train_steps = ceil(len(train_loader) * epochs)\n\n    model = make_net()\n    current_steps = 0\n\n    norm_biases = [p for k, p in model.named_parameters() if 'norm' in k and p.requires_grad]\n    other_params = [p for k, p in model.named_parameters() if 'norm' not in k and p.requires_grad]\n    param_configs = [dict(params=norm_biases, lr=lr_biases, weight_decay=wd/lr_biases),\n                     dict(params=other_params, lr=lr, weight_decay=wd/lr)]\n    optimizer = torch.optim.SGD(param_configs, momentum=momentum, nesterov=True)\n\n    def get_lr(step):\n        warmup_steps = int(total_train_steps * 0.1)\n        warmdown_steps = total_train_steps - warmup_steps\n        if step < warmup_steps:\n            frac = step / warmup_steps\n            return 0.2 * (1 - frac) + 1.0 * frac\n        else:\n            frac = (total_train_steps - step) / warmdown_steps\n            return frac\n    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, get_lr)\n\n    alpha_schedule = 0.95**5 * (torch.arange(total_train_steps+1) / total_train_steps)**3\n    lookahead_state = LookaheadState(model)\n\n    # For accurately timing GPU code\n    starter = torch.cuda.Event(enable_timing=True)\n    ender = torch.cuda.Event(enable_timing=True)\n    total_time_seconds = 0.0\n\n    # Initialize the whitening layer using training images\n    starter.record()\n    train_images = train_loader.normalize(train_loader.images[:5000])\n    init_whitening_conv(model[0], train_images)\n    ender.record()\n    torch.cuda.synchronize()\n    total_time_seconds += 1e-3 * starter.elapsed_time(ender)\n\n    for epoch in range(ceil(epochs)):\n\n        model[0].bias.requires_grad = (epoch < hyp['opt']['whiten_bias_epochs'])\n\n        ####################\n        #     Training     #\n        ####################\n\n        starter.record()\n\n        model.train()\n        for inputs, labels in train_loader:\n\n            outputs = model(inputs)\n            loss = loss_fn(outputs, labels).sum()\n            optimizer.zero_grad(set_to_none=True)\n            loss.backward()\n            optimizer.step()\n            scheduler.step()\n\n            current_steps += 1\n\n            if current_steps % 5 == 0:\n                lookahead_state.update(model, decay=alpha_schedule[current_steps].item())\n\n            if current_steps >= total_train_steps:\n                if lookahead_state is not None:\n                    lookahead_state.update(model, decay=1.0)\n                break\n\n        ender.record()\n        torch.cuda.synchronize()\n        total_time_seconds += 1e-3 * starter.elapsed_time(ender)\n\n        ####################\n        #    Evaluation    #\n        ####################\n\n        # Save the accuracy and loss from the last training batch of the epoch\n        train_acc = (outputs.detach().argmax(1) == labels).float().mean().item()\n        train_loss = loss.item() / batch_size\n        val_acc = evaluate(model, test_loader, tta_level=0)\n        print_training_details(locals(), is_final_entry=False)\n        run = None # Only print the run number once\n\n    ####################\n    #  TTA Evaluation  #\n    ####################\n\n    starter.record()\n    tta_val_acc = evaluate(model, test_loader, tta_level=hyp['net']['tta_level'])\n    ender.record()\n    torch.cuda.synchronize()\n    total_time_seconds += 1e-3 * starter.elapsed_time(ender)\n\n    epoch = 'eval'\n    print_training_details(locals(), is_final_entry=True)\n\n    return tta_val_acc\n\nif __name__ == \"__main__\":\n    with open(sys.argv[0]) as f:\n        code = f.read()\n\n    print_columns(logging_columns_list, is_head=True)\n    #main('warmup')\n    accs = torch.tensor([main(run) for run in range(25)])\n    print('Mean: %.4f    Std: %.4f' % (accs.mean(), accs.std()))\n\n    log = {'code': code, 'accs': accs}\n    log_dir = os.path.join('logs', str(uuid.uuid4()))\n    os.makedirs(log_dir, exist_ok=True)\n    log_path = os.path.join(log_dir, 'log.pt')\n    print(os.path.abspath(log_path))\n    torch.save(log, os.path.join(log_dir, 'log.pt'))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T04:58:35.484145Z","iopub.execute_input":"2024-12-04T04:58:35.484831Z","iopub.status.idle":"2024-12-04T05:06:31.412386Z","shell.execute_reply.started":"2024-12-04T04:58:35.484776Z","shell.execute_reply":"2024-12-04T05:06:31.410873Z"}},"outputs":[{"name":"stdout","text":"------------------------------------------------------------------------------------------------------\n|  run     |  epoch  |  train_loss  |  train_acc  |  val_acc  |  tta_val_acc  |  total_time_seconds  |\n------------------------------------------------------------------------------------------------------\nDownloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to cifar10/cifar-10-python.tar.gz\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 170498071/170498071 [00:02<00:00, 63600621.13it/s]\n","output_type":"stream"},{"name":"stdout","text":"Extracting cifar10/cifar-10-python.tar.gz to cifar10\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_23/1863641720.py:117: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  data = torch.load(data_path, map_location=torch.device(gpu))\n","output_type":"stream"},{"name":"stdout","text":"Files already downloaded and verified\n|       0  |      0  |      1.6826  |     0.5381  |   0.5500  |               |             11.1038  |\n|          |      1  |      1.3838  |     0.7256  |   0.6558  |               |             19.2859  |\n|          |      2  |      1.3447  |     0.7520  |   0.7455  |               |             27.7728  |\n|          |      3  |      1.2832  |     0.7842  |   0.7823  |               |             36.0952  |\n|          |      4  |      1.2422  |     0.7988  |   0.8261  |               |             44.4373  |\n|          |      5  |      1.2373  |     0.7930  |   0.7957  |               |             52.9464  |\n|          |      6  |      1.1963  |     0.8184  |   0.8061  |               |             61.6216  |\n|          |      7  |      1.2090  |     0.8252  |   0.8142  |               |             70.3833  |\n|          |      8  |      1.2197  |     0.8193  |   0.8071  |               |             79.2256  |\n|          |      9  |      1.1963  |     0.8340  |   0.8410  |               |             88.1545  |\n|          |     10  |      1.1729  |     0.8281  |   0.8514  |               |             97.0436  |\n|          |     11  |      1.1455  |     0.8574  |   0.8461  |               |            105.8435  |\n|          |     12  |      1.1523  |     0.8447  |   0.8305  |               |            114.6144  |\n|          |     13  |      1.1543  |     0.8447  |   0.8624  |               |            123.3790  |\n|          |     14  |      1.1631  |     0.8408  |   0.8695  |               |            132.1317  |\n|          |     15  |      1.1426  |     0.8438  |   0.8537  |               |            140.8886  |\n|          |     16  |      1.1436  |     0.8438  |   0.8440  |               |            149.6387  |\n|          |     17  |      1.1240  |     0.8594  |   0.8818  |               |            158.4149  |\n|          |     18  |      1.1211  |     0.8643  |   0.8706  |               |            167.1958  |\n|          |     19  |      1.0918  |     0.8848  |   0.8788  |               |            175.9763  |\n|          |     20  |      1.1074  |     0.8643  |   0.8750  |               |            184.7444  |\n|          |     21  |      1.0791  |     0.8906  |   0.9054  |               |            193.5151  |\n|          |     22  |      1.0869  |     0.8896  |   0.8950  |               |            202.2797  |\n|          |     23  |      1.0654  |     0.8809  |   0.9139  |               |            211.0480  |\n|          |     24  |      1.0684  |     0.8965  |   0.9225  |               |            219.8216  |\n|          |     25  |      1.0400  |     0.9053  |   0.9088  |               |            228.6016  |\n|          |     26  |      1.0107  |     0.9209  |   0.9145  |               |            237.3640  |\n|          |     27  |      1.0059  |     0.9355  |   0.9260  |               |            246.1344  |\n|          |     28  |      0.9941  |     0.9316  |   0.9315  |               |            254.8986  |\n|          |     29  |      1.0000  |     0.9316  |   0.9402  |               |            263.6704  |\n|          |     30  |      0.9678  |     0.9463  |   0.9341  |               |            272.4179  |\n|          |     31  |      0.9629  |     0.9502  |   0.9405  |               |            281.1884  |\n|          |     32  |      0.9448  |     0.9600  |   0.9455  |               |            289.9426  |\n|          |     33  |      0.9170  |     0.9814  |   0.9508  |               |            298.6963  |\n|          |     34  |      0.9136  |     0.9775  |   0.9537  |               |            307.4439  |\n|          |     35  |      0.9219  |     0.9736  |   0.9547  |               |            316.1921  |\n|          |     36  |      0.9043  |     0.9844  |   0.9553  |               |            324.9390  |\n|          |   eval  |      0.9043  |     0.9844  |   0.9553  |       0.9602  |            329.0058  |\n------------------------------------------------------------------------------------------------------\n|       1  |      0  |      1.8301  |     0.4639  |   0.4298  |               |              8.9305  |\n|          |      1  |      1.5586  |     0.6279  |   0.5516  |               |             18.0130  |\n|          |      2  |      1.3789  |     0.7314  |   0.7192  |               |             27.1229  |\n|          |      3  |      1.2959  |     0.7559  |   0.7670  |               |             35.8811  |\n|          |      4  |      1.2607  |     0.7793  |   0.7495  |               |             44.6276  |\n|          |      5  |      1.2305  |     0.7969  |   0.7883  |               |             53.3734  |\n|          |      6  |      1.2070  |     0.8203  |   0.7195  |               |             62.1278  |\n|          |      7  |      1.2217  |     0.8135  |   0.8172  |               |             70.8989  |\n|          |      8  |      1.2158  |     0.7998  |   0.7968  |               |             79.6534  |\n|          |      9  |      1.1875  |     0.8232  |   0.8484  |               |             88.4149  |\n|          |     10  |      1.1885  |     0.8291  |   0.8371  |               |             97.1757  |\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[4], line 493\u001b[0m\n\u001b[1;32m    491\u001b[0m print_columns(logging_columns_list, is_head\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    492\u001b[0m \u001b[38;5;66;03m#main('warmup')\u001b[39;00m\n\u001b[0;32m--> 493\u001b[0m accs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([main(run) \u001b[38;5;28;01mfor\u001b[39;00m run \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m25\u001b[39m)])\n\u001b[1;32m    494\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMean: \u001b[39m\u001b[38;5;132;01m%.4f\u001b[39;00m\u001b[38;5;124m    Std: \u001b[39m\u001b[38;5;132;01m%.4f\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (accs\u001b[38;5;241m.\u001b[39mmean(), accs\u001b[38;5;241m.\u001b[39mstd()))\n\u001b[1;32m    496\u001b[0m log \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcode\u001b[39m\u001b[38;5;124m'\u001b[39m: code, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccs\u001b[39m\u001b[38;5;124m'\u001b[39m: accs}\n","Cell \u001b[0;32mIn[4], line 493\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    491\u001b[0m print_columns(logging_columns_list, is_head\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    492\u001b[0m \u001b[38;5;66;03m#main('warmup')\u001b[39;00m\n\u001b[0;32m--> 493\u001b[0m accs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m run \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m25\u001b[39m)])\n\u001b[1;32m    494\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMean: \u001b[39m\u001b[38;5;132;01m%.4f\u001b[39;00m\u001b[38;5;124m    Std: \u001b[39m\u001b[38;5;132;01m%.4f\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (accs\u001b[38;5;241m.\u001b[39mmean(), accs\u001b[38;5;241m.\u001b[39mstd()))\n\u001b[1;32m    496\u001b[0m log \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcode\u001b[39m\u001b[38;5;124m'\u001b[39m: code, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccs\u001b[39m\u001b[38;5;124m'\u001b[39m: accs}\n","Cell \u001b[0;32mIn[4], line 440\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(run)\u001b[0m\n\u001b[1;32m    437\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m    438\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m inputs, labels \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[0;32m--> 440\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    441\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss_fn(outputs, labels)\u001b[38;5;241m.\u001b[39msum()\n\u001b[1;32m    442\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad(set_to_none\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/container.py:219\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 219\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","Cell \u001b[0;32mIn[4], line 225\u001b[0m, in \u001b[0;36mConvGroup.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    223\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactiv(x)\n\u001b[1;32m    224\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv3(x)\n\u001b[0;32m--> 225\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm3\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    226\u001b[0m x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m x0\n\u001b[1;32m    227\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactiv(x)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py:156\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrack_running_stats:\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;66;03m# TODO: if statement only here to tell the jit to skip emitting this when it is None\u001b[39;00m\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_batches_tracked \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# type: ignore[has-type]\u001b[39;00m\n\u001b[0;32m--> 156\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_batches_tracked\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[has-type]\u001b[39;00m\n\u001b[1;32m    157\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmomentum \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# use cumulative moving average\u001b[39;00m\n\u001b[1;32m    158\u001b[0m             exponential_average_factor \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_batches_tracked)\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":4},{"cell_type":"code","source":"import os\nfrom tqdm import tqdm\nimport torch\nfrom torch.utils.data import DataLoader\nimport torchvision\nimport torchvision.transforms as transforms\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\nfrom fvcore.nn import FlopCountAnalysis, flop_count_table\nfrom torchinfo import summary\nfrom PIL import Image\n\n# Define the ResidualBlock as an updated architecture\nclass ResidualBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n        super(ResidualBlock, self).__init__()\n        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(out_channels)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(out_channels)\n        self.downsample = downsample\n\n    def forward(self, x):\n        residual = x\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n        out = self.conv2(out)\n        out = self.bn2(out)\n        if self.downsample:\n            residual = self.downsample(x)\n        out += residual\n        out = self.relu(out)\n        return out\n\nclass ResNet(nn.Module):\n    def __init__(self, block, layers, num_classes=10):\n        super(ResNet, self).__init__()\n        self.in_channels = 64\n        self.conv = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn = nn.BatchNorm2d(64)\n        self.relu = nn.ReLU(inplace=True)\n        self.layer1 = self.make_layer(block, 64, layers[0])\n        self.layer2 = self.make_layer(block, 128, layers[1], stride=2)\n        self.layer3 = self.make_layer(block, 256, layers[2], stride=2)\n        self.layer4 = self.make_layer(block, 512, layers[3], stride=2)\n        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n        self.fc = nn.Linear(512, num_classes)\n\n    def make_layer(self, block, out_channels, blocks, stride=1):\n        downsample = None\n        if stride != 1 or self.in_channels != out_channels:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(out_channels)\n            )\n        layers = []\n        layers.append(block(self.in_channels, out_channels, stride, downsample))\n        self.in_channels = out_channels\n        for _ in range(1, blocks):\n            layers.append(block(out_channels, out_channels))\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.bn(x)\n        x = self.relu(x)\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        x = self.avg_pool(x)\n        x = torch.flatten(x, 1)\n        x = self.fc(x)\n        return x\n\n# Training configurations\ntransform_train = transforms.Compose([\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomCrop(32, padding=4),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=(0.4914, 0.4822, 0.4465), std=(0.2023, 0.1994, 0.2010)),\n])\n\ntransform_test = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize(mean=(0.4914, 0.4822, 0.4465), std=(0.2023, 0.1994, 0.2010)),\n])\n\ntrain_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\ntest_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n\ntrain_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=2)\ntest_loader = DataLoader(test_dataset, batch_size=100, shuffle=False, num_workers=2)\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Initialize the ResNet model\nmodel = ResNet(ResidualBlock, [2, 2, 2, 2]).to(device)\n\n# Calculate FLOPs and Params\ndummy_input = torch.randn(1, 3, 32, 32).to(device)\nflops = FlopCountAnalysis(model, dummy_input)\nprint(\"FLOPs and Parameters BEFORE Training:\")\nprint(flop_count_table(flops))\nprint(f\"Total Parameters: {sum(p.numel() for p in model.parameters())}\")\n\n# Loss, optimizer, scheduler\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-4)\n\n# Training loop\ndef train(epoch):\n    model.train()\n    running_loss = 0.0\n    for inputs, targets in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{50}\"):\n        inputs, targets = inputs.to(device), targets.to(device)\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs, targets)\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item()\n    print(f\"Training Loss: {running_loss / len(train_loader):.4f}\")\n\n# Test loop\ndef test():\n    model.eval()\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for inputs, targets in test_loader:\n            inputs, targets = inputs.to(device), targets.to(device)\n            outputs = model(inputs)\n            _, predicted = outputs.max(1)\n            total += targets.size(0)\n            correct += predicted.eq(targets).sum().item()\n    print(f\"Test Accuracy: {100. * correct / total:.2f}%\")\n\n# Train and save model\nfor epoch in range(1):\n    train(epoch)\n    test()\n\nprint(\"FLOPs and Parameters AFTER Training:\")\nprint(flop_count_table(flops))\nprint(f\"Total Parameters AFTER Training: {sum(p.numel() for p in model.parameters())}\")\n\ntorch.save(model.state_dict(), \"cifar_resnet_model.pth\")\nprint(\"MODEL SAVED\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T07:12:22.816337Z","iopub.execute_input":"2024-12-04T07:12:22.816655Z","iopub.status.idle":"2024-12-04T07:13:10.609605Z","shell.execute_reply.started":"2024-12-04T07:12:22.816623Z","shell.execute_reply":"2024-12-04T07:13:10.608420Z"}},"outputs":[{"name":"stdout","text":"Files already downloaded and verified\nFiles already downloaded and verified\nFLOPs and Parameters BEFORE Training:\n| module                 | #parameters or shape   | #flops     |\n|:-----------------------|:-----------------------|:-----------|\n| model                  | 11.174M                | 0.559G     |\n|  conv                  |  1.728K                |  1.769M    |\n|   conv.weight          |   (64, 3, 3, 3)        |            |\n|  bn                    |  0.128K                |  0.328M    |\n|   bn.weight            |   (64,)                |            |\n|   bn.bias              |   (64,)                |            |\n|  layer1                |  0.148M                |  0.152G    |\n|   layer1.0             |   73.984K              |   76.153M  |\n|    layer1.0.conv1      |    36.864K             |    37.749M |\n|    layer1.0.bn1        |    0.128K              |    0.328M  |\n|    layer1.0.conv2      |    36.864K             |    37.749M |\n|    layer1.0.bn2        |    0.128K              |    0.328M  |\n|   layer1.1             |   73.984K              |   76.153M  |\n|    layer1.1.conv1      |    36.864K             |    37.749M |\n|    layer1.1.bn1        |    0.128K              |    0.328M  |\n|    layer1.1.conv2      |    36.864K             |    37.749M |\n|    layer1.1.bn2        |    0.128K              |    0.328M  |\n|  layer2                |  0.526M                |  0.135G    |\n|   layer2.0             |   0.23M                |   59.212M  |\n|    layer2.0.conv1      |    73.728K             |    18.874M |\n|    layer2.0.bn1        |    0.256K              |    0.164M  |\n|    layer2.0.conv2      |    0.147M              |    37.749M |\n|    layer2.0.bn2        |    0.256K              |    0.164M  |\n|    layer2.0.downsample |    8.448K              |    2.261M  |\n|   layer2.1             |   0.295M               |   75.825M  |\n|    layer2.1.conv1      |    0.147M              |    37.749M |\n|    layer2.1.bn1        |    0.256K              |    0.164M  |\n|    layer2.1.conv2      |    0.147M              |    37.749M |\n|    layer2.1.bn2        |    0.256K              |    0.164M  |\n|  layer3                |  2.1M                  |  0.135G    |\n|   layer3.0             |   0.919M               |   58.966M  |\n|    layer3.0.conv1      |    0.295M              |    18.874M |\n|    layer3.0.bn1        |    0.512K              |    81.92K  |\n|    layer3.0.conv2      |    0.59M               |    37.749M |\n|    layer3.0.bn2        |    0.512K              |    81.92K  |\n|    layer3.0.downsample |    33.28K              |    2.179M  |\n|   layer3.1             |   1.181M               |   75.661M  |\n|    layer3.1.conv1      |    0.59M               |    37.749M |\n|    layer3.1.bn1        |    0.512K              |    81.92K  |\n|    layer3.1.conv2      |    0.59M               |    37.749M |\n|    layer3.1.bn2        |    0.512K              |    81.92K  |\n|  layer4                |  8.394M                |  0.134G    |\n|   layer4.0             |   3.673M               |   58.843M  |\n|    layer4.0.conv1      |    1.18M               |    18.874M |\n|    layer4.0.bn1        |    1.024K              |    40.96K  |\n|    layer4.0.conv2      |    2.359M              |    37.749M |\n|    layer4.0.bn2        |    1.024K              |    40.96K  |\n|    layer4.0.downsample |    0.132M              |    2.138M  |\n|   layer4.1             |   4.721M               |   75.579M  |\n|    layer4.1.conv1      |    2.359M              |    37.749M |\n|    layer4.1.bn1        |    1.024K              |    40.96K  |\n|    layer4.1.conv2      |    2.359M              |    37.749M |\n|    layer4.1.bn2        |    1.024K              |    40.96K  |\n|  fc                    |  5.13K                 |  5.12K     |\n|   fc.weight            |   (10, 512)            |            |\n|   fc.bias              |   (10,)                |            |\n|  avg_pool              |                        |  8.192K    |\nTotal Parameters: 11173962\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/50: 100%|██████████| 391/391 [00:42<00:00,  9.20it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 1.4544\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 57.75%\nFLOPs and Parameters AFTER Training:\n| module                 | #parameters or shape   | #flops     |\n|:-----------------------|:-----------------------|:-----------|\n| model                  | 11.174M                | 0.559G     |\n|  conv                  |  1.728K                |  1.769M    |\n|   conv.weight          |   (64, 3, 3, 3)        |            |\n|  bn                    |  0.128K                |  0.328M    |\n|   bn.weight            |   (64,)                |            |\n|   bn.bias              |   (64,)                |            |\n|  layer1                |  0.148M                |  0.152G    |\n|   layer1.0             |   73.984K              |   76.153M  |\n|    layer1.0.conv1      |    36.864K             |    37.749M |\n|    layer1.0.bn1        |    0.128K              |    0.328M  |\n|    layer1.0.conv2      |    36.864K             |    37.749M |\n|    layer1.0.bn2        |    0.128K              |    0.328M  |\n|   layer1.1             |   73.984K              |   76.153M  |\n|    layer1.1.conv1      |    36.864K             |    37.749M |\n|    layer1.1.bn1        |    0.128K              |    0.328M  |\n|    layer1.1.conv2      |    36.864K             |    37.749M |\n|    layer1.1.bn2        |    0.128K              |    0.328M  |\n|  layer2                |  0.526M                |  0.135G    |\n|   layer2.0             |   0.23M                |   59.212M  |\n|    layer2.0.conv1      |    73.728K             |    18.874M |\n|    layer2.0.bn1        |    0.256K              |    0.164M  |\n|    layer2.0.conv2      |    0.147M              |    37.749M |\n|    layer2.0.bn2        |    0.256K              |    0.164M  |\n|    layer2.0.downsample |    8.448K              |    2.261M  |\n|   layer2.1             |   0.295M               |   75.825M  |\n|    layer2.1.conv1      |    0.147M              |    37.749M |\n|    layer2.1.bn1        |    0.256K              |    0.164M  |\n|    layer2.1.conv2      |    0.147M              |    37.749M |\n|    layer2.1.bn2        |    0.256K              |    0.164M  |\n|  layer3                |  2.1M                  |  0.135G    |\n|   layer3.0             |   0.919M               |   58.966M  |\n|    layer3.0.conv1      |    0.295M              |    18.874M |\n|    layer3.0.bn1        |    0.512K              |    81.92K  |\n|    layer3.0.conv2      |    0.59M               |    37.749M |\n|    layer3.0.bn2        |    0.512K              |    81.92K  |\n|    layer3.0.downsample |    33.28K              |    2.179M  |\n|   layer3.1             |   1.181M               |   75.661M  |\n|    layer3.1.conv1      |    0.59M               |    37.749M |\n|    layer3.1.bn1        |    0.512K              |    81.92K  |\n|    layer3.1.conv2      |    0.59M               |    37.749M |\n|    layer3.1.bn2        |    0.512K              |    81.92K  |\n|  layer4                |  8.394M                |  0.134G    |\n|   layer4.0             |   3.673M               |   58.843M  |\n|    layer4.0.conv1      |    1.18M               |    18.874M |\n|    layer4.0.bn1        |    1.024K              |    40.96K  |\n|    layer4.0.conv2      |    2.359M              |    37.749M |\n|    layer4.0.bn2        |    1.024K              |    40.96K  |\n|    layer4.0.downsample |    0.132M              |    2.138M  |\n|   layer4.1             |   4.721M               |   75.579M  |\n|    layer4.1.conv1      |    2.359M              |    37.749M |\n|    layer4.1.bn1        |    1.024K              |    40.96K  |\n|    layer4.1.conv2      |    2.359M              |    37.749M |\n|    layer4.1.bn2        |    1.024K              |    40.96K  |\n|  fc                    |  5.13K                 |  5.12K     |\n|   fc.weight            |   (10, 512)            |            |\n|   fc.bias              |   (10,)                |            |\n|  avg_pool              |                        |  8.192K    |\nTotal Parameters AFTER Training: 11173962\nMODEL SAVED\n","output_type":"stream"}],"execution_count":42},{"cell_type":"code","source":"import os\nfrom tqdm import tqdm\nimport torch\nfrom torch.utils.data import DataLoader\nimport torchvision\nimport torchvision.transforms as transforms\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\nfrom fvcore.nn import FlopCountAnalysis, flop_count_table\n\n# Define a lightweight ResidualBlock\nclass ResidualBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n        super(ResidualBlock, self).__init__()\n        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(out_channels)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(out_channels)\n        self.downsample = downsample\n\n    def forward(self, x):\n        residual = x\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n        out = self.conv2(out)\n        out = self.bn2(out)\n        if self.downsample:\n            residual = self.downsample(x)\n        out += residual\n        out = self.relu(out)\n        return out\n\nclass MidResNet(nn.Module):\n    def __init__(self, block, layers, num_classes=10):\n        super(MidResNet, self).__init__()\n        self.in_channels = 32  # Increased from 16 to 32\n        self.conv = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn = nn.BatchNorm2d(32)\n        self.relu = nn.ReLU(inplace=True)\n        self.layer1 = self.make_layer(block, 64, layers[0])  # Increased channels\n        self.layer2 = self.make_layer(block, 128, layers[1], stride=2)  # Increased channels\n        self.layer3 = self.make_layer(block, 256, layers[2], stride=2)  # Increased channels\n        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n        self.fc = nn.Linear(256, num_classes)  # Increased fully connected input size\n\n    def make_layer(self, block, out_channels, blocks, stride=1):\n        downsample = None\n        if stride != 1 or self.in_channels != out_channels:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(out_channels)\n            )\n        layers = []\n        layers.append(block(self.in_channels, out_channels, stride, downsample))\n        self.in_channels = out_channels\n        for _ in range(1, blocks):\n            layers.append(block(out_channels, out_channels))\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.bn(x)\n        x = self.relu(x)\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.avg_pool(x)\n        x = torch.flatten(x, 1)\n        x = self.fc(x)\n        return x\n\n# Training configurations\ntransform_train = transforms.Compose([\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomCrop(32, padding=4),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=(0.4914, 0.4822, 0.4465), std=(0.2023, 0.1994, 0.2010)),\n])\n\ntransform_test = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize(mean=(0.4914, 0.4822, 0.4465), std=(0.2023, 0.1994, 0.2010)),\n])\n\ntrain_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\ntest_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n\ntrain_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=2)\ntest_loader = DataLoader(test_dataset, batch_size=100, shuffle=False, num_workers=2)\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Initialize the modified ResNet model\nmodel = MidResNet(ResidualBlock, [3, 3, 3], num_classes=10).to(device)\n\n# Calculate FLOPs and Params\ndummy_input = torch.randn(1, 3, 32, 32).to(device)\nflops = FlopCountAnalysis(model, dummy_input)\nprint(\"FLOPs and Parameters BEFORE Training:\")\nprint(flop_count_table(flops))\nprint(f\"Total Parameters: {sum(p.numel() for p in model.parameters())}\")\n\n# Loss, optimizer, scheduler\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-4)\n\n# Training loop\ndef train(epoch):\n    model.train()\n    running_loss = 0.0\n    for inputs, targets in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{50}\"):\n        inputs, targets = inputs.to(device), targets.to(device)\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs, targets)\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item()\n    print(f\"Training Loss: {running_loss / len(train_loader):.4f}\")\n\n# Test loop\ndef test():\n    model.eval()\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for inputs, targets in test_loader:\n            inputs, targets = inputs.to(device), targets.to(device)\n            outputs = model(inputs)\n            _, predicted = outputs.max(1)\n            total += targets.size(0)\n            correct += predicted.eq(targets).sum().item()\n    print(f\"Test Accuracy: {100. * correct / total:.2f}%\")\n\n# Train and save model\nfor epoch in range(70):\n    train(epoch)\n    test()\n\nprint(\"FLOPs and Parameters AFTER Training:\")\nprint(flop_count_table(flops))\nprint(f\"Total Parameters AFTER Training: {sum(p.numel() for p in model.parameters())}\")\n\ntorch.save(model.state_dict(), \"cifar_resnet_mid_mode1.pth\")\nprint(\"MODEL SAVED\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T08:27:38.258073Z","iopub.execute_input":"2024-12-04T08:27:38.258464Z","iopub.status.idle":"2024-12-04T09:31:46.569146Z","shell.execute_reply.started":"2024-12-04T08:27:38.258430Z","shell.execute_reply":"2024-12-04T09:31:46.568051Z"}},"outputs":[{"name":"stdout","text":"Files already downloaded and verified\nFiles already downloaded and verified\nFLOPs and Parameters BEFORE Training:\n| module                 | #parameters or shape   | #flops     |\n|:-----------------------|:-----------------------|:-----------|\n| model                  | 4.311M                 | 0.634G     |\n|  conv                  |  0.864K                |  0.885M    |\n|   conv.weight          |   (32, 3, 3, 3)        |            |\n|  bn                    |  64                    |  0.164M    |\n|   bn.weight            |   (32,)                |            |\n|   bn.bias              |   (32,)                |            |\n|  layer1                |  0.206M                |  0.212G    |\n|   layer1.0             |   57.728K              |   59.703M  |\n|    layer1.0.conv1      |    18.432K             |    18.874M |\n|    layer1.0.bn1        |    0.128K              |    0.328M  |\n|    layer1.0.conv2      |    36.864K             |    37.749M |\n|    layer1.0.bn2        |    0.128K              |    0.328M  |\n|    layer1.0.downsample |    2.176K              |    2.425M  |\n|   layer1.1             |   73.984K              |   76.153M  |\n|    layer1.1.conv1      |    36.864K             |    37.749M |\n|    layer1.1.bn1        |    0.128K              |    0.328M  |\n|    layer1.1.conv2      |    36.864K             |    37.749M |\n|    layer1.1.bn2        |    0.128K              |    0.328M  |\n|   layer1.2             |   73.984K              |   76.153M  |\n|    layer1.2.conv1      |    36.864K             |    37.749M |\n|    layer1.2.bn1        |    0.128K              |    0.328M  |\n|    layer1.2.conv2      |    36.864K             |    37.749M |\n|    layer1.2.bn2        |    0.128K              |    0.328M  |\n|  layer2                |  0.821M                |  0.211G    |\n|   layer2.0             |   0.23M                |   59.212M  |\n|    layer2.0.conv1      |    73.728K             |    18.874M |\n|    layer2.0.bn1        |    0.256K              |    0.164M  |\n|    layer2.0.conv2      |    0.147M              |    37.749M |\n|    layer2.0.bn2        |    0.256K              |    0.164M  |\n|    layer2.0.downsample |    8.448K              |    2.261M  |\n|   layer2.1             |   0.295M               |   75.825M  |\n|    layer2.1.conv1      |    0.147M              |    37.749M |\n|    layer2.1.bn1        |    0.256K              |    0.164M  |\n|    layer2.1.conv2      |    0.147M              |    37.749M |\n|    layer2.1.bn2        |    0.256K              |    0.164M  |\n|   layer2.2             |   0.295M               |   75.825M  |\n|    layer2.2.conv1      |    0.147M              |    37.749M |\n|    layer2.2.bn1        |    0.256K              |    0.164M  |\n|    layer2.2.conv2      |    0.147M              |    37.749M |\n|    layer2.2.bn2        |    0.256K              |    0.164M  |\n|  layer3                |  3.28M                 |  0.21G     |\n|   layer3.0             |   0.919M               |   58.966M  |\n|    layer3.0.conv1      |    0.295M              |    18.874M |\n|    layer3.0.bn1        |    0.512K              |    81.92K  |\n|    layer3.0.conv2      |    0.59M               |    37.749M |\n|    layer3.0.bn2        |    0.512K              |    81.92K  |\n|    layer3.0.downsample |    33.28K              |    2.179M  |\n|   layer3.1             |   1.181M               |   75.661M  |\n|    layer3.1.conv1      |    0.59M               |    37.749M |\n|    layer3.1.bn1        |    0.512K              |    81.92K  |\n|    layer3.1.conv2      |    0.59M               |    37.749M |\n|    layer3.1.bn2        |    0.512K              |    81.92K  |\n|   layer3.2             |   1.181M               |   75.661M  |\n|    layer3.2.conv1      |    0.59M               |    37.749M |\n|    layer3.2.bn1        |    0.512K              |    81.92K  |\n|    layer3.2.conv2      |    0.59M               |    37.749M |\n|    layer3.2.bn2        |    0.512K              |    81.92K  |\n|  fc                    |  2.57K                 |  2.56K     |\n|   fc.weight            |   (10, 256)            |            |\n|   fc.bias              |   (10,)                |            |\n|  avg_pool              |                        |  16.384K   |\nTotal Parameters: 4310570\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/50: 100%|██████████| 391/391 [00:45<00:00,  8.59it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 1.4356\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 52.32%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/50: 100%|██████████| 391/391 [00:52<00:00,  7.49it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.9636\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 67.77%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/50: 100%|██████████| 391/391 [00:51<00:00,  7.65it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.7870\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 71.46%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/50: 100%|██████████| 391/391 [00:51<00:00,  7.53it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.6537\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 76.06%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/50: 100%|██████████| 391/391 [00:51<00:00,  7.60it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.5691\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 77.64%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6/50: 100%|██████████| 391/391 [00:51<00:00,  7.59it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.5010\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 79.62%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7/50: 100%|██████████| 391/391 [00:51<00:00,  7.59it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.4482\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 80.76%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8/50: 100%|██████████| 391/391 [00:51<00:00,  7.57it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.4120\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 82.50%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9/50: 100%|██████████| 391/391 [00:51<00:00,  7.54it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.3834\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 82.37%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10/50: 100%|██████████| 391/391 [00:51<00:00,  7.54it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.3453\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 82.15%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 11/50: 100%|██████████| 391/391 [00:51<00:00,  7.52it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.3185\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 83.63%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 12/50: 100%|██████████| 391/391 [00:51<00:00,  7.58it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.2952\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 84.52%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 13/50: 100%|██████████| 391/391 [00:51<00:00,  7.60it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.2811\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 87.33%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 14/50: 100%|██████████| 391/391 [00:51<00:00,  7.60it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.2564\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 86.65%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 15/50: 100%|██████████| 391/391 [00:51<00:00,  7.60it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.2370\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 86.99%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 16/50: 100%|██████████| 391/391 [00:51<00:00,  7.60it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.2267\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 85.34%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 17/50: 100%|██████████| 391/391 [00:51<00:00,  7.57it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.2071\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 88.33%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 18/50: 100%|██████████| 391/391 [00:52<00:00,  7.51it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.1943\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 89.14%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 19/50: 100%|██████████| 391/391 [00:51<00:00,  7.56it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.1787\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 88.62%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 20/50: 100%|██████████| 391/391 [00:51<00:00,  7.58it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.1720\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 89.03%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 21/50: 100%|██████████| 391/391 [00:51<00:00,  7.57it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.1583\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 90.10%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 22/50: 100%|██████████| 391/391 [00:51<00:00,  7.56it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.1516\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 89.48%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 23/50: 100%|██████████| 391/391 [00:51<00:00,  7.53it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.1445\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 87.90%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 24/50: 100%|██████████| 391/391 [00:51<00:00,  7.59it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.1287\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 88.67%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 25/50: 100%|██████████| 391/391 [00:51<00:00,  7.59it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.1245\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 89.19%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 26/50: 100%|██████████| 391/391 [00:51<00:00,  7.60it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.1157\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 89.40%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 27/50: 100%|██████████| 391/391 [00:51<00:00,  7.60it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.1119\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 90.61%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 28/50: 100%|██████████| 391/391 [00:51<00:00,  7.60it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.1017\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 89.91%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 29/50: 100%|██████████| 391/391 [00:51<00:00,  7.60it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.1022\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 89.44%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 30/50: 100%|██████████| 391/391 [00:51<00:00,  7.60it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.0887\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 89.85%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 31/50: 100%|██████████| 391/391 [00:51<00:00,  7.60it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.0912\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 90.10%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 32/50: 100%|██████████| 391/391 [00:51<00:00,  7.59it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.0817\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 89.88%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 33/50: 100%|██████████| 391/391 [00:51<00:00,  7.57it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.0793\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 90.56%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 34/50: 100%|██████████| 391/391 [00:51<00:00,  7.53it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.0735\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 90.02%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 35/50: 100%|██████████| 391/391 [00:51<00:00,  7.59it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.0797\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 90.96%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 36/50: 100%|██████████| 391/391 [00:51<00:00,  7.60it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.0683\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 90.69%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 37/50: 100%|██████████| 391/391 [00:51<00:00,  7.60it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.0675\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 91.44%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 38/50: 100%|██████████| 391/391 [00:51<00:00,  7.60it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.0630\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 89.43%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 39/50: 100%|██████████| 391/391 [00:51<00:00,  7.60it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.0620\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 91.21%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 40/50: 100%|██████████| 391/391 [00:51<00:00,  7.60it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.0586\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 90.11%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 41/50: 100%|██████████| 391/391 [00:51<00:00,  7.60it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.0600\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 90.45%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 42/50: 100%|██████████| 391/391 [00:51<00:00,  7.60it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.0523\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 91.26%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 43/50: 100%|██████████| 391/391 [00:51<00:00,  7.59it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.0533\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 91.12%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 44/50: 100%|██████████| 391/391 [00:51<00:00,  7.55it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.0443\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 90.39%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 45/50: 100%|██████████| 391/391 [00:51<00:00,  7.57it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.0537\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 90.97%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 46/50: 100%|██████████| 391/391 [00:51<00:00,  7.59it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.0449\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 90.75%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 47/50: 100%|██████████| 391/391 [00:51<00:00,  7.58it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.0469\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 91.06%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 48/50: 100%|██████████| 391/391 [00:51<00:00,  7.59it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.0453\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 91.21%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 49/50: 100%|██████████| 391/391 [00:51<00:00,  7.59it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.0419\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 90.92%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 50/50: 100%|██████████| 391/391 [00:51<00:00,  7.57it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.0445\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 91.37%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 51/50: 100%|██████████| 391/391 [00:51<00:00,  7.59it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.0434\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 92.23%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 52/50: 100%|██████████| 391/391 [00:51<00:00,  7.60it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.0348\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 91.14%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 53/50: 100%|██████████| 391/391 [00:51<00:00,  7.60it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.0441\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 91.06%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 54/50: 100%|██████████| 391/391 [00:51<00:00,  7.60it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.0395\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 91.26%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 55/50: 100%|██████████| 391/391 [00:51<00:00,  7.60it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.0333\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 91.91%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 56/50: 100%|██████████| 391/391 [00:51<00:00,  7.60it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.0407\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 92.02%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 57/50: 100%|██████████| 391/391 [00:51<00:00,  7.60it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.0357\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 91.68%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 58/50: 100%|██████████| 391/391 [00:51<00:00,  7.59it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.0345\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 91.46%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 59/50: 100%|██████████| 391/391 [00:51<00:00,  7.60it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.0337\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 91.76%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 60/50: 100%|██████████| 391/391 [00:51<00:00,  7.58it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.0360\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 91.64%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 61/50: 100%|██████████| 391/391 [00:51<00:00,  7.55it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.0313\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 91.63%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 62/50: 100%|██████████| 391/391 [00:51<00:00,  7.58it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.0329\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 90.59%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 63/50: 100%|██████████| 391/391 [00:51<00:00,  7.60it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.0302\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 91.65%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 64/50: 100%|██████████| 391/391 [00:51<00:00,  7.60it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.0339\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 92.36%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 65/50: 100%|██████████| 391/391 [00:51<00:00,  7.60it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.0278\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 91.69%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 66/50: 100%|██████████| 391/391 [00:51<00:00,  7.60it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.0285\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 91.64%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 67/50: 100%|██████████| 391/391 [00:51<00:00,  7.60it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.0334\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 91.66%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 68/50: 100%|██████████| 391/391 [00:51<00:00,  7.59it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.0289\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 91.00%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 69/50: 100%|██████████| 391/391 [00:51<00:00,  7.60it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.0290\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 91.76%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 70/50: 100%|██████████| 391/391 [00:51<00:00,  7.58it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.0245\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 91.86%\nFLOPs and Parameters AFTER Training:\n| module                 | #parameters or shape   | #flops     |\n|:-----------------------|:-----------------------|:-----------|\n| model                  | 4.311M                 | 0.634G     |\n|  conv                  |  0.864K                |  0.885M    |\n|   conv.weight          |   (32, 3, 3, 3)        |            |\n|  bn                    |  64                    |  0.164M    |\n|   bn.weight            |   (32,)                |            |\n|   bn.bias              |   (32,)                |            |\n|  layer1                |  0.206M                |  0.212G    |\n|   layer1.0             |   57.728K              |   59.703M  |\n|    layer1.0.conv1      |    18.432K             |    18.874M |\n|    layer1.0.bn1        |    0.128K              |    0.328M  |\n|    layer1.0.conv2      |    36.864K             |    37.749M |\n|    layer1.0.bn2        |    0.128K              |    0.328M  |\n|    layer1.0.downsample |    2.176K              |    2.425M  |\n|   layer1.1             |   73.984K              |   76.153M  |\n|    layer1.1.conv1      |    36.864K             |    37.749M |\n|    layer1.1.bn1        |    0.128K              |    0.328M  |\n|    layer1.1.conv2      |    36.864K             |    37.749M |\n|    layer1.1.bn2        |    0.128K              |    0.328M  |\n|   layer1.2             |   73.984K              |   76.153M  |\n|    layer1.2.conv1      |    36.864K             |    37.749M |\n|    layer1.2.bn1        |    0.128K              |    0.328M  |\n|    layer1.2.conv2      |    36.864K             |    37.749M |\n|    layer1.2.bn2        |    0.128K              |    0.328M  |\n|  layer2                |  0.821M                |  0.211G    |\n|   layer2.0             |   0.23M                |   59.212M  |\n|    layer2.0.conv1      |    73.728K             |    18.874M |\n|    layer2.0.bn1        |    0.256K              |    0.164M  |\n|    layer2.0.conv2      |    0.147M              |    37.749M |\n|    layer2.0.bn2        |    0.256K              |    0.164M  |\n|    layer2.0.downsample |    8.448K              |    2.261M  |\n|   layer2.1             |   0.295M               |   75.825M  |\n|    layer2.1.conv1      |    0.147M              |    37.749M |\n|    layer2.1.bn1        |    0.256K              |    0.164M  |\n|    layer2.1.conv2      |    0.147M              |    37.749M |\n|    layer2.1.bn2        |    0.256K              |    0.164M  |\n|   layer2.2             |   0.295M               |   75.825M  |\n|    layer2.2.conv1      |    0.147M              |    37.749M |\n|    layer2.2.bn1        |    0.256K              |    0.164M  |\n|    layer2.2.conv2      |    0.147M              |    37.749M |\n|    layer2.2.bn2        |    0.256K              |    0.164M  |\n|  layer3                |  3.28M                 |  0.21G     |\n|   layer3.0             |   0.919M               |   58.966M  |\n|    layer3.0.conv1      |    0.295M              |    18.874M |\n|    layer3.0.bn1        |    0.512K              |    81.92K  |\n|    layer3.0.conv2      |    0.59M               |    37.749M |\n|    layer3.0.bn2        |    0.512K              |    81.92K  |\n|    layer3.0.downsample |    33.28K              |    2.179M  |\n|   layer3.1             |   1.181M               |   75.661M  |\n|    layer3.1.conv1      |    0.59M               |    37.749M |\n|    layer3.1.bn1        |    0.512K              |    81.92K  |\n|    layer3.1.conv2      |    0.59M               |    37.749M |\n|    layer3.1.bn2        |    0.512K              |    81.92K  |\n|   layer3.2             |   1.181M               |   75.661M  |\n|    layer3.2.conv1      |    0.59M               |    37.749M |\n|    layer3.2.bn1        |    0.512K              |    81.92K  |\n|    layer3.2.conv2      |    0.59M               |    37.749M |\n|    layer3.2.bn2        |    0.512K              |    81.92K  |\n|  fc                    |  2.57K                 |  2.56K     |\n|   fc.weight            |   (10, 256)            |            |\n|   fc.bias              |   (10,)                |            |\n|  avg_pool              |                        |  16.384K   |\nTotal Parameters AFTER Training: 4310570\nMODEL SAVED\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## From kaggle","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.optim.lr_scheduler import CosineAnnealingLR, LambdaLR\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision import datasets, transforms\nfrom torchvision.transforms import RandomErasing\nfrom tqdm import tqdm\nimport math\nimport os\nimport pandas as pd\nfrom PIL import Image\nimport py7zr\n\n# 1. Print all the files in the input directory to verify paths\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T09:35:51.481471Z","iopub.execute_input":"2024-12-04T09:35:51.481835Z","iopub.status.idle":"2024-12-04T09:35:51.624729Z","shell.execute_reply.started":"2024-12-04T09:35:51.481803Z","shell.execute_reply":"2024-12-04T09:35:51.623876Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/cifar-10/trainLabels.csv\n/kaggle/input/cifar-10/sampleSubmission.csv\n/kaggle/input/cifar-10/test.7z\n/kaggle/input/cifar-10/train.7z\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"import py7zr\nimport os\n\n# Extract train and test images\nwith py7zr.SevenZipFile('/kaggle/input/cifar-10/train.7z', mode='r') as z:\n    z.extractall(path='/kaggle/working/train_images')\n    \nwith py7zr.SevenZipFile('/kaggle/input/cifar-10/test.7z', mode='r') as z:\n    z.extractall(path='/kaggle/working/test_images')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T09:35:57.517792Z","iopub.execute_input":"2024-12-04T09:35:57.518153Z","iopub.status.idle":"2024-12-04T09:38:39.734875Z","shell.execute_reply.started":"2024-12-04T09:35:57.518091Z","shell.execute_reply":"2024-12-04T09:38:39.734161Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"train_labels_df = pd.read_csv('/kaggle/input/cifar-10/trainLabels.csv')\n\n# Map image filenames with labels\ntrain_filenames = [os.path.join('/kaggle/working/train_images/train', f\"{i}.png\") for i in train_labels_df['id']]\ntrain_labels = train_labels_df['label'].values\n\n# CIFAR-10 classes (convert labels from string to index)\nclasses = ('airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\nlabel_to_index = {label: idx for idx, label in enumerate(classes)}\ntrain_labels = [label_to_index[label] for label in train_labels]\n\n# Data Augmentation with CutOut (RandomErasing) for training data\ntrain_transform = transforms.Compose([\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomCrop(32, padding=4),\n    transforms.ToTensor(),\n    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n    RandomErasing(scale=(0.02, 0.33))\n])\n\n# Define a simpler transform for test data\ntest_transform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T09:38:39.736552Z","iopub.execute_input":"2024-12-04T09:38:39.736903Z","iopub.status.idle":"2024-12-04T09:38:39.831206Z","shell.execute_reply.started":"2024-12-04T09:38:39.736865Z","shell.execute_reply":"2024-12-04T09:38:39.830465Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# Create a custom dataset class for the training data\nclass CIFAR10CustomDataset(Dataset):\n    def __init__(self, image_paths, labels, transform=None):\n        self.image_paths = image_paths\n        self.labels = labels\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.image_paths)\n\n    def __getitem__(self, idx):\n        img_path = self.image_paths[idx]\n        image = Image.open(img_path)\n        label = self.labels[idx]\n\n        if self.transform:\n            image = self.transform(image)\n\n        return image, label\n\n# Create the dataset and dataloader for training data\ntrain_dataset = CIFAR10CustomDataset(train_filenames, train_labels, transform=train_transform)\ntrain_loader = DataLoader(dataset=train_dataset, batch_size=128, shuffle=True, num_workers=2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T09:38:39.832052Z","iopub.execute_input":"2024-12-04T09:38:39.832302Z","iopub.status.idle":"2024-12-04T09:38:39.838955Z","shell.execute_reply.started":"2024-12-04T09:38:39.832278Z","shell.execute_reply":"2024-12-04T09:38:39.838074Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# Define the Residual Block and ResNet Model (same as before)\nclass ResidualBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n        super(ResidualBlock, self).__init__()\n        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(out_channels)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(out_channels)\n        self.downsample = downsample\n\n    def forward(self, x):\n        residual = x\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n        out = self.conv2(out)\n        out = self.bn2(out)\n        if self.downsample:\n            residual = self.downsample(x)\n        out += residual\n        out = self.relu(out)\n        return out\n\nclass ResNet(nn.Module):\n    def __init__(self, block, layers, num_classes=10):\n        super(ResNet, self).__init__()\n        self.in_channels = 64\n        self.conv = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn = nn.BatchNorm2d(64)\n        self.relu = nn.ReLU(inplace=True)\n        self.layer1 = self.make_layer(block, 64, layers[0])\n        self.layer2 = self.make_layer(block, 128, layers[1], stride=2)\n        self.layer3 = self.make_layer(block, 256, layers[2], stride=2)\n        self.layer4 = self.make_layer(block, 512, layers[3], stride=2)\n        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n        self.fc = nn.Linear(512, num_classes)\n\n    def make_layer(self, block, out_channels, blocks, stride=1):\n        downsample = None\n        if stride != 1 or self.in_channels != out_channels:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(out_channels)\n            )\n        layers = []\n        layers.append(block(self.in_channels, out_channels, stride, downsample))\n        self.in_channels = out_channels\n        for _ in range(1, blocks):\n            layers.append(block(out_channels, out_channels))\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.bn(x)\n        x = self.relu(x)\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        x = self.avg_pool(x)\n        x = torch.flatten(x, 1)\n        x = self.fc(x)\n        return x\n\n# Initialize model, loss function, optimizer, and learning rate scheduler\nmodel = ResNet(ResidualBlock, [2, 2, 2, 2]).cuda()\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-4)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T09:38:39.840698Z","iopub.execute_input":"2024-12-04T09:38:39.841289Z","iopub.status.idle":"2024-12-04T09:38:39.944963Z","shell.execute_reply.started":"2024-12-04T09:38:39.841242Z","shell.execute_reply":"2024-12-04T09:38:39.944387Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# Cosine Annealing with Warmup\ndef warmup_cosine_lr_scheduler(optimizer, warmup_iters, max_iters):\n    def lr_lambda(current_iter):\n        if current_iter < warmup_iters:\n            return float(current_iter) / float(warmup_iters)  # Warmup phase\n        else:\n            # Cosine annealing phase\n            return 0.5 * (1 + math.cos(float(current_iter - warmup_iters) / float(max_iters - warmup_iters) * math.pi))\n    return LambdaLR(optimizer, lr_lambda)\n\n# Training and Testing Functions (same as before)\ndef train(model, train_loader, criterion, optimizer, scheduler):\n    model.train()\n    running_loss = 0.0\n    correct = 0\n    total = 0\n    for images, labels in tqdm(train_loader):\n        images, labels = images.cuda(), labels.cuda()\n        optimizer.zero_grad()\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item()\n        _, predicted = outputs.max(1)\n        total += labels.size(0)\n        correct += predicted.eq(labels).sum().item()\n    scheduler.step()\n    print(f\"Train Loss: {running_loss/len(train_loader)}, Train Accuracy: {100 * correct/total:.2f}%\")\n\ndef test(model, test_loader, criterion):\n    model.eval()\n    running_loss = 0.0\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for images, labels in tqdm(test_loader):\n            images, labels = images.cuda(), labels.cuda()\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            running_loss += loss.item()\n            _, predicted = outputs.max(1)\n            total += labels.size(0)\n            correct += predicted.eq(labels).sum().item()\n    print(f\"Test Loss: {running_loss/len(test_loader)}, Test Accuracy: {100 * correct/total:.2f}%\")\n    return 100 * correct/total\n\n# Main training loop\nmax_iters = 45\nwarmup_iters = 20  # Warmup phase epochs\nscheduler = warmup_cosine_lr_scheduler(optimizer, warmup_iters, max_iters)\n\nbest_acc = 0\n\nfor epoch in range(max_iters):\n    print(f\"Epoch {epoch+1}/{max_iters}\")\n    train(model, train_loader, criterion, optimizer, scheduler)\n    acc = test(model, train_loader, criterion)\n    if acc > best_acc:\n        best_acc = acc\n        torch.save(model.state_dict(), \"best_model_high_Test.pth\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T09:38:39.945828Z","iopub.execute_input":"2024-12-04T09:38:39.946057Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/45\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:47<00:00,  8.22it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 2.4084013099865533, Train Accuracy: 9.68%\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:21<00:00, 17.94it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 2.4105548876935563, Test Accuracy: 9.68%\nEpoch 2/45\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:50<00:00,  7.79it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.5593097719085185, Train Accuracy: 42.52%\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:21<00:00, 18.39it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 1.374968390025751, Test Accuracy: 49.99%\nEpoch 3/45\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:50<00:00,  7.72it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.216722205624251, Train Accuracy: 56.04%\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:21<00:00, 18.26it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 1.2339170361723741, Test Accuracy: 56.34%\nEpoch 4/45\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:50<00:00,  7.73it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.0054678285823149, Train Accuracy: 64.26%\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:21<00:00, 18.50it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 1.0142482837752613, Test Accuracy: 64.77%\nEpoch 5/45\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:50<00:00,  7.77it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.861000851139693, Train Accuracy: 69.84%\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:21<00:00, 18.20it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 0.9403767094892614, Test Accuracy: 67.32%\nEpoch 6/45\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:50<00:00,  7.75it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.7692277115171827, Train Accuracy: 72.97%\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:21<00:00, 18.33it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 0.9129568640228427, Test Accuracy: 68.35%\nEpoch 7/45\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:50<00:00,  7.73it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.7033825817010592, Train Accuracy: 75.32%\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:21<00:00, 18.14it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 0.9822658943703108, Test Accuracy: 68.30%\nEpoch 8/45\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:50<00:00,  7.72it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.6431815251517479, Train Accuracy: 77.63%\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:20<00:00, 18.62it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 0.6398160599381723, Test Accuracy: 77.65%\nEpoch 9/45\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:50<00:00,  7.72it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.6104684033052391, Train Accuracy: 78.70%\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:21<00:00, 17.92it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 0.6827639638615386, Test Accuracy: 76.61%\nEpoch 10/45\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:50<00:00,  7.75it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.5687352752746524, Train Accuracy: 80.34%\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:21<00:00, 18.08it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 0.8180861477656742, Test Accuracy: 73.04%\nEpoch 11/45\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:50<00:00,  7.73it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.5424037920239636, Train Accuracy: 81.15%\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:21<00:00, 18.10it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 0.6769786496144121, Test Accuracy: 76.56%\nEpoch 12/45\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:50<00:00,  7.75it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.5136195024870851, Train Accuracy: 82.32%\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:20<00:00, 18.85it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 0.5417249106690097, Test Accuracy: 80.76%\nEpoch 13/45\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:50<00:00,  7.74it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.49686034500141585, Train Accuracy: 82.86%\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:21<00:00, 18.31it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 0.5893441117023264, Test Accuracy: 79.73%\nEpoch 14/45\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:50<00:00,  7.73it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.46676159167990966, Train Accuracy: 83.69%\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:21<00:00, 18.03it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 0.7107227078026823, Test Accuracy: 76.98%\nEpoch 15/45\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:50<00:00,  7.75it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.45325108752836046, Train Accuracy: 84.23%\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:20<00:00, 18.66it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 0.5209427507941985, Test Accuracy: 81.80%\nEpoch 16/45\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:50<00:00,  7.73it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.43237027724075805, Train Accuracy: 84.97%\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:20<00:00, 18.79it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 0.49581391480572695, Test Accuracy: 82.66%\nEpoch 17/45\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:50<00:00,  7.75it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.4268256726548495, Train Accuracy: 85.24%\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:21<00:00, 18.57it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 0.40680152017747045, Test Accuracy: 85.89%\nEpoch 18/45\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:50<00:00,  7.74it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.406308168340522, Train Accuracy: 85.89%\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:21<00:00, 17.95it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 0.4424433704379879, Test Accuracy: 85.01%\nEpoch 19/45\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:50<00:00,  7.72it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.3922664330667242, Train Accuracy: 86.24%\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:21<00:00, 18.33it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 0.4375018700766746, Test Accuracy: 84.80%\nEpoch 20/45\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:50<00:00,  7.74it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.3759881593001163, Train Accuracy: 86.96%\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:20<00:00, 18.68it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 0.39439525236101713, Test Accuracy: 86.35%\nEpoch 21/45\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:50<00:00,  7.74it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.37105606321026297, Train Accuracy: 87.12%\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:20<00:00, 18.76it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 0.36511855479091637, Test Accuracy: 87.14%\nEpoch 22/45\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:50<00:00,  7.78it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.35070710749272493, Train Accuracy: 87.80%\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:21<00:00, 18.61it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 0.3692601552171171, Test Accuracy: 87.44%\nEpoch 23/45\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:50<00:00,  7.71it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.32126259590353806, Train Accuracy: 88.65%\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:20<00:00, 18.77it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 0.3332406451635044, Test Accuracy: 88.43%\nEpoch 24/45\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:50<00:00,  7.76it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.3104703553649775, Train Accuracy: 89.15%\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:20<00:00, 19.03it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 0.29164112730861624, Test Accuracy: 89.75%\nEpoch 25/45\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:50<00:00,  7.72it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.29432198245202185, Train Accuracy: 89.64%\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:20<00:00, 18.86it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 0.31807556782689544, Test Accuracy: 88.71%\nEpoch 26/45\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:50<00:00,  7.74it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.28125737516014165, Train Accuracy: 90.05%\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:21<00:00, 18.44it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 0.34943176375325685, Test Accuracy: 88.09%\nEpoch 27/45\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:50<00:00,  7.74it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.26507108778599886, Train Accuracy: 90.61%\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:21<00:00, 18.38it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 0.3283120730077214, Test Accuracy: 88.69%\nEpoch 28/45\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:50<00:00,  7.75it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.24751411145910276, Train Accuracy: 91.28%\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:20<00:00, 18.76it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 0.2373088863690186, Test Accuracy: 91.66%\nEpoch 29/45\n","output_type":"stream"},{"name":"stderr","text":" 45%|████▍     | 175/391 [00:22<00:27,  7.77it/s]","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}